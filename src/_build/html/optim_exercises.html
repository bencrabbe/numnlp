

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Optimization exercises &#8212; Numerical methods in NLP (foundations)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optim_exercises';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Optimization basics" href="optim.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logoUPC.png" class="logo__image only-light" alt="Numerical methods in NLP (foundations) - Home"/>
    <script>document.write(`<img src="_static/logoUPC.png" class="logo__image only-dark" alt="Numerical methods in NLP (foundations) - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Numerical methods for Natural Language Processing
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vectors.html">Vectors and vector spaces</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="numpy_exercises.html">Numpy exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="euclid.html">The euclidean space</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Euclid_exercises.html">Euclidean spaces : exercises</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="vec2text.html">Applications to information retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrices.html">Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphs.html">Applications to graph analysis : the PageRank algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Applications to statistical analysis : latent semantic analysis</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="optim.html">Optimization basics</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Optimization exercises</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="http://github.com/bencrabbe/numnlp/issues/new?title=Issue%20on%20page%20%2Foptim_exercises.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button"
   title="Open an issue"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optim_exercises.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization exercises</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3">Exercise 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4">Exercise 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5">Exercise 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-6">Exercise 6</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p>Before you turn this problem in, make sure everything runs as expected. First, <strong>restart the kernel</strong> (in the menubar, select Kernel<span class="math notranslate nohighlight">\(\rightarrow\)</span>Restart) and then <strong>run all cells</strong> (in the menubar, select Cell<span class="math notranslate nohighlight">\(\rightarrow\)</span>Run All).</p>
<p>Make sure you fill in any place that says <code class="docutils literal notranslate"><span class="pre">YOUR</span> <span class="pre">CODE</span> <span class="pre">HERE</span></code> or “YOUR ANSWER HERE”, as well as your name and collaborators below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NAME</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="n">COLLABORATORS</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="optimization-exercises">
<h1>Optimization exercises<a class="headerlink" href="#optimization-exercises" title="Permalink to this heading">#</a></h1>
<p>As should be clear there is no silver bullet for optimizing any function. Although convex functions are the theoretically easy case,
non convex optimization is becoming the classical use case since the advent of deep learning.
In practice it is important to monitor the optimization process and to be able to understand when the optimization works well, struggles or entirely fails.</p>
<p><strong>Important Note</strong> some of these exercises are automatically graded. You have to pay attention to:</p>
<ul class="simple">
<li><p>Never modify existing function signatures. You may add other functions and testing code if you like. It does not impact grading.</p></li>
<li><p>Pass the assertion tests found in the notebook. These tests and others you cannot see are used to grade part of your homework</p></li>
<li><p>Pay attention to provide reasonably efficient solutions. Any notebook cell that takes more than 30 seconds at execution is considered as failed.</p></li>
</ul>
<hr class="docutils" />
<section id="exercise-1">
<h2>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this heading">#</a></h2>
<p>For the function <span class="math notranslate nohighlight">\(f(x) = (3x-2)^2\)</span></p>
<ol class="arabic simple">
<li><p>Plot the function within the interval <span class="math notranslate nohighlight">\([-3,3]\)</span></p></li>
<li><p>Compute the first order derivative</p></li>
<li><p>Compute the second order derivative</p></li>
<li><p>Optimize the function with gradient descent.</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="k">def</span> <span class="nf">fun1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># PLOT SECTION</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NotImplementedError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">line</span> <span class="mi">10</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>     <span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="c1"># PLOT SECTION</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> 
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="c1"># YOUR CODE HERE</span>
<span class="ne">---&gt; </span><span class="mi">10</span> <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

<span class="ne">NotImplementedError</span>: 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>

<span class="k">def</span> <span class="nf">fprime1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the first order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">fsec1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the second order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>



<span class="c1"># It is strongly advised to output intermediate results during optimisation such as:</span>
<span class="c1">#  - the successive values of the iterates.</span>
<span class="c1">#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.</span>
<span class="c1">#  - the current distance from the theoretical solution you computed analytically and the current iterate.</span>

<span class="c1">################## NOTE #######################################################</span>
<span class="c1"># For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.</span>
<span class="c1"># @see https://en.wikipedia.org/wiki/Learning_rate</span>
<span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">fprime</span><span class="p">,</span><span class="n">alpha0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0        (float)  : the initial iterate</span>
<span class="sd">        fprime(functional) : the first derivative function</span>
<span class="sd">        alpha0 (float)     : the initial learning rate</span>
<span class="sd">        decay (float)      : scheduler decay </span>
<span class="sd">        epsilon            : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the value of the last iterate</span>
<span class="sd">    &quot;&quot;&quot;</span>    
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># It is strongly advised to output intermediate results during optimisation such as:</span>
<span class="c1">#  - the successive values of the iterates.</span>
<span class="c1">#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.</span>
<span class="c1">#  - the current distance from the theoretical solution you computed and the current iterate.</span>

<span class="c1">################## NOTE #######################################################</span>
<span class="c1"># For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.</span>
<span class="c1"># @see https://en.wikipedia.org/wiki/Learning_rate</span>
<span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">fprime</span><span class="p">,</span><span class="n">fsec</span><span class="p">,</span><span class="n">beta0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0        (float)  : the initial iterate</span>
<span class="sd">        fprime(functional) : the first derivative function</span>
<span class="sd">        fsec(functional)   : the second derivative function</span>
<span class="sd">        beta0 (float)      : the initial heuristic learning rate to be used by the scheduler</span>
<span class="sd">        decay(float)       : the scheduler decay (like in gradient descent)</span>
<span class="sd">        epsilon            : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the value of the last iterate</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited </span>
<span class="c1">#   for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>
<span class="n">hyper1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
          <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>


    
<span class="c1"># ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION/FIND HYPERPARAMETERS</span>
<span class="c1"># test against several initial conditions, several learning rates, several epsilon</span>
<span class="c1"># test against scipy.optimize.minimize</span>
<span class="c1"># it will not be graded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### TEST GRADIENT DESCENT  (2pts)</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="k">assert</span> <span class="n">fprime1</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span> 

<span class="k">assert</span>  <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">fprime1</span><span class="p">,</span>
                          <span class="n">alpha0</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span>
                          <span class="n">decay</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span>
                          <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">]),</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### TEST NEWTON (2pts)</span>

<span class="k">assert</span> <span class="n">fsec1</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="o">==</span> <span class="mi">18</span>

<span class="k">assert</span>  <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">newton</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="n">fprime1</span><span class="p">,</span><span class="n">fsec1</span><span class="p">,</span>
                                      <span class="n">beta0</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;beta0&#39;</span><span class="p">],</span>
                                      <span class="n">decay</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span>
                                      <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper1</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">]),</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<p><strong>Free content and comments section</strong>:</p>
<ul class="simple">
<li><p>What are the key properties of this function ? is it convex ? How many solutions can you find ?</p></li>
<li><p>In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.</p></li>
<li><p>How did you tune alpha, epsilon ?</p></li>
<li><p>…</p></li>
</ul>
<p>YOUR ANSWER HERE</p>
</section>
<hr class="docutils" />
<section id="exercise-2">
<h2>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this heading">#</a></h2>
<p>For the function <span class="math notranslate nohighlight">\(f(x) = x (x-2) (x+2)^2\)</span></p>
<ol class="arabic simple">
<li><p>Plot the function within the interval <span class="math notranslate nohighlight">\([-3,3]\)</span></p></li>
<li><p>Compute the first order derivative</p></li>
<li><p>Compute the second order derivative</p></li>
<li><p>Optimize the function with gradient descent.</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="k">def</span> <span class="nf">fun2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># PLOT SECTION</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>

<span class="k">def</span> <span class="nf">fprime2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the first order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">fsec2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the second order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    
<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>

<span class="n">hyper2</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
          <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span> <span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>


<span class="c1"># FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.</span>
<span class="c1"># test against several initial conditions, several learning rates, several epsilon</span>
<span class="c1"># test against scipy.optimize.minimize</span>
<span class="c1"># it will not be graded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># GRADIENT DESCENT TESTS (2pts)</span>

<span class="k">assert</span> <span class="n">fprime2</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">6</span> 

<span class="n">solset</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1.28077641</span><span class="p">,</span><span class="o">-</span><span class="mf">0.7807764064043429</span><span class="p">]</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="n">fprime2</span><span class="p">,</span>
                          <span class="n">alpha0</span><span class="o">=</span><span class="n">hyper2</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span>
                          <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper2</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span>
<span class="k">assert</span> <span class="nb">any</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">solset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># NEWTON TESTS (2pts)</span>

<span class="k">assert</span> <span class="n">fsec2</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="o">==</span> <span class="o">-</span><span class="mi">8</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="n">fprime2</span><span class="p">,</span><span class="n">fsec2</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="n">hyper2</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span> 
<span class="k">assert</span> <span class="nb">any</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">solset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Free content and comments section</strong>:</p>
<ul class="simple">
<li><p>What are the key properties of this function ? is it convex ? How many solutions can you find ?</p></li>
<li><p>In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.</p></li>
<li><p>How did you tune alpha, epsilon ?</p></li>
<li><p>…</p></li>
</ul>
<p>YOUR ANSWER HERE</p>
</section>
<hr class="docutils" />
<section id="exercise-3">
<h2>Exercise 3<a class="headerlink" href="#exercise-3" title="Permalink to this heading">#</a></h2>
<p>For the function <span class="math notranslate nohighlight">\(f(x) = -e^{-(3x-1)^2} + \frac{x^2}{100}\)</span>.</p>
<ol class="arabic simple">
<li><p>Plot the function within the interval <span class="math notranslate nohighlight">\([-3,3]\)</span></p></li>
<li><p>Compute the first order derivative</p></li>
<li><p>Compute the second order derivative</p></li>
<li><p>Optimize the function with gradient descent.</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">exp</span>

<span class="k">def</span> <span class="nf">fun3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">100</span> 


<span class="c1">#PLOT SECTION</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">exp</span>

<span class="k">def</span> <span class="nf">fprime3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the first order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>    
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">fsec3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (float) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the second order derivative at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>

<span class="n">hyper3</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
          <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span> <span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>




<span class="c1"># FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.</span>
<span class="c1"># test against several initial conditions, several learning rates, several epsilon</span>

<span class="c1"># it will not be graded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="c1">#### TEST GRADIENT DESCENT #### (2pts)</span>

<span class="k">assert</span> <span class="n">fprime3</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.06</span>

<span class="k">assert</span> <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span><span class="n">gradient_descent</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="n">fprime3</span><span class="p">,</span>
                                           <span class="n">alpha0</span><span class="o">=</span><span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span>
                                           <span class="n">decay</span><span class="o">=</span><span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span>
                                           <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">]),</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#### TEST NEWTON #### (2pts)</span>

<span class="k">assert</span> <span class="n">fsec3</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.02</span>

<span class="k">assert</span> <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">newton</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="n">fprime3</span><span class="p">,</span><span class="n">fsec3</span><span class="p">,</span>
                                <span class="n">beta0</span> <span class="o">=</span> <span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;beta0&#39;</span><span class="p">],</span>
                                <span class="n">decay</span> <span class="o">=</span> <span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span>
                                <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper3</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">]),</span><span class="n">abs_tol</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Free content and comments section</strong>:</p>
<ul class="simple">
<li><p>What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ?</p></li>
<li><p>What are the key properties of this function ? is it convex ?</p></li>
<li><p>In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.</p></li>
<li><p>how did you tune alpha, epsilon ?</p></li>
<li><p>…</p></li>
</ul>
<p>YOUR ANSWER HERE</p>
</section>
<hr class="docutils" />
<section id="exercise-4">
<h2>Exercise 4<a class="headerlink" href="#exercise-4" title="Permalink to this heading">#</a></h2>
<p>For the function <span class="math notranslate nohighlight">\(f(x_1,x_2) = \sum_{i=1}^2 x_i^4 - 16x_i^2 + 5 x_i\)</span>.</p>
<ol class="arabic simple">
<li><p>Plot the function for <span class="math notranslate nohighlight">\(x_1 \in [-5,5]\)</span> and <span class="math notranslate nohighlight">\(x_2 \in [-5,5]\)</span></p></li>
<li><p>Compute the gradient</p></li>
<li><p>Compute the hessian</p></li>
<li><p>Optimize the function with gradient descent.</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="k">def</span> <span class="nf">fun4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">4</span> <span class="o">-</span> <span class="mi">16</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>

<span class="c1"># PLOT SECTION</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">inv</span><span class="p">,</span><span class="n">norm</span>

<span class="k">def</span> <span class="nf">grad4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (numpy.array) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The gradient vector at x</span>
<span class="sd">    &quot;&quot;&quot;</span>    
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">hess4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (numpy.array) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The hessian matrix at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>



<span class="c1"># It is strongly advised to output intermediate results during optimisation such as:</span>
<span class="c1">#  - the successive values of the iterates.</span>
<span class="c1">#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.</span>
<span class="c1">#  - the current distance from the theoretical solution you computed analytically and the current iterate.</span>

<span class="c1">################## NOTE #######################################################</span>
<span class="c1"># For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.</span>
<span class="c1"># @see https://en.wikipedia.org/wiki/Learning_rate</span>
<span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">gradient_descent_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">alpha0</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0   (numpy.array) : the initial iterate</span>
<span class="sd">        grad  (functional) : the gradient function</span>
<span class="sd">        alpha0             : the initial learning rate</span>
<span class="sd">        epsilon            : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The value of the last iterate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># It is strongly advised to output intermediate results during optimisation such as:</span>
<span class="c1">#  - the successive values of the iterates.</span>
<span class="c1">#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.</span>
<span class="c1">#  - the current distance from the theoretical solution you computed analytically and the current iterate.</span>

<span class="c1">################## NOTE #######################################################</span>
<span class="c1"># For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.</span>
<span class="c1"># @see https://en.wikipedia.org/wiki/Learning_rate</span>
<span class="c1">###############################################################################</span>

<span class="k">def</span> <span class="nf">newton_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">hessian</span><span class="p">,</span><span class="n">beta0</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0   (numpy.array) : the initial iterate</span>
<span class="sd">        grad  (functional) : the gradient function</span>
<span class="sd">        hessian(functional): the hessian function</span>
<span class="sd">        epsilon            : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The value of the last iterate</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>

<span class="n">hyper4</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
          <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span> <span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>

<span class="c1"># FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.</span>
<span class="c1"># e.g test with scipy.optimize.minimize</span>
<span class="c1"># it will not be graded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### GRADIENT DESCENT TEST SECTION (2pts)</span>

<span class="n">solution_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.90353404</span><span class="p">,</span>  <span class="mf">2.74680293</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.74680289</span><span class="p">,</span> <span class="mf">2.74680289</span><span class="p">],</span> <span class="p">[</span> <span class="mf">2.74680277</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.90353403</span><span class="p">],[</span><span class="o">-</span><span class="mf">2.90353403</span> <span class="p">,</span> <span class="mf">2.74680277</span><span class="p">],[</span><span class="o">-</span><span class="mf">2.90353403</span> <span class="p">,</span> <span class="o">-</span><span class="mf">2.90353403</span><span class="p">]])</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">sol</span> <span class="o">=</span> <span class="n">gradient_descent_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad4</span><span class="p">,</span><span class="n">alpha0</span> <span class="o">=</span> <span class="n">hyper4</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper4</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span>
<span class="k">assert</span> <span class="nb">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">solution_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### NEWTON TEST SECTION (2pts)</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">newton_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad4</span><span class="p">,</span><span class="n">hess4</span><span class="p">,</span> <span class="n">beta0</span> <span class="o">=</span> <span class="n">hyper4</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;beta0&#39;</span><span class="p">],</span> <span class="n">decay</span> <span class="o">=</span> <span class="n">hyper4</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span><span class="n">epsilon</span><span class="o">=</span><span class="n">hyper4</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span>
<span class="k">assert</span> <span class="nb">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">solution_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Free content and comments section</strong>:</p>
<ul class="simple">
<li><p>What is the exact solution ? how did you compute it ?</p></li>
<li><p>What are the key properties of this function ? is it convex ? how many solutions can you identify ?</p></li>
<li><p>In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.</p></li>
<li><p>how did you tune alpha, beta epsilon ?</p></li>
<li><p>…</p></li>
</ul>
<p>YOUR ANSWER HERE</p>
</section>
<hr class="docutils" />
<section id="exercise-5">
<h2>Exercise 5<a class="headerlink" href="#exercise-5" title="Permalink to this heading">#</a></h2>
<p>For the function  <span class="math notranslate nohighlight">\(f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2\)</span></p>
<ol class="arabic simple">
<li><p>Plot the function for <span class="math notranslate nohighlight">\(x_1 \in [-2,2]\)</span> and <span class="math notranslate nohighlight">\(x_2 \in [-2,2]\)</span></p></li>
<li><p>Compute the gradient</p></li>
<li><p>Compute the hessian</p></li>
<li><p>Optimize the function with gradient descent.</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="k">def</span> <span class="nf">fun5</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>


<span class="c1"># PLOT SECTION</span>

<span class="c1"># YOUR CODE HERE</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>

<span class="k">def</span> <span class="nf">grad5</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (numpy.array) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The gradient vector at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">hess5</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x (numpy.array) : the x value</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The hessian matrix at x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>

<span class="n">hyper5</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
          <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span> <span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>


<span class="c1"># FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.</span>
<span class="c1"># it will not be graded</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">######### GRADIENT DESCENT TEST (2pts)</span>

<span class="n">x0</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>
<span class="n">rsol</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">gradient_descent_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad5</span><span class="p">,</span><span class="n">alpha0</span> <span class="o">=</span> <span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span> <span class="n">decay</span><span class="o">=</span> <span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">rsol</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">######### NEWTON TEST (2pts)</span>

<span class="n">sol</span> <span class="o">=</span> <span class="n">newton_mv</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">grad5</span><span class="p">,</span><span class="n">hess5</span><span class="p">,</span><span class="n">beta0</span> <span class="o">=</span> <span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;beta0&#39;</span><span class="p">],</span> <span class="n">decay</span><span class="o">=</span><span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">hyper5</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">rsol</span><span class="p">,</span><span class="n">sol</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Free content and comments section</strong>:</p>
<ul class="simple">
<li><p>What is the exact solution ? how did you compute it ? with scipy.optimize.minimize ?</p></li>
<li><p>What are the key properties of this function ? is it convex ?</p></li>
<li><p>In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.</p></li>
<li><p>how did you tune alpha, epsilon ?</p></li>
<li><p>…</p></li>
</ul>
<p>YOUR ANSWER HERE</p>
</section>
<hr class="docutils" />
<section id="exercise-6">
<h2>Exercise 6<a class="headerlink" href="#exercise-6" title="Permalink to this heading">#</a></h2>
<p>In this exercise we implement parameter estimation for the multivariate linear regression model on the <code class="docutils literal notranslate"><span class="pre">Iris</span></code> dataset. The first step if to preprocess the raw dataset in order to get a design matrix  <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times k}\)</span> with <span class="math notranslate nohighlight">\(n\)</span> lines whose <span class="math notranslate nohighlight">\(k\)</span> columns are predictors and a <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> vector
with reference values to be predicted. In matrix notation the sum of squares loss can be reformulated as:</p>
<div class="math notranslate nohighlight">
\[
ssq_{\cal D}(\mathbf{p}) = \sum_{i=1}^n(\mathbf{X}\mathbf{p} - \mathbf{y})^2
\]</div>
<ol class="arabic simple">
<li><p>Compute the gradient. To do so you may compute it analytically or take advantage of sympy.
The result can be expressed in matrix form as <span class="math notranslate nohighlight">\(\nabla ssq_{\cal D}(\mathbf{p}) = \mathbf{X}^\top (\mathbf{X}\mathbf{p} - \mathbf{y})\)</span></p></li>
<li><p>Compute the hessian.  To do so you may compute it analytically or take advantage of sympy.
The result can be expressed in matrix form as <span class="math notranslate nohighlight">\(\mathbf{H}_{ssq_{\cal D}} (\mathbf{p}) = \mathbf{X}^\top \mathbf{X}\)</span></p></li>
<li><p>Optimize the function with gradient descent</p></li>
<li><p>Optimize the function with the newton method</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pa</span>


<span class="c1">#PREPARE DATA SECTION</span>
<span class="c1">#grabs the full iris dataset from the sklearn library </span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iris variable names&#39;</span><span class="p">,</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">do_data_matrix</span><span class="p">(</span><span class="n">iris_frame</span><span class="p">,</span><span class="n">x_colnames</span><span class="p">,</span><span class="n">y_colname</span><span class="p">,</span><span class="n">add_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a numpy matrix encoding the dataset.</span>
<span class="sd">    It converts a pandas dataframe to a couple (X,y) of numpy arrays </span>
<span class="sd">    Args:</span>
<span class="sd">        iris_frame (pandas.DataFrame): the iris dataframe</span>
<span class="sd">        x_colnames (list) : list of strings, the predictor names</span>
<span class="sd">        y_colname   (str) : the name of the predicted variable</span>
<span class="sd">        add_bias    (bool): whether we add a bias to the model or not   </span>
<span class="sd">    Returns:</span>
<span class="sd">        (numpy.array,numpy.array). Tuple (X,y) the first element is a design matrix. </span>
<span class="sd">                                   A matrix whose columns are x predictor variables with one row for each data line</span>
<span class="sd">                                   The second element is a vector with the y values of the predicted variable</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span>             <span class="o">=</span> <span class="n">iris_frame</span><span class="p">[</span><span class="n">x_colnames</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
    <span class="n">nlines</span><span class="p">,</span> <span class="n">ncols</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">if</span> <span class="n">add_bias</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">nlines</span><span class="p">,</span><span class="mi">1</span><span class="p">))],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">iris_frame</span><span class="p">[</span><span class="n">y_colname</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
 
<span class="c1">#  *** AUTOGRADING SECTION ***</span>
<span class="c1">#</span>
<span class="c1">#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!</span>
<span class="c1">#</span>
<span class="c1">#  Each of your functions must return a value (@see docstring)</span>


<span class="k">def</span> <span class="nf">leastsq_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">yref</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        params         (numpy.array): the vector of variables (linear regression parameters)</span>
<span class="sd">        X              (numpy.array): the design matrix (columns are predictor variables)</span>
<span class="sd">        yref           (numpy.array): the vector of y values</span>
<span class="sd">    Returns:</span>
<span class="sd">        float. the sum of squares value for this dataset and the current value of the parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">gradient_lstsq</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">yref</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">       params (numpy.array): a paremeter vector</span>
<span class="sd">        X     (numpy.array): the design matrix (columns are predictor variables)</span>
<span class="sd">        yref  (numpy.array): the vector of y values</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. a gradient vector </span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    
<span class="k">def</span> <span class="nf">hessian_lstsq</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">       params (numpy.array): a paremeter vector</span>
<span class="sd">       X      (numpy.array): the design matrix (columns are predictor variables)</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. a hessian matrix </span>
<span class="sd">        </span>
<span class="sd">    @note: for linear regression, the hessian is constant for a given dataset, </span>
<span class="sd">    we keep the signature with params for homogeneity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>


    

<span class="c1"># It is strongly advised to output intermediate results during optimisation such as:</span>
<span class="c1">#  - the current value of the loss</span>
<span class="c1">#  - the current value of the iterates</span>
<span class="c1">#  - for gradient descent, the successive values of alpha at each iteration in case alpha is non constant in your implementation.</span>
<span class="c1"># ...    </span>

<span class="c1">################## NOTE #######################################################</span>
<span class="c1"># For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.</span>
<span class="c1"># @see https://en.wikipedia.org/wiki/Learning_rate</span>
<span class="c1">###############################################################################</span>
    
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">inv</span><span class="p">,</span><span class="n">norm</span>   


<span class="k">def</span> <span class="nf">gradient_descent_lstsq</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">loss_fnc</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">alpha0</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0   (numpy.array) : the initial iterate</span>
<span class="sd">        grad  (functional) : the gradient function</span>
<span class="sd">        alpha0 (float)     : the initial learning rate</span>
<span class="sd">        decay (float)      : a decay parameter for the scheduler</span>
<span class="sd">        epsilon (float)    : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The value of the last iterate</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    
<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses these values when calling your optimization functions</span>

<span class="n">hyper_lstsq</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.01</span><span class="p">},</span>
               <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>
    
    
<span class="k">def</span> <span class="nf">newton_lstsq</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">loss_fnc</span><span class="p">,</span><span class="n">grad</span><span class="p">,</span><span class="n">hessian</span><span class="p">,</span><span class="n">beta0</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0000001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x0   (numpy.array)  : the initial iterate</span>
<span class="sd">        loss_fnc(functional): the loss function</span>
<span class="sd">        grad  (functional)  : the gradient function</span>
<span class="sd">        hessian(functional) : the hessian function</span>
<span class="sd">        epsilon             : the precision of the solution</span>
<span class="sd">    Returns:</span>
<span class="sd">        numpy.array. The value of the last iterate</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    
    
    
<span class="k">def</span> <span class="nf">fit_lm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">predictor_lst</span><span class="p">,</span><span class="n">predicted</span><span class="p">,</span><span class="n">optim_method</span><span class="p">,</span><span class="n">alpha0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">beta0</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span><span class="n">decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span><span class="n">add_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given a data matrix, returns the estimates of the parameters using least squares estimation.</span>
<span class="sd">    Optimisation is performed either with the Newton method or the gradient method.</span>
<span class="sd">    </span>
<span class="sd">    Args: </span>
<span class="sd">        dataset (pandas.DataFrame) : a dataset</span>
<span class="sd">        predictor_lst        (list): a list of strings with the predictor variable names</span>
<span class="sd">        predicted             (str): the name of the predicted variable</span>
<span class="sd">        optim_method          (str): either &#39;grad_descent&#39; or &#39;newton&#39;</span>
<span class="sd">        epsilon             (float): epsilon value for the optimizer</span>
<span class="sd">    Returns :</span>
<span class="sd">        numpy.array: a vector with parameter estimates</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>



<span class="c1"># HYPERPARAMETERS</span>
<span class="c1">#  You may change the default hyperparameter values and set them to the values best suited for your implementation</span>
<span class="c1">#  The autograder uses theses values when calling your optimization functions</span>

<span class="n">lr_params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_descent&quot;</span><span class="p">:{</span><span class="s2">&quot;alpha0&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span><span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">},</span>
             <span class="s2">&quot;newton&quot;</span>          <span class="p">:{</span><span class="s1">&#39;beta0&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span><span class="s1">&#39;decay&#39;</span><span class="p">:</span><span class="mf">0.</span> <span class="p">,</span><span class="s1">&#39;epsilon&#39;</span><span class="p">:</span><span class="mf">0.00001</span><span class="p">}}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### TEST LINEAR REGRESSION (5pts)</span>


<span class="n">sols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>  <span class="mf">0.22282854</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.20726607</span><span class="p">,</span>  <span class="mf">0.52408311</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.24030739</span> <span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">fit_lm</span><span class="p">(</span><span class="n">iris</span><span class="p">,[</span><span class="s1">&#39;sepal width (cm)&#39;</span><span class="p">,</span><span class="s1">&#39;sepal length (cm)&#39;</span><span class="p">,</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">],</span><span class="s1">&#39;petal width (cm)&#39;</span><span class="p">,</span>
                <span class="n">alpha0</span>  <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;alpha0&#39;</span><span class="p">],</span>
                <span class="n">decay</span>   <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> 
                <span class="n">epsilon</span> <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;gradient_descent&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> 
                <span class="n">optim_method</span> <span class="o">=</span> <span class="s1">&#39;grad_descent&#39;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">sols</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>


<span class="n">result</span> <span class="o">=</span> <span class="n">fit_lm</span><span class="p">(</span><span class="n">iris</span><span class="p">,[</span><span class="s1">&#39;sepal width (cm)&#39;</span><span class="p">,</span><span class="s1">&#39;sepal length (cm)&#39;</span><span class="p">,</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">],</span><span class="s1">&#39;petal width (cm)&#39;</span><span class="p">,</span>
                <span class="n">beta0</span>  <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;beta0&#39;</span><span class="p">],</span>
                <span class="n">decay</span>   <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;decay&#39;</span><span class="p">],</span> 
                <span class="n">epsilon</span> <span class="o">=</span>  <span class="n">lr_params</span><span class="p">[</span><span class="s1">&#39;newton&#39;</span><span class="p">][</span><span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> 
                <span class="n">optim_method</span> <span class="o">=</span> <span class="s1">&#39;newton&#39;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">sols</span><span class="p">,</span><span class="n">atol</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="optim.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Optimization basics</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3">Exercise 3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4">Exercise 4</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-5">Exercise 5</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-6">Exercise 6</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Benoit Crabbé
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>