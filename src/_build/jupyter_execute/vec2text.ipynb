{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11aae7c9",
   "metadata": {},
   "source": [
    "# Applications to information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d819ff",
   "metadata": {},
   "source": [
    "We provide an example of the application of the notions seen on Euclidean spaces to the representation of words by high-dimensional vectors. We give some elementary algorithms that take advantage of these representations to compute shape similarities between words, and implement them using the *numpy* library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bc6cd",
   "metadata": {},
   "source": [
    "## What is information Retrieval\n",
    "\n",
    "Information retrieval is the task of finding material of unstructured nature that satisfies an information need from within a large collection of documents. For instance this may be retrieving textual content from a collection of books or from collections of written documents. We suppose the documents to be stored in digital form on computers.\n",
    "\n",
    "Then given a collection of documents, the Information Retrieval system stores them in such a way that we can issue a query for which the system returns the relevant documents. A well known such system is a web search engine that indexes the web pages and allows users to issue queries for which they get the relevant pages. \n",
    "\n",
    "### Boolean-based information retrieval\n",
    "\n",
    "In a boolean model we can exress queries by combining a set of *terms* with connectives *and*, *or* , *not*. A document is returned if the boolean query is true. For instance could ask : *Which plays of Shakespeare\n",
    "contain the terms 'Brutus' and 'Caesar' but not 'Calpurnia'. In general to get an efficient retrieval process we want to index the text collection. Indexing is an offline operation that creates a table where for each term we have documents where it occurs. Such a table (or matrix) is called a **term document** table and for instance it looks like :\n",
    "\n",
    "|         |Antony and Cleopatra| Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |\n",
    "|  ---    |         ---        |        ---    |       ---   |   ---  |    ---  |   ---   |\n",
    "| Antony  |           1        |       1       |       0     |    0   |    0    |   1     |\n",
    "| Brutus  |           1        |       1       |       0     |    1   |    0    |   0     |\n",
    "| Caesar  |           1        |       1       |       0     |    1   |    1    |   1     |\n",
    "|Calpurnia|           0        |       1       |       0     |    0   |    0    |   0     |\n",
    "|Cleopatra|           1        |       0       |       0     |    0   |    0    |   0     |\n",
    "| mercy   |           1        |       0       |       1     |    1   |    1    |   1     |\n",
    "| worser  |           1        |       0       |       1     |    1   |    1    |   0     |\n",
    "\n",
    "Given this table, the query returns two documents : *Antony and Cleopatra* and *Hamlet*.\n",
    "\n",
    "\n",
    "**Limits of boolean information retrieval** Boolean information retrieval is an all or nothing method. For instance a spelling error may prevent from finding errors. Another typical problem is the absence of ranking. All results that satisfy the query are returned.\n",
    "In case the set of such results is large, there is no built-in method for ranking them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475a46a",
   "metadata": {},
   "source": [
    "## Vector space based information retrieval\n",
    "\n",
    "The basic vector space model for information retrieval fixes the problem of naive information retrieval by naturally providing a ranking method. The basic idea amounts to represent the query and each document by real vectors rather than boolean vectors. The search method computes a similarity between vectors. The similarity scores can be used as criterion for ranking the results.\n",
    "\n",
    "Let' consider again our Shakespeare works, this time we use words counts rather than booleans to fill in the table:\n",
    "\n",
    "\n",
    "|         |Antony and Cleopatra| Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |\n",
    "|  ---    |         ---        |        ---    |       ---   |   ---  |    ---  |   ---   |\n",
    "| Antony  |           157        |       73       |       0     |    0   |    0    |   1     |\n",
    "| Brutus  |           4        |       157       |       0     |    2   |    0    |   0     |\n",
    "| Caesar  |           232        |       227       |       0     |    2   |   1    |   8     |\n",
    "|Calpurnia|           0        |       10       |       0     |    0   |    0    |   0     |\n",
    "|Cleopatra|           57        |       0       |       0     |    0   |    0    |   0     |\n",
    "| mercy   |           2        |       0       |       3     |    8   |    5    |   5     |\n",
    "| worser  |           2        |       0       |       1     |    1   |    1    |   0     |\n",
    "\n",
    "We can code a query such as \"Find those works where we have Antony and Caesar\" as a vector: \n",
    "\n",
    "|         | Query              |\n",
    "|  ---    |         ---        |     \n",
    "| Antony  |           1        |       \n",
    "| Brutus  |           0        |      \n",
    "| Caesar  |           1        |       \n",
    "|Calpurnia|           0        |      \n",
    "|Cleopatra|           0        |      \n",
    "| mercy   |           0        |      \n",
    "| worser  |           0        |\n",
    "\n",
    "Computing the dot product of the query with each document yields the following similarity scores:\n",
    "\n",
    "|         |Antony and Cleopatra| Julius Caesar | The Tempest | Hamlet | Othello | Macbeth |\n",
    "|  ---    |         ---        |        ---    |       ---   |   ---  |    ---  |   ---   |\n",
    "| scores  |          389       |       300     |       0     |    0   |    1    |   9     |\n",
    "\n",
    "\n",
    "This time the results are scored and we can use the scores to rank them. We will see in the next chapters how to improve this first and naive count based representation by taking into account natural language properties and some further notions of linear algebra.  \n",
    "\n",
    "**Summary** To summarize, the vector space model works on a collection of $n$ documents  $D = (\\mathbf{k}_1,v_1)\\ldots (\\mathbf{k}_n,v_n)$ where $\\mathbf{k}_i$ is the index vector and $v_i$ the corresponding value. The search procedure amounts to score the similarity $s_i$ between a query vector $\\mathbf{q}$ and each key $\\mathbf{k}_i$ and to sort the documents based on their score.\n",
    "The similarity score can be the dot product, $s_i = \\mathbf{q}^\\top\\mathbf{k}_i$, but most of the time the cosine is used as it ranges in $[-1,1]$:\n",
    "\n",
    "$$\n",
    "s_i = \\frac{\\mathbf{q}^\\top\\mathbf{k}_i}{||\\mathbf{q}||\\, ||\\mathbf{k}_i||}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## K-nearest neighbors and classification\n",
    "\n",
    "**Generalization** The method described for information retrieval can be generalized to a wide range of problems in Natural Language Processing as soon as we can design an encoding of linguistic symbols $S$ to vectors, that is a function $\\Phi: S \\mapsto\\mathbb{R}^n$ analog to the encoding from documents to vectors exemplified so far. \n",
    "\n",
    "For instance we can choose $\\Phi$ to encode a set of words on 26 dimensional vectors containing the character counts of each of the 26 letters of the alphabet found in the word. Using generalized euclidean geometry we can then measure angles and distances between those word vectors and issue queries. This is just a variant of what we illustrated earlier, now we assume a database $D = (\\mathbf{k}_1,v_1)\\ldots (\\mathbf{k}_n,v_n)$ whose keys $\\mathbf{k}_i = \\Phi(v_i)$ are the vectors encoded by the function $\\Phi$, and whose values $v_i$ are word strings\n",
    "\n",
    "\n",
    "### K-nearest neighbor algorithm (KNN)\n",
    "\n",
    "The KNN algorithm is a variant of the preceding scheme dedicated to classification. Given a query $q\\in S$\n",
    "the algorithm aims to classify $q$. A **classifier** is a function $f:S\\mapsto C$ mapping the input set $S$ to some output discrete set $C$.\n",
    "\n",
    "For instance we might wish to learn a function from a set of surnames to their nationalities\n",
    "\n",
    "|  Surname  | Nationality | \n",
    "|  ---    |         ---   |    \n",
    "| Campos |     Portuguese |\n",
    "| Barros | Portuguese |\n",
    "| Silva  | Portuguese |\n",
    "| Aoki   | Japanese |\n",
    "| Daishi | Japanese |\n",
    "| Nagano | Japanese |\n",
    "\n",
    "\n",
    "A trivial solution to the issue is to enumerate the function by creating some sort of dictionary.\n",
    "But in many cases we cannot practically enumerate the full set $S$, here the set of surnames. There may be new surnames appearing with time for instance.\n",
    "\n",
    "A classifier is a function that is able to map elements from $S$ to a class in $C$ without memorizing how to map each element of $S$. Rather the function is *learned* from a subset of $S$ paired with actual classes from $C$, the training set, and it is expected that the function *generalizes* its predictions to those elements of $S$ that have not been seen in the training set. For instance a training set might be the Surname,nationality set of examples given above and we expect the classification function to be able to predict the class for unseen input, e.g. *hiroshige* to be Japanese.\n",
    "\n",
    "\n",
    "The KNN is an instance of such a classifier. We assume a dataset $D = (s_1,c_1)\\ldots (s_n,c_n)$ whose elements are couples from surnames to nationality classes. We assume a vectorization function $\\Phi:S\\mapsto \\mathbb{R}^n$ mapping symbols to vectors. Given a query $q\\in S$, we compute for each $s_i$:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    \\text{sim}(q,s_i) &= \\frac{\\mathbf{q}^\\top \\mathbf{k}_i}{||\\mathbf{q}|| ||\\mathbf{k}_i||}\\\\\n",
    "     \\mathbf{q}   &= \\Phi(q)\\\\\n",
    "     \\mathbf{k}_i &= \\Phi(s_i)\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "then for some constant $K$ the algorithm creates the set $N = \\text{K-argmax}_{(s_i,c_i)\\in D}  \\text{sim}\\quad(\\mathbf{q},\\mathbf{k}_i)$ of K nearest neighbours and assigns to the query $q$ the class which is the most frequent among the $c_i$ in $N$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### An example in Python\n",
    "\n",
    "We consider as dataset of people's names given as couples made of strings and nationalities in Python. \n",
    "\n",
    "We first provide an example on how to encode strings to character vectors and to create a database whose keys are vectors and values are nationality classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783031bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value P \n",
      "\n",
      "key [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value P \n",
      "\n",
      "key [2. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value P \n",
      "\n",
      "key [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      "value P \n",
      "\n",
      "key [0. 0. 0. 0. 2. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "value P \n",
      "\n",
      "key [1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "value J \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=120)\n",
    "\n",
    "\n",
    "\n",
    "alpha  = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "def word2vec(word):\n",
    "    \n",
    "    counts   = Counter(word)\n",
    "    vec      = np.zeros(len(alpha))        \n",
    "    for idx, char in enumerate(alpha):\n",
    "        if char in counts:\n",
    "            vec[idx] = counts[char]\n",
    "    \n",
    "    return vec\n",
    "\n",
    "\n",
    "dataset = [('campos','P'),\n",
    "           ('barros','P'),\n",
    "           ('machado','P'),\n",
    "           ('silva','P'),\n",
    "           ('henriques','P'),\n",
    "           ('aoki','J'),\n",
    "           ('asano','J'),\n",
    "           ('daishi','J'),\n",
    "           ('hayakawa','J'),\n",
    "           ('nagano','J')]\n",
    "\n",
    "knn_dataset = [ (word2vec(word),cls)  for (word,cls) in dataset]\n",
    "\n",
    "#prints the 6 first lines\n",
    "for key, value in knn_dataset[:6]:\n",
    "    print('key',key)\n",
    "    print('value',value,'\\n')\n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705aa718",
   "metadata": {},
   "source": [
    "Given a properly encoded dataset we design the KNN algorithm and we test it on a new surname token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ef9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction : J, majority vote : 2/3\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "\n",
    "def similarity(x,y):\n",
    "    \n",
    "    return np.dot(x,y) / ( norm(x) * norm(y) )\n",
    "    \n",
    "    \n",
    "def knn_classify(query,data,K=3):\n",
    "    vquery = word2vec(query)\n",
    "    scores = [ (similarity(vquery,key),cls) for key,cls in data]  \n",
    "    scores.sort(reverse=True)\n",
    "    knn    = scores[:K]\n",
    "    counts = Counter(cls for _ ,cls in knn)    \n",
    "    cls,c  = counts.most_common(1)[0]\n",
    "    return cls,c\n",
    "    \n",
    "K = 3\n",
    "cls,c = knn_classify('hiroshige',knn_dataset,K)\n",
    "print(f'prediction : {cls}, majority vote : {c}/{K}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781c114",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Conjecture why we prefer to use the angular measure (cosine similarity) or the euclidean distance when comparing query and keys ?\n",
    "**hint**: compare the case where you take the measure between two texts $x$ and $y$ with a case where the measure is taken between $x$ and a version of $y$ where the content is replicated multiple times.\n",
    "\n",
    "2. Test the `knn_classify` function with other queries and identify its potential problems. For instance, what happens in case of ties ? how to choose the value for parameter K ?\n",
    "\n",
    "3. Provide a Python implementation of a (toy) vector-based twitter search engine using the tweets provided below. \n",
    "This requires to design a tweet to vector function and a query system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26dc987",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"@mattcutts have google profiles stopped showing up in searches? cant see them anymore\",\n",
    "    \"@ArunBasilLal I love Google Translator too ! :D Good day mate !\",\n",
    "    \"reading on my new Kindle2!\",\n",
    "    \"My Kindle2 came and I LOVE it! :)\",\n",
    "    'LOVING my new Kindle2.  Named her Kendra in case u were wondering. The \"cookbook\" is THE tool cuz it tells u all the tricks!  Best gift EVR!',\n",
    "    \"The real AIG scandal / http://bit.ly/b82Px\",\n",
    "    \"Any twitter to aprs apps yet?\",\n",
    "    \"45 Pros You Should Be Following on Twitter - http://is.gd/sMbZ\",\n",
    "    \"Obama is quite a good comedian! check out his dinner speech on CNN :) very funny jokes.\",\n",
    "    'Barack Obama shows his funny side \" &gt;&gt; http://tr.im/l0gY !! Great speech..',\n",
    "    \"I like this guy : ' Barack Obama shows his funny side \\\" &gt;&gt; http://tr.im/l0gY !!\",\n",
    "    \"Obama's speech was pretty awesome last night! http://bit.ly/IMXUM\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}