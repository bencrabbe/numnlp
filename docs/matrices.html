
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Matrices &#8212; Numerical methods in NLP (foundations)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'matrices';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Applications to graph analysis : the PageRank algorithm" href="graphs.html" />
    <link rel="prev" title="Applications to information retrieval" href="vec2text.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logoUPC.png" class="logo__image only-light" alt="Numerical methods in NLP (foundations) - Home"/>
    <img src="_static/logoUPC.png" class="logo__image only-dark pst-js-only" alt="Numerical methods in NLP (foundations) - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Numerical methods for Natural Language Processing
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vectors.html">Vectors and vector spaces</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="numpy_exercises.html">Numpy exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="euclid.html">The euclidean space</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Euclid_exercises.html">Euclidean spaces : exercises</a></li>

<li class="toctree-l2"><a class="reference internal" href="euclid_appendix.html">Appendix to Euclidean spaces</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="vec2text.html">Applications to information retrieval</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphs.html">Applications to graph analysis : the PageRank algorithm</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="stats.html">Applications to statistical analysis : latent semantic analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="simple">
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="optim.html">Optimization basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="optim_exercises.html">Optimization exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="systems.html">Systems of linear equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="constrained.html">Constrained optimization</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="http://github.com/bencrabbe/numnlp/issues/new?title=Issue%20on%20page%20%2Fmatrices.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button"
   title="Open an issue"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/matrices.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Matrices</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-do-matrices-come-from">Where do matrices come from ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-transpose">Matrix transpose</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-addition">Matrix addition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-scalar-product">Matrix scalar product</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product">Matrix vector product</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-a-mathbf-x"><span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{x}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-x-top-mathbf-a"><span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{A}\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-matrix-multiplication">Matrix matrix multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-maps">Linear maps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#square-matrices">Square matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-from-geometry">Examples from geometry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagonal-matrices">Diagonal matrices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-inverse">Matrix inverse</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvectors-and-eigenvalues">Eigenvectors and eigenvalues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-diagonalization">Matrix Diagonalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-theorem">Spectral theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="matrices">
<h1>Matrices<a class="headerlink" href="#matrices" title="Link to this heading">#</a></h1>
<p>A <strong>matrix</strong> is a rectangular array of numbers, for instance</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  0 &amp; 1 &amp;-2.3 &amp;0.1 \\
  1.3 &amp;4 &amp;-0.1 &amp;0\\
4.1 &amp;-1 &amp;0 &amp;1.7 
\end{bmatrix}
\end{split}\]</div>
<p>A matrix <span class="math notranslate nohighlight">\(m\times n\)</span> is characterized by its size, that is the number
<span class="math notranslate nohighlight">\(m\)</span> of rows and <span class="math notranslate nohighlight">\(n\)</span> of columns, and the set its value are from.
In particular, the set of matrices of real numbers of size <span class="math notranslate nohighlight">\(m\times n\)</span> will be denoted <span class="math notranslate nohighlight">\(\mathbb{R}^{m\times n}\)</span>.
Here the matrix has size <span class="math notranslate nohighlight">\(3\times 4\)</span> since it has 3 rows and 4
columns. To further specify its content (real numbers) we write that the matrix
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{3\times 4}\)</span>. By convention, we write matrices with uppercase bold characters, such
as <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> or <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>. By contrast we use lowercase bold
characters for vectors and plain characters for scalars.</p>
<p>A matrix is a <strong>square matrix</strong> if its size is <span class="math notranslate nohighlight">\(n\times n\)</span> otherwise
it is a <strong>rectangular matrix</strong>. A <strong>line matrix</strong> is matrix with size
<span class="math notranslate nohighlight">\(1\times n\)</span> and a <strong>column matrix</strong> is  a matrix with size <span class="math notranslate nohighlight">\(n\times
1\)</span>.  Thus the line matrix and the column matrix given here</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
   13 &amp; -2 &amp; 77
 \end{bmatrix}
 \qquad
  \begin{bmatrix}
   13 \\ -2 \\ 77
 \end{bmatrix}
\end{split}\]</div>
<p>both encode a vector, but viewed as matrices they have different sizes
and their algebric properties are
different.</p>
<p>The <strong>elements</strong> (or <strong>coefficients</strong>) of a matrix are denoted by the indices <span class="math notranslate nohighlight">\(i,j\)</span> which designate the row
and column respectively. If we denote <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> the matrix above,
then <span class="math notranslate nohighlight">\(A_{1,2}\)</span> denotes the scalar element 4. By convention, row and column numbering
starts at 1. Two matrices are said to be equal if they have the same size and the corresponding entries contain the same elements.</p>
<p>Sometimes to get more compact notations we write a matrix by
enumerating its column vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
|        &amp;        |         &amp;      |        &amp;      |       \\
    \mathbf{c}_1 &amp; \mathbf{c}_2 &amp;\mathbf{c}_3&amp;\mathbf{c}_4\\
        	|      &amp;         |        &amp;      |         &amp;      |           
  \end{bmatrix}
\end{split}\]</div>
<p>and sometimes we write a matrix by enumerating its row vectors:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
    \mathbf{r}_1 \\
	\mathbf{r}_2 \\
	\mathbf{r}_3 	
  \end{bmatrix}
\end{split}\]</div>
<section id="where-do-matrices-come-from">
<h2>Where do matrices come from ?<a class="headerlink" href="#where-do-matrices-come-from" title="Link to this heading">#</a></h2>
<p>Matrices likely arise in data analysis, image representation, graph representation and as mathematical operators.</p>
<p><strong>Data analysis</strong> In data analysis, measures are most of the time arranged as array of numbers, in other words matrices.
Here are the first few lines of a classic data set reporting measures from Iris flowers across species:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Sepal Length</p></th>
<th class="head"><p>Sepal Width</p></th>
<th class="head"><p>Petal Length</p></th>
<th class="head"><p>Petal Width</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>5.1</p></td>
<td><p>3.5</p></td>
<td><p>1.4</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>4.9</p></td>
<td><p>3.0</p></td>
<td><p>1.4</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>4.7</p></td>
<td><p>3.2</p></td>
<td><p>1.3</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>4.6</p></td>
<td><p>3.1</p></td>
<td><p>1.5</p></td>
<td><p>0.2</p></td>
</tr>
</tbody>
</table>
</div>
<p>where the following picture provides hints on how to interpret these measures.</p>
<img alt="iris" class="align-center" src="_images/iris-machinelearning.png" />
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>By contrast with many scientific fields, most of the time linguistic data does not come
as tables of measured values but rather as raw text. As illustrated in the previous chapter,
tables such as term-document matrices are the output of a function mapping text to a numerical representation.</p>
</div>
<p><strong>Image representation</strong> Matrices can be used to code images. A grayscale image is represented by a matrix whose integer coefficents range from  0 to 255 .
Here is a matrix coding an image:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
255 &amp; 255 &amp; 255 &amp; 255 &amp; 0   &amp; 0   &amp; 0   &amp;   0  &amp; 255 &amp; 255 &amp; 255 &amp; 255  \\
255 &amp; 255 &amp;  0    &amp;   0   &amp;45&amp;0    &amp;   0 &amp; 0    &amp;   0   &amp; 0     &amp; 255 &amp; 255  \\
255 &amp; 0     &amp; 45 &amp; 45 &amp;45&amp;45&amp;   0 &amp; 0    &amp;   0   &amp; 0     &amp; 0    &amp; 255  \\
255 &amp; 0     &amp; 45 &amp; 230 &amp;45&amp;45&amp;45&amp;45  &amp;   0   &amp; 0     &amp; 0    &amp; 255  \\
0     &amp; 45  &amp; 230 &amp; 230&amp;230&amp;45&amp;45&amp;45  &amp;  45 &amp; 0     &amp; 0    &amp; 0  \\
0     &amp; 45  &amp; 45&amp; 230&amp;45&amp;45&amp;45&amp;45  &amp;  45 &amp; 0     &amp; 0    &amp; 0 \\
0     &amp; 45  &amp; 45&amp; 45&amp;45&amp; 0   &amp;  0  &amp;  0    &amp; 45  &amp; 0      &amp; 0     &amp; 0  \\ 
0     &amp;    0   &amp;45 &amp;45&amp; 0    &amp;230&amp; 180&amp;0     &amp;    0    &amp;  0    &amp;   0   &amp;   0\\
0      &amp;  230&amp;  0   &amp;   0 &amp;  0   &amp;180&amp;180&amp;   0   &amp;    0    &amp;    0  &amp;  230 &amp;  0 \\
255 &amp;  0     &amp;  230&amp;230&amp;230&amp; 0   &amp;   0   &amp; 230&amp;230    &amp;230  &amp;   0    &amp; 255\\
255 &amp;  0     &amp;  230&amp;230&amp;230&amp; 230 &amp; 230&amp; 230&amp;230    &amp;230  &amp;   0    &amp; 255\\
255 &amp;  255  &amp;  0&amp; 0&amp;230&amp; 230 &amp; 230&amp; 230&amp;       0    &amp; 0  &amp;  255    &amp; 255\\
255 &amp;  255  &amp; 255&amp;255 &amp;0&amp; 0 &amp; 0&amp; 0&amp;    255   &amp; 255  &amp;  255    &amp; 255\\
\end{bmatrix}
\end{split}\]</div>
<p>and here is the corresponding image visualisation.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/321418db63ba98a02aacb0a335356317194920d251ac407ff65a47bb0d63828e.png" src="_images/321418db63ba98a02aacb0a335356317194920d251ac407ff65a47bb0d63828e.png" />
</div>
</div>
<p><strong>Graph data representation</strong> Graphs are another important source of matrices. In natural language processing they come from networks such as web pages
and their hyperlink relation, such as semantic networks like Wordnet, such as social networks and their friend or follow relations etc. A graph such as this one</p>
<img alt="sample-graph" class="align-center" src="_images/graph.png" />
<p>can be represented by its <strong>adjacency matrix</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}  = \begin{bmatrix}
  0&amp;1&amp;1\\
  1&amp;0&amp;0\\
  0&amp;1&amp;1
  \end{bmatrix}
\end{split}\]</div>
<p>In general, for a graph <span class="math notranslate nohighlight">\(G=\langle V , E\rangle\)</span> with <span class="math notranslate nohighlight">\(n = |V|\)</span> nodes, and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> an adjacency matrix <span class="math notranslate nohighlight">\(n\times n\)</span>, the element <span class="math notranslate nohighlight">\(A_{ij}\)</span> is set to 1 if <span class="math notranslate nohighlight">\((i,j) \in E\)</span> and is set to 0 otherwise.</p>
<p>As <strong>linear maps</strong>. In this case matrices are viewed as mathematical operators mapping source vectors to transformed destination vectors.
Examples of such linear maps are scaling or rotation matrices among others.</p>
</section>
<section id="matrix-transpose">
<h2>Matrix transpose<a class="headerlink" href="#matrix-transpose" title="Link to this heading">#</a></h2>
<p>In the next few sections, we review classical algebric operations involving matrices.
Let <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> be an <span class="math notranslate nohighlight">\(m\times n\)</span> matrix, its transpose <span class="math notranslate nohighlight">\(\mathbf{A}^\top\)</span>
is a matrix <span class="math notranslate nohighlight">\(n\times m\)</span> where each entry a has value <span class="math notranslate nohighlight">\(A^\top_{ij} = A_{ji}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} = 
\begin{bmatrix}
    A_{11} &amp; \cdots &amp; A_{1n}\\
    \vdots &amp; \ddots  &amp; \vdots\\
    A_{m1} &amp; \cdots &amp; A_{mn}\\
  \end{bmatrix}
  \qquad
  \mathbf{A}^\top = 
\begin{bmatrix}
    A_{11} &amp; \cdots &amp; A_{1m}\\
    \vdots &amp; \ddots  &amp; \vdots\\
    A_{n1} &amp; \cdots &amp; A_{nm}\\
  \end{bmatrix}
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathbf{A} =
  \begin{bmatrix}
    1 &amp; 2 &amp; 3\\
    -1&amp;0 &amp;1
  \end{bmatrix}
  \qquad
   \mathbf{A}^\top =
    \begin{bmatrix}
      1  &amp;-1\\
      2  &amp; 0 \\
      3  &amp; 1 
  \end{bmatrix}
\end{split}\]</div>
</div>
<p>Note that if we transpose again the transposed matrix, we obtain the original matrix: <span class="math notranslate nohighlight">\((\mathbf{A}^\top)^\top = \mathbf{A}\)</span>.
There is a remarkable case: the symmetric matrix. A <strong>symmetric matrix</strong> is a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
such that <span class="math notranslate nohighlight">\(\mathbf{A} = \mathbf{A}^\top\)</span>, as illustrated below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  1&amp; 0 &amp;1&amp;1\\
  0&amp;0&amp; 0&amp;0\\
  1&amp;0 &amp;1&amp;1\\
  1&amp;0&amp;1&amp;0
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="matrix-addition">
<h2>Matrix addition<a class="headerlink" href="#matrix-addition" title="Link to this heading">#</a></h2>
<p>The <strong>addition</strong> of two matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>
of size <span class="math notranslate nohighlight">\(m\times n\)</span> consists in producing a new matrix whose elements
are the sum of the corresponding elements in the original matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 \begin{bmatrix}
    a_{11}&amp;\cdots &amp; a_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    a_{m1}&amp;\cdots &amp; a_{mn}
  \end{bmatrix}
  +
 \begin{bmatrix}
    b_{11}&amp;\cdots &amp; b_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    b_{m1}&amp;\cdots &amp; b_{mn}
  \end{bmatrix}
=
\begin{bmatrix}
    a_{11}+b_{11}&amp;\cdots &amp; a_{1n}+b_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    a_{m1}+b_{m1}&amp;\cdots &amp; a_{mn}+b_{mn}
  \end{bmatrix}\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
    0&amp;1\\
    7&amp;-2\\
    1&amp;0
  \end{bmatrix}
  +
  \begin{bmatrix}
    10&amp;-1\\
    5&amp;-3\\
    0&amp;0
  \end{bmatrix}
  =
   \begin{bmatrix}
    10&amp;0\\
    12&amp;-5\\
    1&amp;0
  \end{bmatrix}
\end{split}\]</div>
</div>
<p>Matrix addition is commutative <span class="math notranslate nohighlight">\(\mathbf{A}+\mathbf{B} = \mathbf{B}+\mathbf{A}\)</span>, associative <span class="math notranslate nohighlight">\((\mathbf{A}+\mathbf{B}) + \mathbf{C}= \mathbf{A}+(\mathbf{B} + \mathbf{C})\)</span>
has neutral element <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> and the transposition of a sum is the sum of the transpositions:   <span class="math notranslate nohighlight">\((\mathbf{A} +\mathbf{B})^\top = \mathbf{A}^\top+\mathbf{B}^\top\)</span></p>
</section>
<section id="matrix-scalar-product">
<h2>Matrix scalar product<a class="headerlink" href="#matrix-scalar-product" title="Link to this heading">#</a></h2>
<p>Given a scalar <span class="math notranslate nohighlight">\(a\)</span> and  a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, the matrix scalar product is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a\, 
\begin{bmatrix}
    x_{11}&amp;\cdots &amp; x_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    x_{m1}&amp;\cdots &amp; x_{mn}
  \end{bmatrix}
=
\begin{bmatrix}
    ax_{11}&amp;\cdots &amp; ax_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    ax_{m1}&amp;\cdots &amp; ax_{mn}
  \end{bmatrix}
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  -2
  \begin{bmatrix}
    1 &amp; 9\\
    6&amp;-3\\
    0&amp;-1
  \end{bmatrix}
  =
   \begin{bmatrix}
    -2 &amp; -18\\
    -12&amp;6\\
    0&amp;2
  \end{bmatrix}
\end{split}\]</div>
</div>
<p>Matrix scalar product is distributive for scalar addition  <span class="math notranslate nohighlight">\((a+b) \mathbf{A} = a\mathbf{A}+b\mathbf{A}\)</span>
and associative for scalar multiplication  <span class="math notranslate nohighlight">\((ab) \mathbf{A} = a(b\mathbf{A})\)</span></p>
</section>
<section id="matrix-vector-product">
<h2>Matrix vector product<a class="headerlink" href="#matrix-vector-product" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(S\)</span> be a set of numbers, the matrix vector product supposes a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\in S^{m\times n}\)</span> and a vector <span class="math notranslate nohighlight">\(\mathbf{x}\in S^n\)</span>
and yields a vector in <span class="math notranslate nohighlight">\(S^m\)</span>.</p>
<p>This operation is not commutative and breaks down in two cases. Either <span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{x}\)</span> is the product where the vector is at the right of the matrix
or <span class="math notranslate nohighlight">\( \mathbf{x}^\top \mathbf{A} \)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}^\top\)</span> is a line vector on the left of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span></p>
<section id="mathbf-a-mathbf-x">
<h3><span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{x}\)</span><a class="headerlink" href="#mathbf-a-mathbf-x" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
    A_{11}&amp;\cdots &amp; A_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    A_{m1}&amp;\cdots &amp; A_{mn}
  \end{bmatrix}
\begin{bmatrix}
x_1\\
\vdots\\
x_n
\end{bmatrix}
=
\begin{bmatrix}
    x_1 A_{11} + \cdots + x_n A_{1n}\\
    \vdots\\
    x_1 A_{m1}+ \cdots + x_n A_{mn}
  \end{bmatrix}
\end{split}\]</div>
<p>There are several ways to view and interpret this operation. First as <strong>a parallel dot product</strong> where the result is seen as the dot product of each line of the matrix
with the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
    \mathbf{a}_1\\
    \vdots   \\
    \mathbf{a}_m
  \end{bmatrix}
\mathbf{x}
=
\begin{bmatrix}
  \mathbf{a}_1 \cdot \mathbf{x}\\
    \vdots\\
  \mathbf{a}_m\cdot \mathbf{x}\\
  \end{bmatrix}
\end{split}\]</div>
<p>We can also view the operation as a <strong>linear combination</strong> of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with the matrix columns:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
     |       &amp;  |   &amp;  |\\
    \mathbf{a}_1 &amp;\cdots &amp;     \mathbf{a}_m\\
	|  &amp;    | &amp; |
  \end{bmatrix}
\begin{bmatrix}
x_1\\
\vdots\\
x_n
\end{bmatrix}
=
\begin{bmatrix}
x_1  \mathbf{a}_1 +\cdots +x_n \mathbf{a}_n
\end{bmatrix}
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Here is the computation performed as a parallel dot product:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  7 &amp; 0&amp;-1\\
  0 &amp; -2 &amp;-1
\end{bmatrix}
\begin{bmatrix}
  1\\
  -5\\
  3
 \end{bmatrix} 
 =
 \begin{bmatrix}
  7+0-3\\
 0 + 10-3
\end{bmatrix}
=
\begin{bmatrix}
  4\\
 7
\end{bmatrix}
\end{split}\]</div>
<p>and now as a linear combination:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  7 &amp; 0&amp;-1\\
  0 &amp; -2 &amp;-1
\end{bmatrix}
\begin{bmatrix}
  1\\
  -5\\
  3
 \end{bmatrix} 
 =
1
 \begin{bmatrix}
  7\\
 0 
\end{bmatrix}
-5
 \begin{bmatrix}
  0\\
 -2
\end{bmatrix}
+3
\begin{bmatrix}
  -1\\
 -1
\end{bmatrix}
=
\begin{bmatrix}
 4\\
 7
\end{bmatrix}
\end{split}\]</div>
</div>
</section>
<section id="mathbf-x-top-mathbf-a">
<h3><span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{A}\)</span><a class="headerlink" href="#mathbf-x-top-mathbf-a" title="Link to this heading">#</a></h3>
<p>In this second case, the operation takes the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
x_1 &amp; \cdots &amp; x_m
\end{bmatrix}
\begin{bmatrix}
    A_{11}&amp;\cdots &amp; A_{1n}\\
    \vdots&amp;\ddots &amp; \vdots\\
    A_{m1}&amp;\cdots &amp; A_{mn}
  \end{bmatrix}
  =
  \begin{bmatrix}
   x_1 A_{11} + \ldots +  x_1 A_{m1}  &amp;\cdots &amp;  x_m A_{1n}  + \ldots +  x_m A_{mn}  
  \end{bmatrix}
\end{split}\]</div>
<p>Again the operation can be seen as a <strong>parallel dot product</strong> between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and each column of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x}^\top
\begin{bmatrix}
| &amp; | &amp; | \\ 
 \mathbf{a}_1&amp;\ldots&amp;\mathbf{a}_n\\
| &amp; | &amp; |  
  \end{bmatrix}
  =
  \begin{bmatrix}
 \mathbf{x}^\top \cdot \mathbf{a}_1&amp;\cdots&amp;\mathbf{x}^\top \cdot \mathbf{a}_n
  \end{bmatrix}
\end{split}\]</div>
<p>and it can also be viewed as a <strong>linear combination</strong> between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and each line of  <span class="math notranslate nohighlight">\(\mathbf{A}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
x_1 &amp; \cdots &amp; x_m
\end{bmatrix}
\begin{bmatrix}
    \mathbf{a}_1\\
	\vdots\\
	\mathbf{a}_m
  \end{bmatrix}
  =
  \begin{bmatrix}
   x_1 \mathbf{a}_{1} + \cdots + x_m \mathbf{a}_n
  \end{bmatrix}
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>The operation viewed as a paralell dot product:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  1 &amp;  -5&amp;  3 
 \end{bmatrix} 
\begin{bmatrix}
  7 &amp; 0\\
  0 &amp; -2 \\
  -1&amp;-1
\end{bmatrix}
= 
\begin{bmatrix}
7 + 0 -3 &amp; 0 +10 -3
\end{bmatrix}
=
\begin{bmatrix}
4 &amp; 7
\end{bmatrix}
\end{split}\]</div>
<p>and now as a linear combination:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
  1 &amp;  -5&amp;  3 
 \end{bmatrix} 
\begin{bmatrix}
  7 &amp; 0\\
  0 &amp; -2 \\
  -1&amp;-1
\end{bmatrix}
= 
 1
 \begin{bmatrix}
 7 &amp; 0
 \end{bmatrix}
 -5 
 \begin{bmatrix}
0  &amp; -2
\end{bmatrix}
+3
 \begin{bmatrix}
 -1&amp;-1
 \end{bmatrix}
 =
\begin{bmatrix}
4 &amp; 7
\end{bmatrix}
\end{split}\]</div>
</div>
</section>
</section>
<section id="matrix-matrix-multiplication">
<h2>Matrix matrix multiplication<a class="headerlink" href="#matrix-matrix-multiplication" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> be two matrices of sizes <span class="math notranslate nohighlight">\(m\times p\)</span> and <span class="math notranslate nohighlight">\(p\times n\)</span> respectively,
the multiplication of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> generates the matrix <span class="math notranslate nohighlight">\(\mathbf{AB}\)</span> of size <span class="math notranslate nohighlight">\(m \times n\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\underbrace{
\begin{bmatrix}
\mathbf{a}_1\\
\vdots \\
\mathbf{a}_m
\end{bmatrix}
}_{m\times p}
\underbrace{
\begin{bmatrix}
| &amp; | &amp; |\\
\mathbf{b}_1 &amp; \cdots &amp; \mathbf{b}_n\\
| &amp; | &amp; |
\end{bmatrix}
}_{p\times n}
=
\underbrace{
\begin{bmatrix}
\mathbf{a}^\top_1 \mathbf{b}_1&amp;\cdots&amp;\mathbf{a}^\top_1\mathbf{b}_n\\
\vdots&amp;\ddots&amp;\vdots \\
\mathbf{a}^\top_m\mathbf{b}_1&amp;\cdots&amp;\mathbf{a}^\top_m\mathbf{b}_n
\end{bmatrix}
}_{m\times n}
\end{split}\]</div>
<p>In other words each coefficient <span class="math notranslate nohighlight">\(c_{ij}\)</span> of the resulting matrix <span class="math notranslate nohighlight">\(\mathbf{AB}\)</span> is computed from the dot product of line <span class="math notranslate nohighlight">\(\mathbf{a}_i\)</span> and column <span class="math notranslate nohighlight">\(\mathbf{b}_j\)</span>.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \mathbf{A} =
  \begin{bmatrix}
    12 &amp; 21 &amp; 0\\
    -1 &amp; 0   &amp; 2
  \end{bmatrix}
  \quad
  \mathbf{B} =
  \begin{bmatrix}
    0   &amp;-2&amp; 1 &amp; 0\\
   -1 &amp; 0  &amp; -1&amp;0\\
    1  &amp;  0  &amp;2  &amp; 1
  \end{bmatrix}
  \qquad
  \mathbf{AB}
  =
   \begin{bmatrix}
    -21 &amp; -24 &amp; -9 &amp; 0 \\
       2  &amp; 2    &amp;  3   &amp;  2
  \end{bmatrix}
\end{split}\]</div>
</div>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>The vector dot product and the matrix vector product are specific cases of matrix matrix products.
The vector dot product involves multiplying a line vector <span class="math notranslate nohighlight">\(\mathbf{x}^\top\)</span> of size <span class="math notranslate nohighlight">\(1\times p\)</span> with a column vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> of size <span class="math notranslate nohighlight">\(p\times 1\)</span>
yielding a matrix of size <span class="math notranslate nohighlight">\(1\times 1\)</span> which is a scalar.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
x_1 &amp; \cdots &amp; x_p
\end{bmatrix}
\begin{bmatrix}
y_1 \\ \cdots \\ x_p
\end{bmatrix}
=
\mathbf{x}_1^\top \mathbf{y}
\end{split}\]</div>
<p>and it should be quite clear that the matrix vector product involves the multiplication of matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> of size <span class="math notranslate nohighlight">\(m\times p\)</span> with a column vector
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of size <span class="math notranslate nohighlight">\(p\times 1\)</span> (or a line vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of size <span class="math notranslate nohighlight">\(1\times p\)</span> with a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> of size <span class="math notranslate nohighlight">\(p\times n\)</span>)</p>
</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> , <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> be matrices. In general <strong>matrix multiplication is not commutative</strong> but <strong>matrix multiplication is
associative</strong> provided that the sizes of the matrices follow a pattern of the form    <span class="math notranslate nohighlight">\(\mathbf{A} : m\times p\)</span>, <span class="math notranslate nohighlight">\(\mathbf{B} : p\times q\)</span> et  <span class="math notranslate nohighlight">\(\mathbf{C} : q\times n\)</span>.
If matrices follow the pattern   <span class="math notranslate nohighlight">\(\mathbf{A} : m\times p\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{B} : p\times n\)</span>,  <span class="math notranslate nohighlight">\(\mathbf{C} : p\times n\)</span>, then <strong>matrix
multiplication distributes over matrix addition</strong>: <span class="math notranslate nohighlight">\(\mathbf{A}(\mathbf{B}+\mathbf{C}) = \mathbf{AB}+\mathbf{BC}\)</span>.
And finally <strong>the transpose of a matrix matrix product is the product of the transposes of each matrix in reverse order</strong> <span class="math notranslate nohighlight">\((\mathbf{AB})^\top = \mathbf{B}^\top \mathbf{A}^\top\)</span></p>
</section>
<section id="linear-maps">
<h2>Linear maps<a class="headerlink" href="#linear-maps" title="Link to this heading">#</a></h2>
<p>Matrices can be viewed as an implementation for mathematical operators generalizing scalar linear functions to vectors.
For scalars from some set <span class="math notranslate nohighlight">\(S\)</span> we define a linear function as <span class="math notranslate nohighlight">\(f(x) = ax\)</span> performing the mapping <span class="math notranslate nohighlight">\(f:S\mapsto S\)</span>.
Linear maps generalizes this notion to vectors and define maps of the form <span class="math notranslate nohighlight">\(F:S^n \mapsto S^m\)</span> from an input vector space to an output vector space.
Thus for a vector <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1\ldots x_n)\)</span> a linear map outputs a vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
f_1(\mathbf{x})\\
\vdots\\
f_m(\mathbf{x})
\end{bmatrix}
\end{split}\]</div>
<p>in such a way that each <span class="math notranslate nohighlight">\(f_i(\mathbf{x})\)</span> is linear wrt its input, that is <span class="math notranslate nohighlight">\(f_i(\mathbf{x}) = a_1x_1 + \ldots a_n x_n\)</span> where the <span class="math notranslate nohighlight">\(a_j\)</span> are constants.
It should now be obvious that matrices implement linear maps and that matrix vector products instanciate their evaluation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}\mathbf{x}=
\begin{bmatrix}
a_{11}x_1 &amp; \cdots &amp; a_{1n} x_n\\
\vdots &amp; \ddots &amp; \vdots\\
a_{m1}x_1 &amp; \cdots &amp; a_{mn}x_n
\end{bmatrix}
\end{split}\]</div>
<p>To check that a map <span class="math notranslate nohighlight">\(F\)</span> is linear, we check that <span class="math notranslate nohighlight">\(F\)</span> verifies:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F(\mathbf{x}+\mathbf{y}) = F(\mathbf{x})+F(\mathbf{y})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(F(a\mathbf{x}) = a F(\mathbf{x})\)</span></p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>We can verify that <span class="math notranslate nohighlight">\(f(x) = x^2\)</span>
is not linear.
This can be seen by considering the multiplicative case:
<span class="math notranslate nohighlight">\(a f(x) = a x^2\)</span> whereas <span class="math notranslate nohighlight">\(f(a x^2) = a^2 x^2\)</span>.</p>
<p>On the other hand, <span class="math notranslate nohighlight">\(f(x) = 2x\)</span> is linear. We verify both cases:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
f(a+b) &amp;= f(a)+f(b)\\
2(a+b) &amp;= 2a + 2b\\
2a + 2b &amp;= 2a+2b
\end{align}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
f(ax) &amp;= a f(x)\\
2 ax  &amp;= a 2x 
\end{align}
\end{split}\]</div>
</div>
<div class="admonition-note-linearity admonition">
<p class="admonition-title">Note (Linearity)</p>
<p>Why do we care about linearity ? In general problems that can be reduced or approximated reasonably well by linear functions or linear maps
are very substantially easier to solve than non linear problems.
In general linear problems can be solved by analytical and/or efficient methods while non linear problems
require more involved developments such as numerical simulations.</p>
</div>
</section>
<section id="square-matrices">
<h2>Square matrices<a class="headerlink" href="#square-matrices" title="Link to this heading">#</a></h2>
<p>A <strong>square matrix</strong> is a matrix whose size is <span class="math notranslate nohighlight">\(n\times n\)</span>.
Among matrices, square matrices hold a special position: they are used to express many common linear maps,
we can compute their <em>determinant</em> and we can also compute an <em>inverse</em> for this class of matrices. And finally
we can compute their <em>eigenvectors</em> with their associated <em>eigenvalues</em>.</p>
<section id="examples-from-geometry">
<h3>Examples from geometry<a class="headerlink" href="#examples-from-geometry" title="Link to this heading">#</a></h3>
<p>We start by providing a few examples of square matrices used as linear maps for 2D geometrical transformations: scaling, rotation and projection.
For illustration purposes, we assume a data set of <span class="math notranslate nohighlight">\(n\)</span> vectors stored as columns in a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{2\times n}\)</span>.
Each such vector is interpreted as a point in the plane. We represent as blue point in figure <a class="reference internal" href="#geom-map"><span class="std std-numref">Fig. 1</span></a>
the vectors <span class="math notranslate nohighlight">\((1,1)\)</span>, <span class="math notranslate nohighlight">\((2,2)\)</span> and <span class="math notranslate nohighlight">\((3,3)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}=
\begin{bmatrix}
1 &amp; 2  &amp; 3 \\
1 &amp; 2  &amp; 3
\end{bmatrix}
\end{split}\]</div>
<figure class="align-center" id="geom-map">
<img alt="geometric linear maps" src="_images/geom_map.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Common Geometric transforms as linear maps</span><a class="headerlink" href="#geom-map" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The first transformation performed is the <strong>scaling of vectors</strong>. This operation is generally performed using a <strong>diagonal matrix</strong>
of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
\lambda_1 &amp; 0\\
0 &amp; \lambda_2
\end{bmatrix}
\end{split}\]</div>
<p>In figure <a class="reference internal" href="#geom-map"><span class="std std-numref">Fig. 1</span></a> , the orange points are generated from the blue points using the linear map expressed by the matrix
<span class="math notranslate nohighlight">\(\mathbf{S}=\frac{1}{5}\mathbf{I}\)</span>, that is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{SX}=\begin{bmatrix}
  \frac{1}{5}&amp;0\\
  0              &amp; \frac{1}{5}
  \end{bmatrix}
  \begin{bmatrix}
1 &amp; 2  &amp; 3 \\
1 &amp; 2  &amp; 3
\end{bmatrix}
=
\begin{bmatrix}
0.2 &amp; 0.4  &amp; 0.6 \\
0.2 &amp; 0.4  &amp; 0.6
\end{bmatrix}
\end{split}\]</div>
<p>Note that scaling need not be uniform, we illustrate this case with the green dots.
and a diagonal matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> where the <span class="math notranslate nohighlight">\(\lambda_i\)</span> have different values.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{DX}=\begin{bmatrix}
  \frac{1}{5}&amp;0\\
  0              &amp; \frac{5}{4}
  \end{bmatrix}
  \begin{bmatrix}
1 &amp; 2  &amp; 3 \\
1 &amp; 2  &amp; 3
\end{bmatrix}
=
\begin{bmatrix}
0.2 &amp; 0.4  &amp; 0.6 \\
1.25 &amp; 2.5  &amp; 2.75
\end{bmatrix}
\end{split}\]</div>
<p>To generate a <strong>rotation</strong>, one instanciates a rotation matrix, that is a matrix of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{R} = 
\begin{bmatrix}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{bmatrix}
\end{split}\]</div>
<p>The vectors resulting from the computation <span class="math notranslate nohighlight">\(\mathbf{RX}\)</span> are illustrated with red dots on  figure <a class="reference internal" href="#geom-map"><span class="std std-numref">Fig. 1</span></a>.
We fixed <span class="math notranslate nohighlight">\(\theta = \pi/2\)</span> (angles are expressed in radians). Finally we illustrate a <strong>projection</strong> map. Such  a map can be expressed with a matrix
of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{P} = 
\begin{bmatrix}
1 &amp; 0\\
0 &amp; 0
\end{bmatrix}
\end{split}\]</div>
<p>This is a diagonal matrix where some <span class="math notranslate nohighlight">\(\lambda_i\)</span> on the diagonal are equal to 0. The result of the projection
<span class="math notranslate nohighlight">\(\mathbf{PX}\)</span> is illustrated with purple dots in  figure <a class="reference internal" href="#geom-map"><span class="std std-numref">Fig. 1</span></a>.</p>
</section>
<section id="diagonal-matrices">
<h3>Diagonal matrices<a class="headerlink" href="#diagonal-matrices" title="Link to this heading">#</a></h3>
<p>Among square matrices, diagonal matrices enjoy some appealing mathematical properties: computation of determinants, inverses, eigenvalues and eigenvectors
are substantially easier than for the general class of square matrices.</p>
<p>The <strong>diagonal</strong> of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the subset of elements <span class="math notranslate nohighlight">\(a_{i,j}\)</span> whose coordinates satisfy <span class="math notranslate nohighlight">\(i=j\)</span>.
A <strong>diagonal matrix</strong> is a matrix whose elements are null except those of the diagonal that may be non null. Thus the scaling matrices and the projection matrix given
earlier are all instances of diagonal matrices. Here is another example of diagonal matrix
with size <span class="math notranslate nohighlight">\(4\times 4\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 2 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0&amp; 0  &amp; 0 &amp;-3
\end{bmatrix}
\end{split}\]</div>
<p>A diagonal matrix whose elements of the diagonal are all equal to 1 is called the <strong>identity matrix</strong> and written as <span class="math notranslate nohighlight">\(\mathbf{I}\)</span>.</p>
</section>
<section id="determinant">
<h3>Determinant<a class="headerlink" href="#determinant" title="Link to this heading">#</a></h3>
<p>The determinant of a matrix is the scalar <span class="math notranslate nohighlight">\(|\mathbf{A}|\)</span> (or det(<span class="math notranslate nohighlight">\(\mathbf{A}\)</span>) ) that can be computed for any square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.
The determinant is of crucial importance when computing the inverse of a matrix.  A <strong>singular matrix</strong>  <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>  is a matrix whose <span class="math notranslate nohighlight">\(\det(\mathbf{A}) = 0\)</span>
and it is therefore non invertible. In case <span class="math notranslate nohighlight">\(\det(\mathbf{A}) \not= 0\)</span>, <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is invertible.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> be a <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix with coefficients:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
a &amp; b\\
c &amp; d
\end{bmatrix}
\end{split}\]</div>
<p>The determinant of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is given by the formula:</p>
<div class="math notranslate nohighlight">
\[
\det(\mathbf{A}) = ad - bc
\]</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Here is how to compute the determinant of a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{vmatrix}
    3 &amp;-3\\
    1 &amp; 1
  \end{vmatrix}
= 3\times 1 - (-3 \times 1) = 6
\end{split}\]</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For the general case of <span class="math notranslate nohighlight">\(n\times n\)</span> matrices (with <span class="math notranslate nohighlight">\(n &gt; 2\)</span>) the computations become more involved as <span class="math notranslate nohighlight">\(n\)</span> increases significantly and it is generally
practically easier to rely on numerical methods. There are a few special cases however where the computation of the determinant remains simple even with large <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>For any value of <span class="math notranslate nohighlight">\(n\)</span>, the determinant of upper triangular (<span class="math notranslate nohighlight">\(\mathbf{U}\)</span>), lower triangular matrices (<span class="math notranslate nohighlight">\(\mathbf{L}\)</span>) and diagonal matrices
is equal to the product of the elements of the diagonal.</p>
<p>An <strong>upper triangular</strong> matrix is a matrix whose elements below the diagonal are 0 (<span class="math notranslate nohighlight">\(a_{ij} = 0\)</span> if <span class="math notranslate nohighlight">\(j &gt; i\)</span>).
A <strong>lower triangular</strong> matrix is a matrix whose elements above the diagonal are 0 (<span class="math notranslate nohighlight">\(a_{ij} = 0\)</span> if <span class="math notranslate nohighlight">\(i&lt;j\)</span>). Thus diagonal matrices
are both lower triangular and lower triangular. Numerical methods for computing determinants take advantage of these special cases for solving the more
general case.</p>
</div>
<div class="admonition-note-condition-number admonition">
<p class="admonition-title">Note (condition number)</p>
<p>Trying to inverse a matrix with a null determinant is to be thought as the counterpart as attempting to divide a real number by 0.
When a matrix has a determinant close to 0, it becomes a tricky object to compute with.</p>
<p>The condition number <span class="math notranslate nohighlight">\(\kappa(\mathbf{A})\)</span> is a number that indicates how close a matrix is from being singular and thus likely to create numerical instabilities.
It relies on the <strong>Frobenius Norm</strong>, which is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
||\mathbf{A}|| = 
\begin{Vmatrix}
a_{11}&amp;\cdots&amp;a_{1n}\\
\vdots &amp; \ddots &amp; \vdots\\
a_{m1} &amp; \cdots &amp; a_{mn}
\end{Vmatrix}
=
\sqrt{\sum_{ij} a^2_{ij}}
\end{split}\]</div>
<p>The <strong>condition number</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\kappa(\mathbf{A}) = ||\mathbf{A} || \, ||\mathbf{A}^{-1} ||
\]</div>
<p>The higher the condition number the more likely the matrix is likely to be numerically instable, the lower the better</p>
</div>
</section>
<section id="matrix-inverse">
<h3>Matrix inverse<a class="headerlink" href="#matrix-inverse" title="Link to this heading">#</a></h3>
<p>The matrix inverse provides a form of division for matrices. The inverse <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span>, of the square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
satisfies the relation:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{AA}^{-1} = \mathbf{I}
\]</div>
<p>That is <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> multiplied by its inverse yields the identity matrix much like a real <span class="math notranslate nohighlight">\(a\)</span> multiplied by its inverse <span class="math notranslate nohighlight">\(\frac{1}{a}\)</span> yields <span class="math notranslate nohighlight">\(1\)</span>.
For the case of a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} = 
\begin{bmatrix}
a &amp; b\\
c &amp; d
\end{bmatrix}
\end{split}\]</div>
<p>the inverse is found analytically with the formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}^{-1} = \frac{1}{\det(\mathbf{A})} 
\begin{bmatrix}
d &amp; -b\\
-c &amp; a
\end{bmatrix}
\end{split}\]</div>
<p>that is by multiplying a scalar, the inverse of the determinant, with the comatrix of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>.
As can be seen, the computation fails in case <span class="math notranslate nohighlight">\(\det(\mathbf{A}) = 0\)</span>. The matrix is therefore non invertible when its determinant is null.</p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
3 &amp; -1\\
2 &amp;  0 
\end{bmatrix}^{-1}
=
\frac{1}{2}
\begin{bmatrix}
0&amp;1\\
-2&amp;3
\end{bmatrix}
=
\begin{bmatrix}
0&amp;\frac{1}{2}\\
-1&amp;\frac{3}{2}
\end{bmatrix}
\end{split}\]</div>
<p>We can check that <span class="math notranslate nohighlight">\(\mathbf{AA}^{-1} = \mathbf{I}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
3 &amp; -1\\
2 &amp;  0 
\end{bmatrix}
\begin{bmatrix}
0&amp;\frac{1}{2}\\
-1&amp;\frac{3}{2}
\end{bmatrix}
=
\begin{bmatrix}
1 &amp;0\\
0&amp;1
\end{bmatrix}
\end{split}\]</div>
</div>
<p>We can easily prove the formula for the <span class="math notranslate nohighlight">\(2\times 2\)</span> case:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \begin{bmatrix}
      1&amp;0\\
      0&amp;1
    \end{bmatrix}
       &amp;=
         \frac{1}{ad-bc}
    \begin{bmatrix}
      a&amp;b\\
     c&amp;d
    \end{bmatrix}
    \begin{bmatrix}
      d&amp;-b\\
     -c&amp;a
    \end{bmatrix}\\
       &amp;=  \frac{1}{ad-bc}
         \begin{bmatrix}
      ad-bc  &amp;-ab+ba\\
      cd-cd  &amp;-bc+ad
    \end{bmatrix}\\
    &amp;=  \begin{bmatrix}
      1&amp;0\\
      0&amp;1
    \end{bmatrix}
  \end{align}
\end{split}\]</div>
<p>For cases where <span class="math notranslate nohighlight">\(n &gt;2\)</span> while analytical methods still exist, it is generally more convenient to rely on numerical
methods for computing the matrix inverse such as the Gauss Jordan method for instance.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Matrices with colinear columns are non invertible. We can illustrate this case with the following example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
    3 &amp; 6\\
    1 &amp; 2
  \end{bmatrix}^{-1}
  = \frac{1}{\color{red}\mathbf{6-6}}
 \begin{bmatrix}
    2 &amp;-6\\
    -1 &amp; 3
  \end{bmatrix}
\end{split}\]</div>
<p>The determinant is null and the inversion is not possible.</p>
</div>
<div class="admonition-diagonal-matrices admonition">
<p class="admonition-title">Diagonal matrices</p>
<p>In case <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a diagonal matrix of size <span class="math notranslate nohighlight">\(n\times n\)</span>, its inverse is easy to compute and it neither requires a determinant nor a comatrix.
Let <span class="math notranslate nohighlight">\(\lambda_1 \ldots \lambda_n\)</span> be the elements of the diagonal of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span>
is a diagonal matrix with diagonal elements <span class="math notranslate nohighlight">\(\frac{1}{\lambda_1} \ldots \frac{1}{\lambda_n}\)</span>. A diagonal matrix is non
invertible if at least one of its diagonal element is zero.</p>
</div>
<div class="admonition-orthonormal-matrices admonition">
<p class="admonition-title">Orthonormal matrices</p>
<p>Two vectors are orthonormal if they are orthogonal and they are unit vectors: they have unit norm.
An <strong>orthonormal matrix</strong> is a matrix such that its rows and columns are orthonormal vectors.</p>
<p>Orthonormal matrices are another case of “easy to invert” matrices since an orthonormal matrix verifies the property:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{AA}^\top = \mathbf{A}^\top\mathbf{A} = \mathbf{I}
\]</div>
<p>That is for this family of matrices we have that <span class="math notranslate nohighlight">\(\mathbf{A}^{-1} =  \mathbf{A}^\top \)</span></p>
</div>
</section>
<section id="eigenvectors-and-eigenvalues">
<h3>Eigenvectors and eigenvalues<a class="headerlink" href="#eigenvectors-and-eigenvalues" title="Link to this heading">#</a></h3>
<p>We start by providing a visual introduction to eigenvectors and eigenvalues from <a class="reference internal" href="#eigen-figs"><span class="std std-numref">Fig. 2</span></a>.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<figure class="align-default" id="eigen-figs">
<img alt="eigen-figures" class="shadow bg-primary" src="_images/f176a07164a3cd4098b60fcf0394856211ddcf22a51e9f11c4ff906d96d477fc.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">The transformation of a diagonal matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> (left) and the transformation of a symmetric matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> (right) with their respective eigenvectors</span><a class="headerlink" href="#eigen-figs" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
<p>The figure displays the effect of mapping a matrix  <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{2\times 8}\)</span> of 2D vectors (in blue)
with a diagonal matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span>  on the left and with a symmetrix matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> on the right. The resulting vectors,
respectively <span class="math notranslate nohighlight">\(\mathbf{DX}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{SX}\)</span>, are plotted in red.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{D} =
\begin{bmatrix}
4&amp;0\\
0&amp;2
\end{bmatrix}
\qquad
\mathbf{S}=
\begin{bmatrix}
3 &amp; \frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
\end{split}\]</div>
<p>When comparing input and output vector in both plots we can observe that most vectors are both rotated and scaled. Some of them are not, they are only scaled, and are illustrated with black arrows: these are the eigenvectors of each matrix.
The scale of each eigenvector is given by a corresponding eigenvalue. These scaled eigenvectors are provided in gray.</p>
<p>As can be seen from the illustration, eigenvectors are important as they form a basis for the output vectors where the scaling can be performed by a diagonal matrix.
For matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> the basis for the input and output vectors does not change. But for matrix <span class="math notranslate nohighlight">\(\mathbf{S}\)</span>, the eigenvectors provide a basis from which the transformation
is just a scaling. In other words, within this new basis the transformation is given by a diagonal matrix.</p>
<p><strong>Eigenvectors and Eigenvalues</strong> A non null vector that right multiplies a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and whose resulting vector is only scaled by the matrix is an <strong>eigenvector</strong> of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Ax} = \lambda \mathbf{x}
\]</div>
<p>the scaling scalar value <span class="math notranslate nohighlight">\(\lambda\)</span> is the corresponding <strong>eigenvalue</strong>.</p>
<p><strong>Computing eigenvectors and eigenvalues</strong>
Computing analytically the eigenvectors and eigenvalues of an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix is only doable when <span class="math notranslate nohighlight">\(n\)</span> is small.
We illustrate the method for a <span class="math notranslate nohighlight">\(2\times 2\)</span> matrix. We start by rewriting:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbf{Ax} &amp;= \lambda \mathbf{x}\\
\mathbf{Ax} -\lambda \mathbf{x} &amp;= \mathbf{0}\\
(\mathbf{A} - \lambda \mathbf{I})\,\mathbf{x} &amp; = \mathbf{0}
\end{align}
\end{split}\]</div>
<p>To satisfy this last equality, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> cannot be null by definition of eigenvectors, therefore <span class="math notranslate nohighlight">\( \mathbf{A} - \lambda \mathbf{I} \)</span> must be null
and if it is null it is non invertible which means that the determinant must be null, that is: <span class="math notranslate nohighlight">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span>.</p>
<p>For the <span class="math notranslate nohighlight">\(2\times 2\)</span> case, we can solve for <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\det\left(
\begin{bmatrix}
a &amp; b\\
c &amp; d
\end{bmatrix}
-
\begin{bmatrix}
\lambda &amp; 0\\
0 &amp; \lambda
\end{bmatrix}
\right) = 0
\end{split}\]</div>
<p>For each eigenvalue, that is for each value of <span class="math notranslate nohighlight">\(\lambda\)</span>, we can search its eigenvector from the equality  <span class="math notranslate nohighlight">\( (\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =
\mathbf{0}\)</span></p>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Let us consider first the diagonal matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} = 
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 2
\end{bmatrix}
\end{split}\]</div>
<p>Setting <span class="math notranslate nohighlight">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span> and solving yields:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\det\left(
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 2
\end{bmatrix}
-
\begin{bmatrix}
\lambda &amp; 0\\
0 &amp; \lambda
\end{bmatrix}
\right) &amp;= 0
\\
\det\left(
\begin{bmatrix}
4-\lambda &amp; 0\\
0 &amp; 2-\lambda
\end{bmatrix}
\right) &amp;= 0\\
(4-\lambda)(2-\lambda) - 0 &amp;= 0
\end{align}
\end{split}\]</div>
<p>Thus we have that either <span class="math notranslate nohighlight">\(\lambda = 4\)</span> or <span class="math notranslate nohighlight">\(\lambda = 2\)</span>. We have two distinct eigenvalues.
For each such eigenvalue we get the corresponding eigenvector by solving  <span class="math notranslate nohighlight">\( (\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =
\mathbf{0}\)</span>, that is for <span class="math notranslate nohighlight">\(\lambda = 4\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\left(
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 2
\end{bmatrix}
-
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 4
\end{bmatrix}
\right)
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
0 &amp; 0\\
0 &amp; -2
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
0 x_1 + 0 x_2\\
0 x_1 -2 x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}
\end{align}
\end{split}\]</div>
<p>We conclude that <span class="math notranslate nohighlight">\(x_2 = 0\)</span> and that there are an infinite number of solutions for <span class="math notranslate nohighlight">\(x_1\)</span>. By convention we keep the solution for which
<span class="math notranslate nohighlight">\(||\mathbf{x}|| = 1\)</span> that is <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}=
\begin{bmatrix}
1\\
0
\end{bmatrix}
\)</span></p>
<p>Now solving for <span class="math notranslate nohighlight">\(\lambda = 2\)</span> we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\left(
\begin{bmatrix}
4 &amp; 0\\
0 &amp; 2
\end{bmatrix}
-
\begin{bmatrix}
2 &amp; 0\\
0 &amp; 2
\end{bmatrix}
\right)
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
2 &amp; 0\\
0 &amp; 0
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
2 x_1 + 0 x_2\\
0 x_1 + 0 x_2
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}
\end{align}
\end{split}\]</div>
<p>We conclude that <span class="math notranslate nohighlight">\(x_1 = 0\)</span> and we choose <span class="math notranslate nohighlight">\(x_2 = 1\)</span> to get an eigenvector with unit norm. The second eigenvector
is therefore <span class="math notranslate nohighlight">\(\mathbf{x}^{(2)} = \begin{bmatrix} 0 \\ 1\end{bmatrix}\)</span>. Note that for diagonal matrix, it always true that the diagonal coefficients
are eigenvalues and that we can get one hot unit eigenvectors.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example</p>
<p>Let us consider now the symmetric matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} =
\begin{bmatrix}
3&amp;\frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
\end{split}\]</div>
<p>Setting <span class="math notranslate nohighlight">\(\det(\mathbf{A} - \lambda \mathbf{I}) = 0\)</span> and solving yields:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\det\left(
\begin{bmatrix}
3 &amp; \frac{1}{2}\\
\frac{1}{2} &amp; 3
\end{bmatrix}
-
\begin{bmatrix}
\lambda &amp; 0\\
0 &amp; \lambda
\end{bmatrix}
\right) &amp;= 0
\\
\det\left(
\begin{bmatrix}
3-\lambda &amp; \frac{1}{2}\\
\frac{1}{2} &amp; 3-\lambda
\end{bmatrix}
\right) &amp;= 0\\
(3-\lambda)^2 - \frac{1}{4} &amp;= 0\\
9 - 6\lambda + \lambda^2 - \frac{1}{4} &amp;=  0\\
 \lambda^2  - 6\lambda +\frac{35}{4} &amp;= 0 
\end{align}
\end{split}\]</div>
<p>Solving the second order polynomial yields <span class="math notranslate nohighlight">\(\lambda = \frac{5}{2}\)</span> or <span class="math notranslate nohighlight">\(\lambda = \frac{7}{2}\)</span>.</p>
<p>For each  eigenvalue we get the corresponding eigenvector by solving  <span class="math notranslate nohighlight">\( (\mathbf{A} - \lambda\mathbf{I})\mathbf{x} =
\mathbf{0}\)</span>, that is for <span class="math notranslate nohighlight">\(\lambda = \frac{5}{2}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\left(
\begin{bmatrix}
3 &amp; \frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
-
\begin{bmatrix}
\frac{5}{2}&amp;0\\
0&amp;\frac{5}{2}
\end{bmatrix}
\right)
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
\frac{1}{2}&amp;\frac{1}{2}\\
\frac{1}{2}&amp;\frac{1}{2}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
\frac{1}{2}x_1+\frac{1}{2}x_2\\
\frac{1}{2}x_1+\frac{1}{2}x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}
\end{align}
\end{split}\]</div>
<p>There is again an infinity of solutions, each of which requires <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> to have the same value but opposite signs. To stick with the convention
that the eigenvector has unit norm we can use <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}=\begin{bmatrix} \sqrt{\frac{1}{2}}\\-\sqrt{\frac{1}{2}}  \end{bmatrix}\)</span></p>
<p>Now solving for <span class="math notranslate nohighlight">\(\lambda = \frac{7}{2}\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\left(
\begin{bmatrix}
3 &amp; \frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
-
\begin{bmatrix}
\frac{7}{2}&amp;0\\
0&amp;\frac{7}{2}
\end{bmatrix}
\right)
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
-\frac{1}{2}&amp;\frac{1}{2}\\
\frac{1}{2}&amp;-\frac{1}{2}
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
-\frac{1}{2} x_1 +\frac{1}{2} x_2\\
\frac{1}{2} x_1 -\frac{1}{2} x_2
\end{bmatrix}
&amp;=
\begin{bmatrix}
0\\
0
\end{bmatrix}
\end{align}
\end{split}\]</div>
<p>All solutions have the property that <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> have the same value. To stick with the convention
that the eigenvector has unit norm we can use <span class="math notranslate nohighlight">\(\mathbf{x}^{(2)}=\begin{bmatrix} \sqrt{\frac{1}{2}}\\\sqrt{\frac{1}{2}}  \end{bmatrix}\)</span></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>For a square matrix of size <span class="math notranslate nohighlight">\(n\times n\)</span> there are at most <span class="math notranslate nohighlight">\(n\)</span> eigenvalues/eigenvectors</p>
</div>
</section>
<section id="matrix-diagonalization">
<h3>Matrix Diagonalization<a class="headerlink" href="#matrix-diagonalization" title="Link to this heading">#</a></h3>
<p>Diagonal matrices make life easy…  There is interest in expressing a given linear map as a diagonal matrix to make computations easier.
In this section we show how to factorize some non diagonal matrices as a product of three matrices, including
a diagonal matrix.  This is called matrix diagonalization.
Matrix diagonalization actually provides methods to simplify computations such as computing matrix power.
But it also has interest in data analysis when <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a “data” matrix or in graph analysis when <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a “graph” matrix as illustrated in other chapters.</p>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> of size <span class="math notranslate nohighlight">\(n\times n\)</span> is diagonalizable if it can be factorized as a product of 3 matrices:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A} = \mathbf{P} \mathbf{D} \mathbf{P}^{-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is a matrix whose columns are eigenvectors and <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is a diagonal matrix whose elements<br />
<span class="math notranslate nohighlight">\(\lambda_1\ldots \lambda_n\)</span> are eigenvalues.</p>
<p>To see that it is true, assume <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is the square matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{P} = 
\begin{bmatrix}
| &amp; &amp;|\\
\mathbf{v_1}&amp;\cdots &amp; \mathbf{v}_n\\
| &amp; &amp;|\\
\end{bmatrix}
\end{split}\]</div>
<p>then the equality relating <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> with the diagonal matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{AP} = \mathbf{PD}
\]</div>
<p>holds if and only if the columns <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span>of <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> are eigenvectors and the <span class="math notranslate nohighlight">\(\lambda_i\)</span> are their corresponding eigenvalues.
This is shown by expanding the equality:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mathbf{A} 
\begin{bmatrix}
| &amp; &amp; |\\
\mathbf{v}_1&amp;\cdots &amp; \mathbf{v}_n\\
| &amp; &amp; |  \\
\end{bmatrix}
&amp;=
\begin{bmatrix}
| &amp; &amp;|\\
\mathbf{v}_1&amp;\cdots &amp; \mathbf{v}_n\\
| &amp; &amp;|\\
\end{bmatrix}
\begin{bmatrix}
\lambda_1 &amp;\cdots&amp;0\\
\vdots &amp;\ddots &amp; \vdots\\
0 &amp; \cdots &amp; \lambda_n
\end{bmatrix}\\
\begin{bmatrix}
| &amp; &amp; |\\
\mathbf{Av}_1&amp;\cdots &amp; \mathbf{Av}_n\\
| &amp; &amp; |  \\
\end{bmatrix}
&amp;=
\begin{bmatrix}
| &amp; &amp; |\\
\lambda_1\mathbf{v}_1&amp;\cdots &amp; \lambda_n\mathbf{v}_n\\
| &amp; &amp; |  \\
\end{bmatrix}
\end{align}
\end{split}\]</div>
<p>thus for each <span class="math notranslate nohighlight">\(\mathbf{Av}_i = \lambda_i\mathbf{v}_i\)</span>, <span class="math notranslate nohighlight">\( \lambda_i\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span>
are respectively eigenvalues and eigenvectors. We further require <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> to be invertible and we get either
the diagonalization formula <span class="math notranslate nohighlight">\(\mathbf{D} = \mathbf{P}^{-1}\mathbf{A}\mathbf{P}\)</span> or the decomposition used elsewhere in this text:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A} = \mathbf{PDP}^{-1}
\]</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Not all matrices are diagonalizable. Diagonalization requires <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> to be invertible, that is to have a non zero determinant
or, stated differently, the vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1\ldots \mathbf{v}_n\)</span> must be linearly independent.</p>
<p>In case the matrix is diagonalizable, observe that there are multiple possible diagonalizations for a given matrix, typically depending on the ordering of the eigenvalues.
The usual convention is to order them from greater to lower, thus <span class="math notranslate nohighlight">\(\lambda_1 &gt; \lambda_2 &gt; \ldots &gt; \lambda_n\)</span></p>
</div>
</section>
<section id="spectral-theorem">
<h3>Spectral theorem<a class="headerlink" href="#spectral-theorem" title="Link to this heading">#</a></h3>
<p>In the case of <strong>symmetric</strong> matrices there is a stronger form of factorization given by the spectral theorem:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A} = \mathbf{PDP}^\top
\]</div>
<p>Every symmetric matrix is diagonalizable and the columns of the <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> matrix are guaranteed to be orthogonal eigenvectors.</p>
<div class="tip admonition">
<p class="admonition-title">Exemple</p>
<p>To get an intuition of the factorization described here, let us consider the product of the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
with the data <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{A} =
\begin{bmatrix}
3&amp;\frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{2\times n}\)</span> be the set of 2D coordinates depicted in Figure <a class="reference internal" href="#spectral"><span class="std std-numref">Fig. 3</span></a>.
Thus given the spectral theorem we can express the product as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}\mathbf{X} = \mathbf{PDP}^\top \mathbf{X}
\]</div>
<p>Results from previous exercises allow the substitution:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
3&amp;\frac{1}{2}\\
\frac{1}{2}&amp;3
\end{bmatrix}
\mathbf{X}
=
\begin{bmatrix}
\sqrt{\frac{1}{2}}&amp;\sqrt{\frac{1}{2}}\\
\sqrt{\frac{1}{2}}&amp;-\sqrt{\frac{1}{2}}
\end{bmatrix}
\begin{bmatrix}
\frac{7}{2}&amp;0\\
0&amp;\frac{5}{2}
\end{bmatrix}
\begin{bmatrix}
\sqrt{\frac{1}{2}}&amp;\sqrt{\frac{1}{2}}\\
\sqrt{\frac{1}{2}}&amp;-\sqrt{\frac{1}{2}}
\end{bmatrix}^\top
\mathbf{X}
\end{split}\]</div>
<p>We illustrate in figure <a class="reference internal" href="#spectral"><span class="std std-numref">Fig. 3</span></a> each step of the computation</p>
</div>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<figure class="align-default" id="spectral">
<a class="shadow bg-primary reference internal image-reference" href="_images/ddf28a7c0a41d3608a6c7ec85ba22e3fea1cc2d725a65b3ae58098c2373a1b6d.png"><img alt="spectral decomposition" class="shadow bg-primary" src="_images/ddf28a7c0a41d3608a6c7ec85ba22e3fea1cc2d725a65b3ae58098c2373a1b6d.png" style="width: 1000px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">The transformation of the matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> through the decomposition of <span class="math notranslate nohighlight">\(\mathbf{AX}\)</span>. From left to right. Given the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{P}^\top\mathbf{X}\)</span> first rotates the data, then it scales it with the diagonal matrix <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> and finally rotates it again with <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> yielding the final result</span><a class="headerlink" href="#spectral" title="Link to this image">#</a></p>
</figcaption>
</figure>
</div>
</div>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Given the matrices and vectors:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} =
    \begin{bmatrix}
      1&amp;-1&amp;2\\
      0&amp;3&amp;-1
    \end{bmatrix}
    \qquad
     \mathbf{B} =
    \begin{bmatrix}
      0&amp;4&amp;-2\\
      -4&amp;-3&amp;0
    \end{bmatrix}
	    \qquad
\mathbf{x} = 
\begin{bmatrix}
2\\
1\\
0
\end{bmatrix}
    \qquad
\mathbf{y} = 
\begin{bmatrix}
0\\1\\2
\end{bmatrix}
    \qquad
\mathbf{z} =
\begin{bmatrix}
0\\0
\end{bmatrix}
\end{split}\]</div>
<p>Evaluate:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Ax}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Bx}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Az}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{B}^\top\mathbf{x}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{By}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}^\top\mathbf{z}\)</span></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Given matrices <span class="math notranslate nohighlight">\(\mathbf{A},\mathbf{B}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
such that:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split} 
    \mathbf{A} =
    \begin{bmatrix}
      2&amp;0&amp;-1\\
      0&amp;3&amp;-2\\
      0&amp;0&amp;-1
    \end{bmatrix}
    \qquad
     \mathbf{B} =
    \begin{bmatrix}
      0&amp;0&amp;2\\
      -1&amp;3&amp;-1\\
      0&amp;-3&amp;0
    \end{bmatrix}
    \qquad
    \mathbf{x}=
    \begin{bmatrix}
      2\\0\\-1
    \end{bmatrix}
	
\end{split}\]</div>
<p>Evaluate the following expressions:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\mathbf{AB})\mathbf{x}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}(\mathbf{B}\mathbf{x})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{A}\mathbf{x}^\top\mathbf{B}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{B}\mathbf{A}\mathbf{x}\)</span></p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>In python, plot the following matrix as 2D vectors:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} = 
\begin{bmatrix}
1 &amp;2&amp;3\\
1 &amp;2 &amp;3
\end{bmatrix}
\end{split}\]</div>
<p>and the matrix <span class="math notranslate nohighlight">\(\mathbf{SRX}\)</span> with <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>
defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{S} = 
\begin{bmatrix}
0.5 &amp; 0\\
0    &amp;0.5
\end{bmatrix}
\qquad
\mathbf{R} = 
\begin{bmatrix}
\cos\frac{\pi}{2} &amp; -\sin\frac{\pi}{2} \\
\sin\frac{\pi}{2}     &amp;\cos\frac{\pi}{2} 
\end{bmatrix}
\end{split}\]</div>
<p>explain in words what the operation <span class="math notranslate nohighlight">\(\mathbf{SR}\)</span> represents.</p>
<ol class="arabic simple" start="4">
<li><p>Check that the following matrices are inverse of each other:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
7 &amp; 3\\
2 &amp; 1
\end{bmatrix}
\quad
\begin{bmatrix}
1 &amp; -3\\
-2 &amp; 7
\end{bmatrix}
\end{split}\]</div>
<ol class="arabic simple" start="5">
<li><p>For exercises 1. 2. and 4. implement them with <code class="docutils literal notranslate"><span class="pre">numpy</span></code> (find the
relevant functions in the library)  and check that
the results match those you get by manual resolution.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="vec2text.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Applications to information retrieval</p>
      </div>
    </a>
    <a class="right-next"
       href="graphs.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Applications to graph analysis : the PageRank algorithm</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-do-matrices-come-from">Where do matrices come from ?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-transpose">Matrix transpose</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-addition">Matrix addition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-scalar-product">Matrix scalar product</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-vector-product">Matrix vector product</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-a-mathbf-x"><span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{x}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-x-top-mathbf-a"><span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{A}\)</span></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-matrix-multiplication">Matrix matrix multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-maps">Linear maps</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#square-matrices">Square matrices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-from-geometry">Examples from geometry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#diagonal-matrices">Diagonal matrices</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#determinant">Determinant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-inverse">Matrix inverse</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigenvectors-and-eigenvalues">Eigenvectors and eigenvalues</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-diagonalization">Matrix Diagonalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-theorem">Spectral theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Benoit Crabbé
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>