

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Optimization basics &#8212; Numerical methods in NLP (foundations)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optim';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Optimization exercises" href="optim_exercises.html" />
    <link rel="prev" title="Applications to statistical analysis : latent semantic analysis" href="stats.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Numerical methods in NLP (foundations) - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Numerical methods in NLP (foundations) - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Numerical methods for Natural Language Processing
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="vectors.html">Vectors and vector spaces</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="numpy_exercises.html">Numpy exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="euclid.html">The euclidean space</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Euclid_exercises.html">Euclidean spaces : exercises</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="vec2text.html">Applications to information retrieval</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrices.html">Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="graphs.html">Applications to graph analysis : the PageRank algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="stats.html">Applications to statistical analysis : latent semantic analysis</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Optimization basics</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="optim_exercises.html">Optimization exercises</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bencrabbe/numnlp.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bencrabbe/numnlp.github.io/issues/new?title=Issue%20on%20page%20%2Foptim.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optim.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimization basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-and-least-squares">Linear regression and least squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization-of-univariate-functions">Convex optimization of univariate functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-derivative">Pointwise derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-derivative-function">The derivative function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analytical-root-finding-method">Analytical root finding method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-root-finding-methods">Numerical root  finding methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization-of-multivariate-functions">Convex optimization of multivariate functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives-and-the-gradient">Partial derivatives and the gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Numerical root finding methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-convex-optimization">Non convex optimization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimization-basics">
<h1>Optimization basics<a class="headerlink" href="#optimization-basics" title="Permalink to this heading">#</a></h1>
<p>Optimization is the task of finding minima and maxima of numeric functions
of the form <span class="math notranslate nohighlight">\(f:\mathbb{R}^n\mapsto \mathbb{R}\)</span>.  This is a key task in many machine learning problems. In general the task has interest
in the multivariate case, but it is convenient to take introductory examples from the monovariate case, that is with functions of the form
<span class="math notranslate nohighlight">\(f:\mathbb{R}\mapsto \mathbb{R}\)</span>.</p>
<p>Optimization methods are well behaved for the convex case, that is the case where the function is “bowl shaped” and where the function has a unique global minimum. This is the case we essentially focus on  here.
The non convex case is important too but more tricky to work with since many local solutions may exist. This latter case is of practical importance when optimizing functions like neural networks.</p>
<section id="linear-regression-and-least-squares">
<h2>Linear regression and least squares<a class="headerlink" href="#linear-regression-and-least-squares" title="Permalink to this heading">#</a></h2>
<p>Linear regression is a common tool for data analysis and an excellent introduction to machine learning.
However it is not straightforwardly applicable for modeling natural language directly. Suppose we have a data set such as the Iris flowers,
whose first few lines are given here:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Sepal Length</p></th>
<th class="head"><p>Sepal Width</p></th>
<th class="head"><p>Petal Length</p></th>
<th class="head"><p>Petal Width</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>5.1</p></td>
<td><p>3.5</p></td>
<td><p>1.4</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>4.9</p></td>
<td><p>3.0</p></td>
<td><p>1.4</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>4.7</p></td>
<td><p>3.2</p></td>
<td><p>1.3</p></td>
<td><p>0.2</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>4.6</p></td>
<td><p>3.1</p></td>
<td><p>1.5</p></td>
<td><p>0.2</p></td>
</tr>
</tbody>
</table>
<p><strong>Bivariate least squares</strong> Given two variables we can plot their relation with a scatter plot, for instance we can plot the petal width as a function of the petal length:</p>
<p><img alt="_images/38404192610b8754e070865dfbfad791a87388415ec17a467cc537cdfe1a6ee2.png" src="_images/38404192610b8754e070865dfbfad791a87388415ec17a467cc537cdfe1a6ee2.png" /></p>
<p>and we may summarize the graphical pattern with a regression line, given here in red.
This kind of summary has many applications in data analysis and is called <strong>linear regression</strong>.</p>
<p>How is this regression line actually computed ? Let a bivariate regression dataset be <span class="math notranslate nohighlight">\({\cal D} = (x_{1}, y_{1}) \ldots (x_{n}, y_{n})\)</span>
and a regression line be a linear function of the form</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = ax + b
\]</div>
<p>we can measure the distance between each observed datapoint <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> and the corresponding <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> on the regression line with a squared difference
<span class="math notranslate nohighlight">\((y_i - \hat{y}_i)^2\)</span>. If we sum all these pointwise distances we get the sum of squares loss for dataset <span class="math notranslate nohighlight">\({\cal D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\text{ssq}_{\cal D}(a,b) &amp;=  \sum_{i=1}^n (\hat{y}_i - y_i)^2 \\
&amp;=  \sum_{i=1}^n (ax_i + b  - y_i)^2 
\end{align}
\end{split}\]</div>
<p>for computing a regression line, we seek the line equation that minimizes the sum of squares loss: that is we seek the values of the variables <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
that make the loss minimal. In formal notation we write:</p>
<div class="math notranslate nohighlight">
\[
(\hat{a},\hat{b}) = \mathop{\text{argmin}}_{(a,b)\in \mathbb{R}^2} \sum_{i=1}^n (ax_i + b  - y_i)^2 
\]</div>
<p>to write that in order to estimate the values <span class="math notranslate nohighlight">\((\hat{a},\hat{b})\)</span> we have to solve the minimization problem of the function <span class="math notranslate nohighlight">\( \sum_{i=1}^n (ax_i + b  - y_i)^2  \)</span>with
variables <span class="math notranslate nohighlight">\((a,b)\)</span>. In the statistical vocabulary we also say that the estimation of the regression line parameters is solved by the <strong>least squares method</strong>.</p>
<p><strong>Multivariate least squares</strong> Note that least squares can be generalized to cases with multiple predictors. In this latter case the dataset has the form <span class="math notranslate nohighlight">\({\cal D} = (x_1, x_2\ldots x_k , y)\)</span>
and we predict <span class="math notranslate nohighlight">\(y\)</span> given a vector of <span class="math notranslate nohighlight">\(x_i\)</span>. In this generalized case the linear function takes
the form:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = a_1 x_{1} + a_2 x_2 + \ldots + a_k x_k + b
\]</div>
<p>the sum of squares and the minimization problem remain essentially the same. For instance the sum of square becomes:</p>
<div class="math notranslate nohighlight">
\[
\text{ssq}_{\cal D}(\mathbf{a},b) = \sum_{i=1}^n (a_1x_{1,i}+\ldots + a_k x_{k,i}+ b - y_i)^2 
\]</div>
<p><strong>Loss function in machine learning</strong> It turns out that linear regression is a simple instance of statistical model whose parameters <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are found by solving a minimization problem.
This kind of methodology, estimating parameters by solving a minimization problem, is common to the vast majority of machine learning methods used today. The function to minimize
is not always the least squares, each model has its own function,  and in machine learning terminology this function is called the <strong>loss function</strong>.</p>
<div class="tip admonition">
<p class="admonition-title">Linear regression without pain</p>
<p>Linear regression estimation is a routine task in applied statistics. In python we can do that easily with, for instance, the <code class="docutils literal notranslate"><span class="pre">numpy.linalg</span></code> package
and the <code class="docutils literal notranslate"><span class="pre">lstsq</span></code> function.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pa</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">lstsq</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">pa</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;petal length (cm)&#39;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span><span class="o">.</span><span class="n">T</span> <span class="c1">#adds the intercept</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;petal width (cm)&#39;</span><span class="p">]</span>
<span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.4157554163524115 -0.363075521319029
</pre></div>
</div>
</div>
</div>
<p>More elaborate natural language processing and machine learning problems, however, cannot be implemented by simple library calls,
hence we provide a more detailed introduction to optimization methods.</p>
</section>
<section id="convex-optimization-of-univariate-functions">
<h2>Convex optimization of univariate functions<a class="headerlink" href="#convex-optimization-of-univariate-functions" title="Permalink to this heading">#</a></h2>
<p>Optimization methods seek to find the unique minimum of functions, and in the monovariate case functions have the form <span class="math notranslate nohighlight">\(f:\mathbb{R}\mapsto \mathbb{R}\)</span>.
Not all such functions have a unique minimum. We thus focus on a subset of functions that have this property: the strictly <em>convex functions</em>.</p>
<p>Informally speaking, convex functions have the following properties :</p>
<ul class="simple">
<li><p>convex functions are continuous and one can draw them on a paper without raising up the pen</p></li>
<li><p>convex functions are convex (!) that is one can choose any couple of points on the function graph and draw a straight line between the two such that this line will be above the function graph.</p></li>
<li><p>when the function is strictly convex, its critical point is unique and is a global minimum of the function.</p></li>
<li><p>convex function are not necessarily differentiable. For instance, the absolute value and the maximum function are convex but not everywhere differentiable.</p></li>
</ul>
<div class="admonition-continuity-and-discontinuity admonition">
<p class="admonition-title">Continuity and discontinuity</p>
<p>Here is an example of two discontinuous functions. As can be seen the two functions exhibit a “jump” that would require to raise up the pen
to draw the graph.</p>
<p><img alt="_images/c6100d9a9781767b0049a45c236b905994ec017fbe69278510c73962d8a93db2.png" src="_images/c6100d9a9781767b0049a45c236b905994ec017fbe69278510c73962d8a93db2.png" /></p>
<p>Technically, a function <span class="math notranslate nohighlight">\(f\)</span> is continous at <span class="math notranslate nohighlight">\(a\)</span> iff :</p>
<div class="math notranslate nohighlight">
\[
\lim_{x\rightarrow a} f(x) = f(a) 
\]</div>
<p>and <span class="math notranslate nohighlight">\(f\)</span> is <strong>continuous</strong> on the interval <span class="math notranslate nohighlight">\(S\)</span> if <span class="math notranslate nohighlight">\(f\)</span> is continuous for every <span class="math notranslate nohighlight">\(a\in S\)</span>.
Otherwise <span class="math notranslate nohighlight">\(f\)</span> is <strong>discontinuous</strong> on the interval <span class="math notranslate nohighlight">\(S\)</span>. Discontinuity at <span class="math notranslate nohighlight">\(a\)</span> arises when the limit at <span class="math notranslate nohighlight">\(a\)</span> does not exist.
On both examples, at 0, the left limit is different from the right limit.</p>
</div>
<section id="convexity">
<h3>Convexity<a class="headerlink" href="#convexity" title="Permalink to this heading">#</a></h3>
<p>Optimizing a strictly convex function, amounts to find its unique minimum.
Informally, a strictly convex function is a continuous U-shaped function with a unique minimum.</p>
<p>In the univariate case, the definition formalizes the intuition that a line from a couple of points <span class="math notranslate nohighlight">\(x_1\)</span>
and <span class="math notranslate nohighlight">\(x_2\)</span> is above (or equal) the function <span class="math notranslate nohighlight">\(f\)</span>. Let <span class="math notranslate nohighlight">\(t \in [0,1]\)</span> then <span class="math notranslate nohighlight">\( f(t x_1 + (1-t) x_2)\)</span> is the set of
values for the function <span class="math notranslate nohighlight">\(f\)</span> between <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> and <span class="math notranslate nohighlight">\(t f(x_1) + (1-t) f(x_2)\)</span> is the set of values on the line on the same interval.
In this context, a convex function verifies:</p>
<div class="math notranslate nohighlight">
\[
f(t x_1 + (1-t) x_2) \leq t f(x_1) + (1-t) f(x_2) \qquad t \in [0,1] 
\]</div>
<p>as can be better understood with the following illustration:</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="_images/348cdb580d4bcf7457f7d7000483ce30c8512c27239609d2461e5aa47b4f8138.png" src="_images/348cdb580d4bcf7457f7d7000483ce30c8512c27239609d2461e5aa47b4f8138.png" />
</div>
</div>
<div class="admonition-example-convexity admonition">
<p class="admonition-title">Example (convexity)</p>
<p>Here are a few examples of convex and non convex functions.
Observe that linear functions like <span class="math notranslate nohighlight">\(f(x) = x - 1\)</span> are convex and that convex functions can have several minima,
for instance <span class="math notranslate nohighlight">\(f(x) = \text{max}(x^2,1)\)</span> has an infinite set of minimal values <span class="math notranslate nohighlight">\(x\)</span> for <span class="math notranslate nohighlight">\(f(x) = 1\)</span>. One can finally observe that
<span class="math notranslate nohighlight">\(- f(x)\)</span> is <strong>concave</strong> if <span class="math notranslate nohighlight">\(f(x)\)</span> is convex as illustrated with <span class="math notranslate nohighlight">\(f(x) = -(2x^2+3x-2)\)</span></p>
<p><img alt="_images/98017031466c87a673c516a944eaf1bdff3046f584f25e9b6b69d788605e02d9.png" src="_images/98017031466c87a673c516a944eaf1bdff3046f584f25e9b6b69d788605e02d9.png" /></p>
</div>
<p>To rule out linear functions like <span class="math notranslate nohighlight">\(x-1\)</span> or functions with a set of minimal values like <span class="math notranslate nohighlight">\(\text{max}(x^2,1)\)</span> we define a more constrained version of convexity.
A function <span class="math notranslate nohighlight">\(f\)</span> is <strong>strictly convex</strong> iff:</p>
<div class="math notranslate nohighlight">
\[
f(t x_1 + (1-t) x_2) &lt; t f(x_1) + (1-t) f(x_2) \qquad t \in [0,1] 
\]</div>
<p>A strictly convex function on interval <span class="math notranslate nohighlight">\(S\)</span> has a single minimal value, or more precisely <span class="math notranslate nohighlight">\(\text{argmax}_{x\in S} f(x)\)</span> has a unique solution when <span class="math notranslate nohighlight">\(f(x)\)</span> is strictly convex.</p>
</section>
<section id="pointwise-derivative">
<h3>Pointwise derivative<a class="headerlink" href="#pointwise-derivative" title="Permalink to this heading">#</a></h3>
<p>To find the minimum of a function <span class="math notranslate nohighlight">\(f\)</span> we fundamentally rely on a quantity, the pointwise derivative, that essentially says how the function evolves at some chosen point.
When the derivative is null for some point <span class="math notranslate nohighlight">\(x_t\)</span>, it means that <span class="math notranslate nohighlight">\(f(x)\)</span> is constant around this point.
If the function is strictly convex, a null derivative for some value <span class="math notranslate nohighlight">\(x_t\)</span> indicates that the minimum of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(x_t\)</span>.</p>
<div class="admonition-the-linear-approximation admonition">
<p class="admonition-title">The linear approximation</p>
<p>Derivatives approximate the rate of change of a function by a straight line.
For a point <span class="math notranslate nohighlight">\((x_t,f(x_t))\)</span> we measure the slope of this function by measuring the ratio
<span class="math notranslate nohighlight">\(\Delta y = f(x_0+ h) - f(x_0)\)</span>, that is the vertical change as a function of the horizontal change
<span class="math notranslate nohighlight">\(\Delta x = (x_0 + h) - x_0\)</span> where <span class="math notranslate nohighlight">\(h &gt; 0\)</span> is a non null positive number.</p>
<p><img alt="_images/d580421c17eccbbee87ebf6e14522b73cdbe45411afb15a20a7ea3948e4e5be2.png" src="_images/d580421c17eccbbee87ebf6e14522b73cdbe45411afb15a20a7ea3948e4e5be2.png" /></p>
<p>The rate of change at <span class="math notranslate nohighlight">\(x_0\)</span> is measured by the ratio:</p>
<div class="math notranslate nohighlight">
\[
\frac{\Delta y}{\Delta x}(x_0) = \frac{f(x_0+ h) - f(x_0)}{ (x_0 + h) - x_0}
\]</div>
<p>The problem with this first approximation comes from the fact that the larger the <span class="math notranslate nohighlight">\(h\)</span> the larger the approximation.
To avoid unjustified approximations, we should choose <span class="math notranslate nohighlight">\(h\)</span> as close to 0 as possible. That is the motivations for using derivatives.</p>
</div>
<p>A derivative crucially defines <span class="math notranslate nohighlight">\(h\)</span> to be infinitesimal (or infinitely close to 0).
This leads to the definition of the <strong>pointwise derivative</strong> of the function <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span> as the limit:</p>
<div class="math notranslate nohighlight">
\[
\frac{d y}{d x}(x_0) = \lim_{h\rightarrow 0} \frac{f(x_0 + h )-f(x_0)}{(x_0+h)-x_0}
\]</div>
<p>where <span class="math notranslate nohighlight">\(dy\)</span> and <span class="math notranslate nohighlight">\(dy\)</span> implicitly denote respectively the infinitesimal vertical and horizontal differences of the function.
To make explicit that we compute the derivative of the function <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span> we rather write:</p>
<div class="math notranslate nohighlight">
\[
\frac{d f}{d x}(x_0) = \lim_{h\rightarrow 0} \frac{f(x_0 + h )-f(x_0)}{(x_0+h)-x_0}
\]</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>We compute the pointwise derivative of the function <span class="math notranslate nohighlight">\(f:x\mapsto \frac{x^2}{4}\)</span> at <span class="math notranslate nohighlight">\(x_0=1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
 \frac{d y}{d x}(x_0)&amp;=\lim_{h\rightarrow 0}   \frac{\frac{(x_{0}+h)^2}{4} -  \frac{x_0^2}{4}}{(x_0+h)-x_0}\\
 &amp;=\lim_{h\rightarrow 0}  \frac{1}{4} \frac{(x_{0}+h)^2 -  x_0^2}{(x_0+h)-x_0}\\
   &amp;= \lim_{h\rightarrow 0} \frac{1}{4}\frac{ [(x_0+h)+x_0] [(x_{0}+h)-x_0]}{(x_{0}+h)-x_0}\\
  &amp;= \lim_{h\rightarrow 0} \frac{1}{4} (x_{0}+h+x_0)\\
  &amp;= \frac{1}{4} (x_{0} + x_0)\\
  &amp;=\frac{1}{2}
\end{align*} 
\end{split}\]</div>
<p>Now if we want to compute the derivative of the function <span class="math notranslate nohighlight">\(f:x\mapsto \frac{x^2}{4}\)</span> at <span class="math notranslate nohighlight">\(x_0=4\)</span> we redo almost everything:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
 \frac{d y}{d x}(x_0)&amp;=\lim_{h\rightarrow 0}   \frac{\frac{(x_{0}+h)^2}{4} -  \frac{x_0^2}{4}}{(x_0+h)-x_0}\\
 &amp;=\lim_{h\rightarrow 0}  \frac{1}{4} \frac{(x_{0}+h)^2 -  x_0^2}{(x_0+h)-x_0}\\
   &amp;= \lim_{h\rightarrow 0} \frac{1}{4}\frac{ [(x_0+h)+x_0] [(x_{0}+h)-x_0]}{(x_{0}+h)-x_0}\\
  &amp;= \lim_{h\rightarrow 0} \frac{1}{4} (x_{0}+h+x_0)\\
  &amp;= \frac{1}{4} (x_{0} + x_0)\\
  &amp;= 2
\end{align*} 
\end{split}\]</div>
<p>One can observe however that these derivatives could be computed more easily for every <span class="math notranslate nohighlight">\(x\)</span> by using the function <span class="math notranslate nohighlight">\(\frac{d f}{d x}=\frac{2 x}{4}\)</span>.
This function,  <span class="math notranslate nohighlight">\(\frac{d f}{d x}\)</span>, is called the <strong>derivative function</strong> of <span class="math notranslate nohighlight">\(f\)</span>. <img alt="_images/9345c690fb035a5617edfc7cc9f37e09e0dc8b4f64c98d9b61369956911fb2e2.png" src="_images/9345c690fb035a5617edfc7cc9f37e09e0dc8b4f64c98d9b61369956911fb2e2.png" /></p>
</div>
<div class="admonition-interpretation-of-the-pointwise-derivative admonition">
<p class="admonition-title">Interpretation of the pointwise derivative</p>
<p>The derivative provides a crucial indicator for optimization purposes. It indicates whether the function increases, decreases or remains stable at point <span class="math notranslate nohighlight">\(x_0\)</span>:</p>
<ul class="simple">
<li><p>the derivative is positive when the function increases at <span class="math notranslate nohighlight">\(x_0\)</span></p></li>
<li><p>the derivative is negative when the function decreases at <span class="math notranslate nohighlight">\(x_0\)</span></p></li>
<li><p>the derivative is zero when the function is constant at <span class="math notranslate nohighlight">\(x_0\)</span></p></li>
</ul>
<p>We illustrate this property in the picture below for 3 values of <span class="math notranslate nohighlight">\(x\)</span> on the same function <span class="math notranslate nohighlight">\(f(x) = \frac{x^2}{4}\)</span>.
Each time we provide the tangent function. We can observe at <span class="math notranslate nohighlight">\(x = -2\)</span> that the function decreases and that the derivative <span class="math notranslate nohighlight">\(\frac{d f}{d x}(-2)= -1\)</span>,
at <span class="math notranslate nohighlight">\(x = 2\)</span> that the function increases and that the derivative <span class="math notranslate nohighlight">\(\frac{d f}{d x}(2)= 1\)</span> and at <span class="math notranslate nohighlight">\(x = 0\)</span> that the function is constant (has a minimum)
and that the  derivative <span class="math notranslate nohighlight">\(\frac{d f}{d x}(0)= 0\)</span>,</p>
<p><img alt="_images/8e63f7ce6e19197d16f276e4ef3ea8dfce18f9aa9f617a5d56a1500ad216f072.png" src="_images/8e63f7ce6e19197d16f276e4ef3ea8dfce18f9aa9f617a5d56a1500ad216f072.png" /></p>
</div>
</section>
<section id="the-derivative-function">
<h3>The derivative function<a class="headerlink" href="#the-derivative-function" title="Permalink to this heading">#</a></h3>
<p>For a function <span class="math notranslate nohighlight">\(f\)</span>, its derivative function is the function <span class="math notranslate nohighlight">\(\frac{f y}{d x}\)</span>. It turns out that one can find the derivative function
from the a analytical form of the function <span class="math notranslate nohighlight">\(f\)</span> by using a small set of functional rewriting rules.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Source</p></th>
<th class="head"><p>Result</p></th>
<th class="head text-right"><p>Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d x}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d c}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(c\)</span> is a constant</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d \lambda f(x)}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\lambda \frac{d f(x)}{d x}\)</span></p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(\lambda\)</span> is a constant</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d f(x)  +  g(x) }{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \frac{d f(x)}{d x} + \frac{d g(x)}{d x} \)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d f(x) \times   g(x) }{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \frac{d f(x)}{d x} g(x) + f(x) \frac{d g(x)}{d x} \)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d \frac{f(x)} {g(x)} }{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{ \frac{d f(x)}{d x} g(x) - f(x) \frac{d g(x)}{d x} } {g(x)^2}\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d f(x)^n}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(n f(x)^{n-1} \frac{d f(x)}{dx} \)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d \frac{1}{f(x)}}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(-\frac{1}{f(x)^2}\frac{d f(x)}{dx}\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d \sqrt{f(x)}}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{2\sqrt{f(x)}}\frac{d f(x)}{d x}\)</span></p></td>
<td class="text-right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d e^{f(x)}}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( e^{f(x)} \frac{d f(x)}{d x} \)</span></p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(e\)</span> is the exponential function</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><span class="math notranslate nohighlight">\(\frac{d \ln (f(x))}{d x}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\frac{1}{f(x)}\frac{d f(x)}{d x} \)</span></p></td>
<td class="text-right"><p><span class="math notranslate nohighlight">\(\ln\)</span> is the natural logarithm function</p></td>
</tr>
</tbody>
</table>
<div class="admonition-example-manual-computation admonition">
<p class="admonition-title">Example (manual computation)</p>
<p>Here is how to compute the derivative of <span class="math notranslate nohighlight">\(f(x) = \frac{x^2}{4}\)</span> with the rewriting rules:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{d \frac{1}{4} x^2}{dx} &amp;= \frac{1}{4} \frac{d x^2}{dx}\\
                             &amp;= \frac{1}{4} 2 x \frac{d x}{x}\\
							 &amp;=  \frac{1}{2} x
\end{align}
\end{split}\]</div>
<p>we conclude that <span class="math notranslate nohighlight">\(\frac{d f}{d x} = \frac{1}{2} x \)</span></p>
</div>
<div class="admonition-example-symbolic-computation admonition">
<p class="admonition-title">Example (symbolic computation)</p>
<p>Note that the computation of derivative function can be made easy with symbolic computation libraries.
In Python the <code class="docutils literal notranslate"><span class="pre">sympy</span></code> library computes derivatives. Here is an example for computing the derivative of
<span class="math notranslate nohighlight">\(f(x) = \frac{x^2}{4}\)</span>:</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="n">symbols</span><span class="p">,</span><span class="n">diff</span>

<span class="n">x</span>          <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>     <span class="c1">#declares &#39;x&#39; as a symbol</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="c1">#computes the derivative function wrt to &#39;x&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="n">derivative</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>x/2
</pre></div>
</div>
</div>
</div>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>Where do the the rules come from ? Actually they can be derived from the definition of the derivative.
Here is an example for the addition rule. We simply instanciate the definition for the addition of <span class="math notranslate nohighlight">\(f(x)+g(x)\)</span>
and do the development:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    \frac{d f(x_0) + g (x_0)}{dx}(x) &amp;= \lim_{h \rightarrow 0}\frac{f(x_0+h)+g(x_0+h)-(f(x_0)+g(x_0))}{(x_0+h)-x_0}\\
	                &amp;=\lim_{h \rightarrow 0}\frac{f(x_0+h) - f(x_0)+g(x_0+h)-g(x_0)}{(x_0+h)-x_0}\\
				    &amp;=\lim_{h \rightarrow 0}\frac{f(x_0+h)- f(x_0)}{(x_{0}+h)-x_0} +\lim_{h \rightarrow 0}\frac{g(x_0+h) - g(x_0)}{(x_{0}+h)-x_0}\\
    &amp;= \frac{d f(x)}{d x}(x) + \frac{d g(x)}{d x}(x)
  \end{align}
\end{split}\]</div>
</div>
<div class="admonition-note-differentiability admonition">
<p class="admonition-title">Note (differentiability)</p>
<p>The derivative function defined on the interval <span class="math notranslate nohighlight">\(I\)</span> supposes the function <span class="math notranslate nohighlight">\(f\)</span> to be everywhere differentiable over <span class="math notranslate nohighlight">\(I\)</span>.
Not all functions are everywhere differentiable. Non continuous functions are obviously non differentiable
but there are however continuous functions that are not differentiable.</p>
<p>Below are typical example functions that are continuous but non differentiable everywhere: a maximum function and a absolute value function.
Both functions have angular points that are non differentiable.
For <span class="math notranslate nohighlight">\(x = 1\)</span> the absolute value function <span class="math notranslate nohighlight">\(|x-1|\)</span> has a derivative <span class="math notranslate nohighlight">\(-1\)</span> when we approach <span class="math notranslate nohighlight">\(x\)</span> by the left
while it has derivative <span class="math notranslate nohighlight">\(1\)</span> when we approach <span class="math notranslate nohighlight">\(x\)</span> by the right.</p>
<p><img alt="_images/a334bbefff175c383e23435ace4511aa27053b36c8891e182a60dc75d2633368.png" src="_images/a334bbefff175c383e23435ace4511aa27053b36c8891e182a60dc75d2633368.png" /></p>
<p>More generally, a function is <strong>differentiable at <span class="math notranslate nohighlight">\(x\)</span></strong> if its right derivative equals its left derivative, that is:</p>
<div class="math notranslate nohighlight">
\[
\lim_{h \rightarrow 0^+} \frac{f(x_0+h) - f(x_0)}{(x_0+h) - x_0} = \lim_{h \rightarrow 0^-} \frac{f(x_0+h) - f(x_0)}{(x_0+h) - x_0}
\]</div>
</div>
</section>
<section id="analytical-root-finding-method">
<h3>Analytical root finding method<a class="headerlink" href="#analytical-root-finding-method" title="Permalink to this heading">#</a></h3>
<p>Finding the minimum of a function <span class="math notranslate nohighlight">\(f\)</span> over an interval <span class="math notranslate nohighlight">\(S\)</span> amounts to solve:</p>
<div class="math notranslate nohighlight">
\[
x = \mathop{\text{argmin}}_{x \in S} f(x)
\]</div>
<p>and for strictly convex functions, the minimum is guaranteed to be unique. Thus we are guaranteed that setting the derivative to 0 and solving <span class="math notranslate nohighlight">\(\frac{df}{dx}(x) = 0\)</span>
has a unique solution, the minimum. In other words optimization of a convex function can be cast as finding the root (or zero) of the derivative function.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>To find the minimum of the convex function <span class="math notranslate nohighlight">\(f(x) = (x - 4)^2\)</span> we
first compute its derivative:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{df}{dx} &amp;= 2(x-4)\\
             &amp;= 2x - 8
\end{align}
\end{split}\]</div>
<p>Once the derivative is known we solve the equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
 2x - 8 &amp;= 0\\
 2x &amp;= 8\\
 x &amp;= 4
\end{align}
\end{split}\]</div>
<p>and we conclude that <span class="math notranslate nohighlight">\( 4 = \mathop{\text{argmin}}_{x \in \mathbb{R}} (x-4)^2\)</span> .</p>
</div>
</section>
<section id="numerical-root-finding-methods">
<h3>Numerical root  finding methods<a class="headerlink" href="#numerical-root-finding-methods" title="Permalink to this heading">#</a></h3>
<p>The analytical method becomes impractical for non linear functions, like higher order polynoms, and in the multivariate case.
We present here a family of numerical methods for root finding and for functional minimization of convex functions that work generally well both for linear and non linear functions.</p>
<p>All of theses methods share the same abstract algorithm. They build a sequence of approximate solutions (or iterates) <span class="math notranslate nohighlight">\(x_1 x_2 \ldots x_n\)</span> starting from an initial random guess <span class="math notranslate nohighlight">\(x_1\)</span>
and refining the solution at each step <span class="math notranslate nohighlight">\(x_t\)</span> by taking advantage of  information given by the derivative at <span class="math notranslate nohighlight">\(x_t\)</span>.</p>
<p><strong>Gradient method</strong>  Given an iterate <span class="math notranslate nohighlight">\(x_t\)</span>, the gradient descent generates <span class="math notranslate nohighlight">\(x_{t+1}\)</span> by evaluating <span class="math notranslate nohighlight">\(\frac{df}{dx}(x_t)\)</span>. There are 3 cases:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\frac{df}{dx}(x_t)\)</span> is positive. Then the minimum is smaller than <span class="math notranslate nohighlight">\(x_t\)</span> and <span class="math notranslate nohighlight">\(x_{t+1} &lt; x_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{df}{dx}(x_t)\)</span> is negative. Then the minimum is greater than <span class="math notranslate nohighlight">\(x_t\)</span> and <span class="math notranslate nohighlight">\(x_{t+1} &gt; x_t\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{df}{dx}(x_t)\)</span>  is zero. Then the solution is found</p></li>
</ul>
<p>These three cases are wrapped up in the update central to  <a class="reference internal" href="#gd-algo">Algorithm 1</a>. Notice that when the derivative is positive we get <span class="math notranslate nohighlight">\(x_{t+1}\)</span> by substracting the full derivative value to <span class="math notranslate nohighlight">\(x_t\)</span>
and when the derivative is negative we get <span class="math notranslate nohighlight">\(x_{t+1}\)</span> by adding the full derivative value to <span class="math notranslate nohighlight">\(x_t\)</span>. The <span class="math notranslate nohighlight">\(\alpha\)</span> parameter, the <strong>learning rate</strong> (<span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>), is a value used to scale the gradient update and <span class="math notranslate nohighlight">\(\epsilon\)</span>
is a <strong>tolerance</strong> parameter usually set to a small positive value to control for the approximation of the returned solution.</p>
<div class="proof algorithm admonition" id="gd-algo">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>while</strong> <span class="math notranslate nohighlight">\( | x_{t+1} - x_t |  &gt; \epsilon \)</span> <strong>do</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{t+1} \gets x_t - \alpha\frac{df}{dx}(x_t)\)</span></p></li>
</ul>
</section>
</div><p>Although simple, the gradient descent algorithm suffers from several drawbacks. Generally speaking it offers few guarantees to reach a solution with a good enough precision in a finite amount of time.
In practice it can overshoot: if the learning rate is too high the updates may be too strong and the algortithm might overshoot the minimum and eventually diverge. On the other hand if the learning rate is too low, the updates may be too mild
and the algorithm may take (almost) forever to reach the minimum. The tuning of the <span class="math notranslate nohighlight">\(\alpha\)</span> parameter remains heuristic and it may be difficult to empirically achieve satisfying results.
Overall the algorithm offers few guarantees to reach a solution with a satisfying precision in a finite amount of time.</p>
<div class="admonition-gradient-descent-illustrated admonition">
<p class="admonition-title">Gradient descent illustrated</p>
<p>The observation of the gradient descent method on a simple function further illustrates its weaknesses.
Here we illustrates the first few iterates of a gradient descent both on the function <span class="math notranslate nohighlight">\(f(x) = \frac{1}{5} x^2\)</span>
and on its derivative <span class="math notranslate nohighlight">\(\frac{df}{dx} =  \frac{2}{5} x\)</span>, starting from a random iterate <span class="math notranslate nohighlight">\(x_1 = 2.5\)</span>.</p>
<p><img alt="_images/7df04ca1a8c15b2f8975b2ff447ff8f88f1400b978d975f5720696ce0791efac.png" src="_images/7df04ca1a8c15b2f8975b2ff447ff8f88f1400b978d975f5720696ce0791efac.png" /></p>
<p>As can be seen the gradient descent is blind to the actual shape of the derivative function itself.
The stepsize is dependent of an arbitrary constant <span class="math notranslate nohighlight">\(\alpha\)</span> and on the pointwise value of the derivative.
The secant and newton method substantially improve this behavior by seeking to define an adaptative <span class="math notranslate nohighlight">\(\alpha\)</span> taking into account the shape of the derivative function.</p>
</div>
<p><strong>The secant method</strong> is a root finding method that given two iterates <span class="math notranslate nohighlight">\(x_{t-1}\)</span> and <span class="math notranslate nohighlight">\(x_t\)</span> generates the next iterate <span class="math notranslate nohighlight">\(x_{t+1}\)</span> by intersecting the secant line joining  <span class="math notranslate nohighlight">\((x_{t-1},\frac{df}{dx}(x_{t-1}))\)</span> and <span class="math notranslate nohighlight">\((x_t, \frac{df}{dx}(x_t))\)</span> with the <span class="math notranslate nohighlight">\(x\)</span> axis.
In other words the secant method approximates the derivative of the derivative function <span class="math notranslate nohighlight">\(\frac{df}{dx}\)</span> with a finite difference approximation.</p>
<div class="proof algorithm admonition" id="sec-algo">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Secant method)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>while</strong> <span class="math notranslate nohighlight">\( | x_{t+1} - x_t | &gt; \epsilon \)</span> <strong>do</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha \gets \frac{x_t-x_{t-1}}{\frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) }\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{t+1} \gets x_t - \alpha\frac{df}{dx}(x_t)\)</span></p></li>
</ul>
</section>
</div><p>The secant method is given as <a class="reference internal" href="#sec-algo">Algorithm 2</a>. To understand the algorithm, we first express the equation of the secant going through <span class="math notranslate nohighlight">\((x_{t-1},\frac{df}{dx}(x_{t-1}))\)</span> and <span class="math notranslate nohighlight">\((x_t, \frac{df}{dx}(x_t))\)</span>:</p>
<div class="math notranslate nohighlight">
\[
y  = \frac{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) } {x_t-x_{t-1}} (x-x_{t}) + \frac{df}{dx}(x_{t})
\]</div>
<p>Thus any point on the secant has coordinates <span class="math notranslate nohighlight">\((x,y)\)</span> subject to the constraint given by the equation.
To select the next iterate <span class="math notranslate nohighlight">\(x_{t+1}\)</span>, we seek the intersection of the secant with the <span class="math notranslate nohighlight">\(x\)</span> axis, that is the  value of <span class="math notranslate nohighlight">\(x\)</span> for which <span class="math notranslate nohighlight">\(y = 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\frac{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) } {x_t-x_{t-1}} (x-x_{t}) + \frac{df}{dx}(x_{t}) &amp;= 0\\
\frac{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) } {x_t-x_{t-1}} (x-x_{t}) &amp;=  - \frac{df}{dx}(x_{t}) \\
x-x_{t} &amp;=  - \frac{x_t-x_{t-1}}{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) }  \frac{df}{dx}(x_{t}) \\
x &amp;= x_{t} - \frac{x_t-x_{t-1}}{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) }  \frac{df}{dx}(x_{t}) 
\end{align}
\end{split}\]</div>
<p>that we rewrite :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\alpha &amp;= \frac{x_t-x_{t-1}}{ \frac{df}{dx}(x_t)- \frac{df}{dx}(x_{t-1}) } \\
x_{t+1} &amp;= x_{t} - \alpha \frac{df}{dx}(x_{t}) 
\end{align}
\end{split}\]</div>
<div class="admonition-the-secant-method-illustrated admonition">
<p class="admonition-title">The secant method illustrated</p>
<p>Here is an example of the first few iterations of the secant method on the function <span class="math notranslate nohighlight">\(f(x) = \frac{x^4}{4}\)</span>
with initialisation <span class="math notranslate nohighlight">\(x_1=2\)</span> and <span class="math notranslate nohighlight">\(x_2=1.9\)</span>. As can be seen, for optimization purposes the secants are computed on the function’s derivative
and at each time step capture the approximative shape of the derivative with the secant in order to guess the next iterate.</p>
<p><img alt="_images/d5407520448255f2f4f34126f55046e2b6a4703076553abb947661214ad165ce.png" src="_images/d5407520448255f2f4f34126f55046e2b6a4703076553abb947661214ad165ce.png" /></p>
</div>
<div class="tip admonition">
<p class="admonition-title">The finite difference approximation</p>
<p>The secant method relies on the finite difference approximation of the second order derivative.
Remember that a derivative is defined as:</p>
<div class="math notranslate nohighlight">
\[
\frac{d y}{d x}(x_0) = \lim_{h\rightarrow 0} \frac{f(x_0 + h )-f(x_0)}{(x_0+h)-x_0}
\]</div>
<p>when <span class="math notranslate nohighlight">\(h\)</span> is infinitesimal the derivative is the exact slope coefficient of the tangent at <span class="math notranslate nohighlight">\((x_0,f(x_0))\)</span>.
If <span class="math notranslate nohighlight">\(h\)</span> is not anymore infinitesimal (is finite) and we use a slope coefficient with finite differences:</p>
<div class="math notranslate nohighlight">
\[
\frac{\Delta y}{\Delta x}(x_0) = \frac{f(x_0 + h )-f(x_0)}{(x_0+h)-x_0}
\]</div>
<p>then the line connecting <span class="math notranslate nohighlight">\((x_0,f(x_0))\)</span> and <span class="math notranslate nohighlight">\((x_0+h,f(x_0+h))\)</span> is a secant to the function. As can be seen on the illustration below
the slope of the secant is an approximation of the slope of the tangent</p>
<p><img alt="_images/bbfd224814cbf17e3956b7380c1c78c94b6c26b644763e2b1594b3f9c65fff21.png" src="_images/bbfd224814cbf17e3956b7380c1c78c94b6c26b644763e2b1594b3f9c65fff21.png" /></p>
</div>
<p><strong>Newton’s method</strong>  is a root finding method that given an iterate <span class="math notranslate nohighlight">\(x_t\)</span> generates the next iterate <span class="math notranslate nohighlight">\(x_{t+1}\)</span> by intersecting the tangent line of the derivative at <span class="math notranslate nohighlight">\((x_t, \frac{df}{dx}(x_t))\)</span> with the <span class="math notranslate nohighlight">\(x\)</span> axis.
In a context of optimization, the Newton method thus uses the second order derivative as slope of the tangent line.</p>
<div class="proof algorithm admonition" id="newton-algo">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Newton method)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>while</strong> <span class="math notranslate nohighlight">\( | x_{t+1} - x_t | &gt; \epsilon \)</span> <strong>do</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha \gets \frac{1}{\frac{d^2f}{dx^2}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_{t+1} \gets x_t - \alpha\frac{df}{dx}(x_t)\)</span></p></li>
</ul>
</section>
</div><p>The Newton method is given as <a class="reference internal" href="#newton-algo">Algorithm 3</a>.  The algorithm is the result of a development similar to the secant method: we start by expressing the equation of the tangent
at <span class="math notranslate nohighlight">\((x_t, \frac{df}{dx}(x_t))\)</span></p>
<div class="math notranslate nohighlight">
\[
y = \frac{d^2 f}{dx^2}(x_t) (x - x_t) + \frac{df}{dx}(x_t) 
\]</div>
<p>The coordinates of the points of the tangent are <span class="math notranslate nohighlight">\((x,y)\)</span> subject to the constraint given by the equation. To get the value of <span class="math notranslate nohighlight">\(x\)</span> when intersecting the <span class="math notranslate nohighlight">\(x-\)</span>axis we solve:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
 \frac{d^2 f}{dx^2}(x_t) (x - x_t) + \frac{df}{dx}(x_t)  &amp;= 0\\
x - x_t  &amp;= - \frac{1}{ \frac{d^2 f}{dx^2}(x_t) }  \frac{df}{dx}(x_t)\\
x &amp;= x_t - \frac{1}{ \frac{d^2 f}{dx^2}(x_t) }  \frac{df}{dx}(x_t)
\end{align}
\end{split}\]</div>
<p>and finally we rewrite to :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\alpha &amp;= \frac{1}{ \frac{d^2 f}{dx^2}(x_t) } \\
x_{t+1} &amp;= x_t -\alpha  \frac{df}{dx}(x_t)
\end{align}
\end{split}\]</div>
<div class="admonition-newton-method-illustrated admonition">
<p class="admonition-title">Newton method illustrated</p>
<p>Here is an example of the first few iterations of the newton method on the function <span class="math notranslate nohighlight">\(f(x) = \frac{x^4}{4}\)</span>, with derivatives
<span class="math notranslate nohighlight">\(\frac{df}{dx} = x^3\)</span>, <span class="math notranslate nohighlight">\(\frac{d^2f}{dx^2} = 3x^2\)</span>  and initialisation <span class="math notranslate nohighlight">\(x_1 = 2\)</span>.</p>
<p><img alt="_images/d81c57e4afffe3fa182df6b64cf42611e653f10d417740c0e4b7ade333654b6d.png" src="_images/d81c57e4afffe3fa182df6b64cf42611e653f10d417740c0e4b7ade333654b6d.png" /></p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Although Newton’s method may look the best of the three methods, the method suffers when used to optimize massively multivariate convex functions
and is generally terrible for non convex functions. For these reasons in context of massively multivariate problems or deep learning problems,
optimization algorithms continue to use gradient descent or quasi-newton methods (guessing an approximate <span class="math notranslate nohighlight">\(\alpha\)</span> like the secant method) as long as the precision of the solution is not critical, which is the case for most modern machine learning problems in natural language processing.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">How to implement it in python ?</p>
<p>Optimization of univariate functions is essentially an in class toy exercise from the natural language processing perspective.
Most univariate problems described in this course can be solved with <code class="docutils literal notranslate"><span class="pre">sympy</span></code> with the functions <code class="docutils literal notranslate"><span class="pre">diff</span></code> and <code class="docutils literal notranslate"><span class="pre">nonlinsolve</span></code> respectively
dedicated to compute the derivatives of functions and then to find the roots of the derivative. Here is an example:</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="n">symbols</span><span class="p">,</span><span class="n">diff</span><span class="p">,</span><span class="n">nonlinsolve</span>

<span class="n">x</span>    <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">f</span>    <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span>
<span class="n">fp</span>   <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>    <span class="c1">#first derivative with respect to x</span>
<span class="n">fs</span>   <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>   <span class="c1">#second derivative with respect to x</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">nonlinsolve</span><span class="p">([</span><span class="n">fp</span><span class="p">],[</span><span class="n">x</span><span class="p">],[</span><span class="mi">10</span><span class="p">])</span> <span class="c1">#optimizes for variable &#39;x&#39; with x_1 = 10</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;f(x) = </span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s1">, first derivative = </span><span class="si">{</span><span class="n">fp</span><span class="si">}</span><span class="s1">, second derivative = </span><span class="si">{</span><span class="n">fs</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;minimum found :&#39;</span><span class="p">,</span><span class="n">root</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>f(x) = 0.25*x**4, first derivative = 1.0*x**3, second derivative = 3.0*x**2
minimum found : {(0,)}
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="convex-optimization-of-multivariate-functions">
<h2>Convex optimization of multivariate functions<a class="headerlink" href="#convex-optimization-of-multivariate-functions" title="Permalink to this heading">#</a></h2>
<p>In this section we consider the optimization problem of multivariate functions of the form <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \mapsto \mathbb{R}\)</span> which is the usual case of interest in machine learning.
Most notions relevant for the univariate case generalize well to the multivariate case, yet the technicality increases and solving massively multivariate problems require more specialized numerical libraries.</p>
<p>We first review how convexity and the notion of derivative extend to the multivariate case before introducing the generalization of the gradient descent and the multivariate newton method.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Here is a graphical illustration, as 3d plot and as contour plot, of an example function  <span class="math notranslate nohighlight">\(f:\mathbb{R}^2 \mapsto \mathbb{R}\)</span>.
It is the bivariate function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = f(x_1,x_2) = 2x_1^2 +  x_2^2\)</span> whose two variables <span class="math notranslate nohighlight">\(x_1,x_2\)</span> are part of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p>
<p><img alt="_images/8ebe3b999d420dc3d38e5a004afb7bd75df44b69cd2666f1fdba8f1598493929.png" src="_images/8ebe3b999d420dc3d38e5a004afb7bd75df44b69cd2666f1fdba8f1598493929.png" /></p>
</div>
<section id="id1">
<h3>Convexity<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The definition of convexity is essentially a direct extension of the univariate case.
Let the function <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \mapsto \mathbb{R}\)</span> whose domain is a convex set, then <span class="math notranslate nohighlight">\(f\)</span> is convex iff:</p>
<div class="math notranslate nohighlight">
\[
f(t \mathbf{x}_1 + (1-t) \mathbf{x}_2) \leq t f(\mathbf{x}_1) + (1-t) f(\mathbf{x}_2) \qquad t \in [0,1] , \mathbf{x}_1\in\mathbb{R}^n, \mathbf{x}_2\in\mathbb{R}^n 
\]</div>
<p>Informally for any two chosen points <span class="math notranslate nohighlight">\((\mathbf{x}_1,f(\mathbf{x}_1))\)</span> and  <span class="math notranslate nohighlight">\((\mathbf{x}_2,f(\mathbf{x}_2))\)</span>, one should be able to draw a string above or on the “function bowl” without crossing its surface.
As for the univariate case, strict convexity is defined as:</p>
<div class="math notranslate nohighlight">
\[
f(t \mathbf{x}_1 + (1-t) \mathbf{x}_2) &lt; t f(\mathbf{x}_1) + (1-t) f(\mathbf{x}_2) \qquad t \in [0,1] , \mathbf{x}_1\in\mathbb{R}^n, \mathbf{x}_2\in\mathbb{R}^n 
\]</div>
<p>and it ensures that the function has a unique minimum.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>As our first example illustrated a convex bivariate function,
here is a graphical illustration, as 3d plot and as contour plot, of a non convex function,
<span class="math notranslate nohighlight">\(f(\mathbf{x}) = f(x_1,x_2) = 2x_1^2 -  x_2^2\)</span>.</p>
<p><img alt="_images/6f02ed60d95b1e748919586acb229a24eac77d98cb4c4e6d84f8e6dd63ef6c08.png" src="_images/6f02ed60d95b1e748919586acb229a24eac77d98cb4c4e6d84f8e6dd63ef6c08.png" /></p>
</div>
</section>
<section id="partial-derivatives-and-the-gradient">
<h3>Partial derivatives and the gradient<a class="headerlink" href="#partial-derivatives-and-the-gradient" title="Permalink to this heading">#</a></h3>
<p>The counterpart of the derivative for the multivariate case is called the gradient.
The gradient of a function <span class="math notranslate nohighlight">\(f\)</span> is a vector <span class="math notranslate nohighlight">\(\nabla f\)</span> of partial derivatives.
For strictly convex functions, a zero gradient at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, that is <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = \mathbf{0}\)</span>
indicates that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is minimum of the function.</p>
<p><strong>Partial derivatives</strong>
For a multivariate function <span class="math notranslate nohighlight">\(f(x_1,\ldots x_n)\)</span> the partial derivative with respect to a variable <span class="math notranslate nohighlight">\(x_i\)</span>
is the derivative of the partial function where all <span class="math notranslate nohighlight">\(x_j\quad (i\not = j)\)</span> are set to constants
except the variable <span class="math notranslate nohighlight">\(x_i\)</span>. Thus the partial derivative with respect to <span class="math notranslate nohighlight">\(x_i\)</span> captures the variation of the function along one of its coordinates.
We write the partial derivative of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x_i\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x_i}(\mathbf{x}) = \lim_{h\rightarrow 0} \frac{f(x_1,\ldots, x_i + h, \ldots x_n )-f(x_1,\ldots, x_i, \ldots x_n )} { h}
\]</div>
<p>alternatively we use the compact vector notation:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x_i}(\mathbf{x}) = \lim_{h\rightarrow 0} \frac{f(\mathbf{x} + h \mathbf{e}_i)  )-f(\mathbf{x} )} { h}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> is a one hot vector at <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Gradient</strong> The gradient of the multivariate function <span class="math notranslate nohighlight">\(f( x_1,\ldots x_n )\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x}) = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} (\mathbf{x})\\
\vdots\\
\frac{\partial f}{\partial x_n} (\mathbf{x})
\end{bmatrix}
\end{split}\]</div>
<div class="admonition-relevance-of-the-gradient-for-optimization admonition">
<p class="admonition-title">Relevance of the gradient for optimization</p>
<p>For optimization purposes the gradient is an indicator on where to search the minimum of a convex function, in particular:</p>
<ul class="simple">
<li><p>when the gradient is null, <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = \mathbf{0}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the minimum of a strictly convex function</p></li>
<li><p>otherwise the gradient is a vector pointing towards the direction of the steepest ascent of the function</p></li>
</ul>
<p>Recall that a partial derivative quantifies the evolution of the function along a direction where a <em>single coordinate</em> changes.
From the gradient we get the difference for one coordinate with:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f}{\partial x_i} = \nabla f(\mathbf{x})^\top \mathbf{e}_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> is a one hot vector.</p>
<p>Now we could quantify the evolution of the function along a chosen unit vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span>
( <span class="math notranslate nohighlight">\( || \mathbf{u} || = 1 \)</span>) not necessarily constrained to be one hot. This is a <strong>directional derivative</strong> where the evolution along
the direction <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
D_\mathbf{u}(\mathbf{x}) &amp;= \nabla f(\mathbf{x})^\top \mathbf{u} \\
&amp;= \frac{\partial f}{\partial x_1}(\mathbf{x}) u_1 + \ldots + \frac{\partial f}{\partial x_n}(\mathbf{x}) u_n
\end{align}
\end{split}\]</div>
<p>Now we ask the question: among all these possible <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> vectors which one creates the biggest increase ?
We thus want to solve :</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{u}} = \mathop{\text{argmax}}_{\mathbf{u} : ||\mathbf{u} || = 1}  \nabla f(\mathbf{x})^\top \mathbf{u}
\]</div>
<p>Recalling that <span class="math notranslate nohighlight">\(\cos\theta = \frac{\nabla f(\mathbf{x})^\top \mathbf{u}}{||\nabla f(\mathbf{x})|| || \mathbf{u} ||}\)</span>, we can conclude that
the maximum value is obtained when <span class="math notranslate nohighlight">\(\cos\theta = 1\)</span> that is when <span class="math notranslate nohighlight">\( \nabla f(\mathbf{x}) \)</span> and <span class="math notranslate nohighlight">\( \mathbf{u} \)</span>
are collinear and head in the same direction. In other words the direction of steepest increase for the function is given by the gradient vector.</p>
</div>
<p><strong>Hessian</strong> For the Newton method it is natural to compute second order
derivatives. In the monovariate case it is straightforward, given the
first order derivative <span class="math notranslate nohighlight">\(\frac{d f}{d x}\)</span> one can compute the second order derivative function by
deriving again for the same variable and get the unique second order derivative function <span class="math notranslate nohighlight">\(\frac{d^2 f}{d x^2}\)</span>.</p>
<p>Now consider the multivariate function <span class="math notranslate nohighlight">\(f(x_1\ldots x_n)\)</span>. This function as <span class="math notranslate nohighlight">\(n\)</span> first order partial derivatives making up the gradient vector. And each of these
<span class="math notranslate nohighlight">\(n\)</span> function can be derived again with respect to each  of the <span class="math notranslate nohighlight">\(n\)</span> variables. Thus deriving a function first with respect to <span class="math notranslate nohighlight">\(x_i\)</span> yields the first order partial derivative
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x_i}\)</span> and again with respect to <span class="math notranslate nohighlight">\(x_j\)</span> yields the second order derivative <span class="math notranslate nohighlight">\(\frac{\partial^2 f}{\partial x_ix_j}\)</span>. Overall the <strong>hessian</strong> is a square symmetric matrix of second order derivatives.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f = 
\begin{bmatrix}
\frac{\partial^2 f }{\partial x^2_1}  &amp; \cdots &amp; \frac{\partial^2 f }{\partial x_1 \partial x_n}\\
\vdots  &amp; \ddots &amp; \vdots\\
\frac{\partial^2 f }{\partial x_n \partial x_1} &amp; \cdots &amp; \frac{\partial^2 f }{\partial x_n^2}
\end{bmatrix}
\end{split}\]</div>
<div class="example admonition">
<p class="admonition-title">Example</p>
<p>Consider the bivariate function <span class="math notranslate nohighlight">\(f(x,y) = x^2 y + 3xy + xy^2\)</span>, its gradient vector is the vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f =
\begin{bmatrix}
2xy + 3y + y^2\\
x^2 + 3x + x 2y
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial f}{\partial x}\\
\frac{\partial f}{\partial y}
\end{bmatrix}
\end{split}\]</div>
<p>From there we derive the hessian:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f = 
\begin{bmatrix}
2 &amp; 2x+3 + 2y\\ 
2x + 3 + 2y &amp; 2
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial^2 f }{\partial x^2} &amp; \frac{\partial^2 f }{\partial x \partial y}\\
 \frac{\partial^2 f }{\partial y \partial x}&amp; \frac{\partial^2 f }{\partial y^2}
\end{bmatrix}
\end{split}\]</div>
</div>
</section>
<section id="id2">
<h3>Numerical root finding methods<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>The multivariate numerical optimization problem amounts to solve the minimization problem:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = \mathop{\text{argmin}}_{\mathbf{x}\in\mathbb{R}^n} f(\mathbf{x})
\]</div>
<p>in case <span class="math notranslate nohighlight">\(f\)</span> is strictly convex, the solution to the problem exists and is unique. Most of the time multivariate optimization problems are not solved analytically
since they often involve solving complicated systems of non linear equations.</p>
<p>Rather numerical optimization methods are the standard way to go. We describe here generalizations of the gradient descent and the newton method to the multivariate case.
Both methods generate sequence of iterates starting from an initial random guess: <span class="math notranslate nohighlight">\(\mathbf{x}_1 \ldots \mathbf{x}_n\)</span>. This time the iterates are vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^n\)</span>.
The iteration ends up when the last iterate is sufficiently close to the minimum.</p>
<p>The <strong>multivariate gradient descent</strong> mirrors the univariate case. Here we take advantage of the property that the gradient vector points towards the steepest increase direction.
To achieve minimization, the update function of the algorithm amounts to move in the opposite direction of the gradient. Again <span class="math notranslate nohighlight">\(\alpha\)</span> is a positive real controlling the update step size.</p>
<div class="proof algorithm admonition" id="gd-algo-mv">
<p class="admonition-title"><span class="caption-number">Algorithm 4 </span> (multivariate gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>while</strong> <span class="math notranslate nohighlight">\( || \nabla f(\mathbf{x}_t) ||  &gt; \epsilon \)</span> <strong>do</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_{t+1} \gets \mathbf{x}_t - \alpha \nabla f(\mathbf{x}_t)\)</span></p></li>
</ul>
</section>
</div><div class="example admonition">
<p class="admonition-title">Example</p>
<p>We illustrate the first few steps of the gradient descent algorithm on the convex function <span class="math notranslate nohighlight">\( f(x_1,x_2) = 3x_1^2+ x^2_2 - x_1x_2 - 3 x_2 \)</span> given an initial iterate
at <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (9,9)\)</span>. We provide the successive iterates on the contour plot of the function as well as successive values of the gradient
norm computed at each update.</p>
<p><img alt="_images/e92ef4718bb3dfe96424f0e578f2485105c42bb883e7ba4a970cd13190ff769a.png" src="_images/e92ef4718bb3dfe96424f0e578f2485105c42bb883e7ba4a970cd13190ff769a.png" /></p>
<p>the gradient of this function is the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x}) =
\begin{bmatrix}
6 x_1 - x_2 \\
- x_1 + 2 x_2 - 3
\end{bmatrix}
\end{split}\]</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Example implementation of the multivariate gradient descent in numpy</span>
<span class="c1"># for the function f(x_1,x_2) = 3x_1^2+ x^2_2 - x_1x_2 - 3 x_2 </span>
<span class="c1"># and alpha = 0.2</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">grad_descent</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
	<span class="n">x</span>  <span class="o">=</span> <span class="n">x0</span>
	<span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
		<span class="n">x</span>     <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dx</span>
		<span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
		
		<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">x</span>

<span class="nb">min</span> <span class="o">=</span> <span class="n">grad_descent</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]),</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;minimum found&#39;</span><span class="p">,</span><span class="nb">min</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.  3.8]
[0.76 2.88]
[0.424 2.48 ]
[0.4112 2.1728]
[0.35232 1.98592]
[0.32672  1.862016]
[0.3070592 1.7825536]
[0.29509888 1.730944  ]
[0.28716902 1.69758618]
[0.28208343 1.67598551]
[0.27878042 1.66200799]
minimum found [0.27878042 1.66200799]
</pre></div>
</div>
</div>
</div>
<p>The <strong>Multivariate Newton method</strong> extends naturally the newton method to the multivariate case. It is provided as <a class="reference internal" href="#newton-mv">Algorithm 5</a>.
The key difference with other algorithms described so far is located in the <span class="math notranslate nohighlight">\(\alpha\)</span>. When optimizing a function <span class="math notranslate nohighlight">\(f(x_1\ldots x_n)\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span>
is now a square matrix of size <span class="math notranslate nohighlight">\(n\times n\)</span>, the inverse hessian <span class="math notranslate nohighlight">\(\mathbf{H}^{-1}_f(\mathbf{x}_t)\)</span>. This matrix contains second order derivatives
and is a counterpart of the second order derivative used in the univariate case. In some sense theres is not a unique learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> anymore but rather a matrix
of learning rates some of them related only to some partial functions.</p>
<div class="proof algorithm admonition" id="newton-mv">
<p class="admonition-title"><span class="caption-number">Algorithm 5 </span> (multivariate Newton)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>while</strong> <span class="math notranslate nohighlight">\( || \nabla f(\mathbf{x}_t) ||  &gt; \epsilon \)</span> <strong>do</strong></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha \gets \mathbf{H}^{-1}_f(\mathbf{x}_t)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}_{t+1} \gets \mathbf{x}_t - \alpha \nabla f(\mathbf{x}_t)\)</span></p></li>
</ul>
</section>
</div><p>To see where the newton update comes from, let us remind ourselves that for finding the minimum the method aims to solve a system of linear equations of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
0\\
\vdots\\
0\\
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial f}{\partial x_1}(\mathbf{x})\\
\vdots\\
\frac{\partial f}{\partial x_n}(\mathbf{x})\\
\end{bmatrix}
\end{split}\]</div>
<p>Since solving this system of equations is not straightforward, like in the univariate case, we rather approximate each partial derivative function by a linear approximation (a tangent plane to the partial derivative function).
The set of tangent plane equations for each  partial derivative and some point <span class="math notranslate nohighlight">\((\mathbf{x}_t,\frac{\partial f}{\partial x_i}(\mathbf{x}_t))\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
y_1 &amp;= \frac{\partial f}{\partial x_1}(\mathbf{x}_t) + \frac{\partial^2 f}{\partial x_1\partial x_1}(\mathbf{x}_t) (x_{1}-x_{t1})+ \ldots + 
\frac{\partial^2 f}{\partial x_1\partial x_n}(\mathbf{x}_t) (x_n-x_{tn}) \\
y_2 &amp;= \frac{\partial f}{\partial x_2}(\mathbf{x}_t) + \frac{\partial^2 f}{\partial x_2\partial x_1}(\mathbf{x}_t) (x_{1}-x_{t1})+ \ldots + 
\frac{\partial^2 f}{\partial x_2\partial x_n}(\mathbf{x}_t) (x_n-x_{tn}) \\
&amp;\vdots \\ 
y_n &amp;= \frac{\partial f}{\partial x_n}(\mathbf{x}_t) + \frac{\partial^2 f}{\partial x_n\partial x_1}(\mathbf{x}_t) (x_{1}-x_{t1})+ \ldots + 
\frac{\partial^2 f}{\partial x_n\partial x_n}(\mathbf{x}_t) (x_n-x_{tn}) \\
\end{align}
\end{split}\]</div>
<p>If we simplify the notation with vector matrix notation we get :</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = \nabla f(\mathbf{x}_t) + \mathbf{H}_f(\mathbf{x}_t) (\mathbf{x} - \mathbf{x}_t)
\]</div>
<p>the Newton step amounts to seek <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> for which <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{0}\)</span>, thus we solve:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\nabla f(\mathbf{x}_t) + \mathbf{H}_f(\mathbf{x}_t) (\mathbf{x} - \mathbf{x}_t) &amp;= \mathbf{0} \\
\mathbf{H}_f(\mathbf{x}_t) (\mathbf{x} - \mathbf{x}_t) &amp;= -\nabla f(\mathbf{x}_t) \\
\mathbf{x} - \mathbf{x}_t &amp;= - \mathbf{H}^{-1}_f(\mathbf{x}_t)   \nabla f(\mathbf{x}_t) \\
\mathbf{x} &amp;=  \mathbf{x}_t - \mathbf{H}^{-1}_f(\mathbf{x}_t)  \nabla f(\mathbf{x}_t) 
\end{align}
\end{split}\]</div>
<p>that we finally rewrite slightly to get the multivariate Newton update:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\alpha &amp;= \mathbf{H}^{-1}_f(\mathbf{x}_t) \\
\mathbf{x}_{t+1} &amp;=  \mathbf{x}_t - \alpha \nabla f(\mathbf{x}_t) 
\end{align}
\end{split}\]</div>
<div class="admonition-line-and-plane-equations admonition">
<p class="admonition-title">Line and plane equations</p>
<p>At several times  in this chapter it may be helpful to have in mind some line and plane equations.
The <em>univariate</em> line equation has the form</p>
<div class="math notranslate nohighlight">
\[
f(x) =  a x + b
\]</div>
<p>where <span class="math notranslate nohighlight">\(a\)</span> is the <strong>slope</strong> coefficient and <span class="math notranslate nohighlight">\(b\)</span> the <strong>intercept</strong> or the height <span class="math notranslate nohighlight">\(f(0)\)</span> of the point <span class="math notranslate nohighlight">\((0,f(0))\)</span> where the line crosses the <span class="math notranslate nohighlight">\(y\)</span>-axis.
When we know the slope and a point <span class="math notranslate nohighlight">\((x_t,f(x_t))\)</span> on the line where <span class="math notranslate nohighlight">\(x_t \not = 0\)</span> then the equation takes the form:</p>
<div class="math notranslate nohighlight">
\[
f(x) = a (x-x_t) + f(x_t)
\]</div>
<p>and observe that this generalized version reduces to the previous equation when <span class="math notranslate nohighlight">\(x_t = 0\)</span>.</p>
<p>In the <em>multivariate</em> case, this generalizes to the plane equation:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) =  a_1 x_1 + a_2 x_2 + \ldots + a_n x_n + b
\]</div>
<p>where each <span class="math notranslate nohighlight">\(a_i\)</span> is a partial slope  and <span class="math notranslate nohighlight">\(b\)</span> the intercept. Again in case we know the slopes and a point <span class="math notranslate nohighlight">\((\mathbf{x}_t,f(\mathbf{x}_t))\)</span>
the equation generalizes to :</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) =  a_1 (x_1-x_{t1}) + a_2 (x_2-x_{t2}) + \ldots + a_n (x_n-x_{tn}) + f(\mathbf{x}_t)
\]</div>
<p>which again reduces to the less general equation when <span class="math notranslate nohighlight">\(\mathbf{x}_t = \mathbf{0}\)</span>.
Finally observe that this last equation can be expressed with vector operations. Let <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1, a_2,\ldots a_n)\)</span>
be the vector of coefficients and <span class="math notranslate nohighlight">\((\mathbf{x}-\mathbf{x}_t) = (x_1-x_{t1}, x_2-x_{t2},\ldots x_n-x_{tn})\)</span>  then we can reformulate as:</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}) =  \mathbf{a}^\top (\mathbf{x}-\mathbf{x}_t)+ f(\mathbf{x}_t)
\]</div>
</div>
<div class="example admonition">
<p class="admonition-title">Example</p>
<p>We illustrate the first few steps of the Newton algorithm on the convex function <span class="math notranslate nohighlight">\(f(x_1,x_2) = 3x_1^2+ x^2_2 - x_1x_2 - 3 x_2\)</span> given an initial iterate
at <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (9,9)\)</span>.
The gradient of this function is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla f(\mathbf{x}) = 
\begin{bmatrix}
6 x_1 - x_2\\
-x_1 + 2 x_2 - 3
\end{bmatrix}
\end{split}\]</div>
<p>and the hessian matrix :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}f(\mathbf{x}) =
\begin{bmatrix}
6 &amp; -1\\
-1 &amp; 2
\end{bmatrix}
\end{split}\]</div>
<p>We provide the successive iterates on the contour plot of the function.
As can be seen the Newton method finds the solution in one iteration.</p>
<p><img alt="_images/26a67b599eef059447b6175ad03bc039c1a44537b4a3f2a3a2ef9a103b896032.png" src="_images/26a67b599eef059447b6175ad03bc039c1a44537b4a3f2a3a2ef9a103b896032.png" /></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Example implementation of the multivariate Newton in numpy</span>
<span class="c1">#for the function f(x_1,x_2) = 3x_1^2+ x^2_2 - x_1x_2 - 3 x_2 </span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.linalg</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">inv</span>

<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span> <span class="o">@</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]])</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">hessian</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">):</span>
	<span class="n">x</span>  <span class="o">=</span> <span class="n">x0</span>
	<span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">while</span> <span class="n">norm</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">epsilon</span><span class="p">:</span>
		<span class="n">alpha</span> <span class="o">=</span> <span class="n">inv</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
		<span class="n">x</span>     <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">@</span> <span class="n">dx</span>
		<span class="n">dx</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

		<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">x</span>

<span class="nb">min</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">9</span><span class="p">,</span><span class="mi">9</span><span class="p">]),</span><span class="n">grad</span><span class="p">,</span><span class="n">hessian</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;minimum found&#39;</span><span class="p">,</span><span class="nb">min</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.27272727 1.63636364]
minimum found [0.27272727 1.63636364]
</pre></div>
</div>
</div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The Newton method is very efficient for many traditional convex optimization problems.
The multivariate method has a weakness in terms of computational complexity: building a square hessian matrix is quadratic, <span class="math notranslate nohighlight">\({\cal O}(n^2)\)</span>,
and matrix inversion is an algorithm with cubic time complexity in <span class="math notranslate nohighlight">\({\cal O}(n^3)\)</span>. In modern machine learning
we often use loss functions where the number <span class="math notranslate nohighlight">\(n\)</span> of variables is high and where the computation of the inverse hessian matrix becomes
a bottleneck that may cause the newton method to become unusable. For this reason there is a large family of algorithms that try to approximate an <span class="math notranslate nohighlight">\(\alpha\)</span>
without computing the true hessian matrix. This is the family of quasi-newton algorithms that is mostly used in modern machine learning. The secant method we described for the univariate case
is an example of such a method: it approximates a second order derivative without ever computing it.</p>
</div>
</section>
</section>
<section id="non-convex-optimization">
<h2>Non convex optimization<a class="headerlink" href="#non-convex-optimization" title="Permalink to this heading">#</a></h2>
<p>Most classical statistical models are convex (and in fact almost linear). Linear regression and logistic regression, the perceptron algorithm (although not differentiable everywhere)
and one gets  a unique global minimum whatever the initial guess <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Current deep learning models are generally not convex.</p>
<p>When the problem is not convex anymore (but still assume a continuous function), there is no more guarantee that</p>
<ul class="simple">
<li><p>the function has a unique minimum, there may be several local minima (or even none). Thus the initial guess <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> matters.
Different choices of the initial guess <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> may lead to different solutions</p></li>
<li><p>when the gradient is zero, the critical point is a minimum: instead it may be a maximum or a saddle point.</p></li>
</ul>
<p>In machine learning, most of the time, non convex optimization problems are problems that are not radically different from convex problems.
The functions are continuous but not necessarily differentiable and they have local or global minima.</p>
<div class="example admonition">
<p class="admonition-title">Example</p>
<p>On the left, an example of a non convex function with several local minima.
On the right, an example of non convex function <span class="math notranslate nohighlight">\(f(x)=x^3\)</span> where the derivative is 0 at a point <span class="math notranslate nohighlight">\((0,0)\)</span> that is not an extremum.
Instead the minimum of the function is located on a point where the derivative is non null at the limit of the interval.</p>
<p><img alt="_images/c1cac2d3979fe1eb6c7606801ba2808fa98012e307cf6b122e46bf094dbc585a.png" src="_images/c1cac2d3979fe1eb6c7606801ba2808fa98012e307cf6b122e46bf094dbc585a.png" /></p>
</div>
<p>Non convex problems may cause difficulties to methods that are otherwise well defined for convex problems. For instance the Newton method may perform too agressive updates
and diverge. In practice, gradient descent methods with some improvements are widely used when optimizing such non convex functions.</p>
<div class="example admonition">
<p class="admonition-title">Example</p>
<p>Here is an example of a non convex optimisation problem with the Rosenbrock function : <span class="math notranslate nohighlight">\(f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2\)</span>
whose unique minimum is <span class="math notranslate nohighlight">\((1,1)\)</span></p>
<p><img alt="_images/c78b94e53a7b01a00059c1232f0f0870f7db7d1948d3c86d282e255a5d503130.png" src="_images/c78b94e53a7b01a00059c1232f0f0870f7db7d1948d3c86d282e255a5d503130.png" /></p>
<p>The gradient of this function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla f = 
\begin{bmatrix}
 2x_1 - 2 - 400 x_1 (x_2 -x_1^2) \\
 200 (x_2 -  x_1^2)
\end{bmatrix}
\end{split}\]</div>
<p>and the hessian:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f =
\begin{bmatrix}
2 (600x_1^2 - 200x_2 + 1) &amp; -400 x_1\\
-400 x_1 &amp; 200
\end{bmatrix}
\end{split}\]</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="n">x1</span><span class="p">,</span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="o">*</span><span class="n">x1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">400</span> <span class="o">*</span> <span class="n">x1</span> <span class="o">*</span><span class="p">(</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">200</span><span class="o">*</span><span class="p">(</span><span class="n">x2</span><span class="o">-</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
	<span class="n">x1</span><span class="p">,</span><span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
	<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="mi">600</span><span class="o">*</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mi">200</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>  <span class="o">-</span><span class="mi">400</span><span class="o">*</span> <span class="n">x1</span><span class="p">],[</span> <span class="o">-</span><span class="mi">400</span> <span class="o">*</span> <span class="n">x1</span><span class="p">,</span> <span class="mi">200</span><span class="p">]])</span>
	

<span class="n">minx</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">]),</span><span class="n">grad</span><span class="p">,</span><span class="n">hessian</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;minimum found&#39;</span><span class="p">,</span><span class="n">minx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1.99916736 3.99666944]
[1.00013852 0.00221845]
[1.00013783 1.00027568]
[1.         0.99999998]
minimum found [1.         0.99999998]
</pre></div>
</div>
</div>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="stats.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Applications to statistical analysis : latent semantic analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="optim_exercises.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimization exercises</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-and-least-squares">Linear regression and least squares</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization-of-univariate-functions">Convex optimization of univariate functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity">Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pointwise-derivative">Pointwise derivative</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-derivative-function">The derivative function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analytical-root-finding-method">Analytical root finding method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-root-finding-methods">Numerical root  finding methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convex-optimization-of-multivariate-functions">Convex optimization of multivariate functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Convexity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives-and-the-gradient">Partial derivatives and the gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Numerical root finding methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-convex-optimization">Non convex optimization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Benoit Crabbe
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>