{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62543973",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f029b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:17.833965Z",
     "iopub.status.busy": "2023-11-21T18:42:17.833630Z",
     "iopub.status.idle": "2023-11-21T18:42:17.842938Z",
     "shell.execute_reply": "2023-11-21T18:42:17.842170Z"
    }
   },
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d2dc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42461c",
   "metadata": {},
   "source": [
    "# Optimization exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782d3d",
   "metadata": {},
   "source": [
    "As should be clear there is no silver bullet for optimizing any function. Although convex functions are the theoretically easy case, \n",
    "non convex optimization is becoming the classical use case since the advent of deep learning. \n",
    "In practice it is important to monitor the optimization process and to be able to understand when the optimization works well, struggles or entirely fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd19e35-dd6d-4585-9153-14729d45db7d",
   "metadata": {},
   "source": [
    "**Important Note** some of these exercises are automatically graded. You have to pay attention to:\n",
    "\n",
    "- Never modify existing function signatures. You may add other functions and testing code if you like. It does not impact grading. \n",
    "- Pass the assertion tests found in the notebook. These tests and others you cannot see are used to grade part of your homework\n",
    "- Pay attention to provide reasonably efficient solutions. Any notebook cell that takes more than 30 seconds at execution is considered as failed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d0e87",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 \n",
    "\n",
    "For the function $f(x) = (3x-2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ee495e",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:17.847881Z",
     "iopub.status.busy": "2023-11-21T18:42:17.847619Z",
     "iopub.status.idle": "2023-11-21T18:42:18.549395Z",
     "shell.execute_reply": "2023-11-21T18:42:18.548400Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20bee017f34500e41e595edfe87807ee",
     "grade": false,
     "grade_id": "cell-d79f91cfe4432eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x113177f10>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def fun1(x):\n",
    "    return (3*x-2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "x = np.arange(-3,3,0.1)\n",
    "y = fun1(x)\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f813e3",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.553201Z",
     "iopub.status.busy": "2023-11-21T18:42:18.552921Z",
     "iopub.status.idle": "2023-11-21T18:42:18.561538Z",
     "shell.execute_reply": "2023-11-21T18:42:18.560964Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01918c356e78cd7dacd0a82f04612bbc",
     "grade": false,
     "grade_id": "cell-d8d0acd6d5c9fd34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666704161968044\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    return 18*x - 12\n",
    "\n",
    "\n",
    "def fsec1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    return 18\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent(x0,fprime,alpha0=1.0,decay=0.,epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : scheduler decay \n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \"\"\"   \n",
    "    xprev =  x0 \n",
    "    x     =  x0+2*epsilon\n",
    "    t     = 0\n",
    "    alpha = alpha0\n",
    "    while np.abs(x-xprev) > epsilon:\n",
    "        xprev = x\n",
    "        dx    = fprime(x)\n",
    "        alpha = alpha/(1+decay*t)\n",
    "        x     = x - alpha * dx\n",
    "    return x    \n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton(x0,fprime,fsec,beta0=1.0,decay=0.0,epsilon=0.000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        fsec(functional)   : the second derivative function\n",
    "        beta0 (float)      : the initial heuristic learning rate to be used by the scheduler\n",
    "        decay(float)       : the scheduler decay (like in gradient descent)\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited \n",
    "#   for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 0.1,'decay':0,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta':1.0,'decay':0,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "    \n",
    "# ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION/FIND HYPERPARAMETERS\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n",
    "\n",
    "\n",
    "print ( gradient_descent(10,fprime1,\n",
    "                          alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                          decay=hyper1['gradient_descent']['decay'],\n",
    "                          epsilon=hyper1['gradient_descent']['epsilon']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf15609b-5a5d-4c62-811c-45191d821949",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.564644Z",
     "iopub.status.busy": "2023-11-21T18:42:18.564437Z",
     "iopub.status.idle": "2023-11-21T18:42:18.832521Z",
     "shell.execute_reply": "2023-11-21T18:42:18.832030Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "909dbced9707b22c38aacaaaef51e1cb",
     "grade": true,
     "grade_id": "cell-7881f5b6bc2d69e6",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fsec1(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m18\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m  math\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m, gradient_descent(\u001b[38;5;241m10\u001b[39m,fprime1,\n\u001b[1;32m     14\u001b[0m                           alpha0\u001b[38;5;241m=\u001b[39mhyper1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_descent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m                           decay\u001b[38;5;241m=\u001b[39mhyper1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_descent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     16\u001b[0m                           epsilon\u001b[38;5;241m=\u001b[39mhyper1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_descent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m]),abs_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m) \n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m  math\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[43mnewton\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfprime1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfsec1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mbeta0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnewton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnewton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnewton\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,abs_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m) \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m### BEGIN HIDDEN TESTS\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x0 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[0;32mIn[3], line 84\u001b[0m, in \u001b[0;36mnewton\u001b[0;34m(x0, fprime, fsec, beta0, decay, epsilon)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    x0        (float)  : the initial iterate\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 0.1,'epsilon':0.001,'decay':0.001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0.0,'epsilon':0.00001}}\n",
    "### END HIDDEN TESTS\n",
    "import math\n",
    "\n",
    "### TEST EXAMPLES\n",
    "\n",
    "assert fprime1(1) == 6 \n",
    "\n",
    "assert fsec1(-1)  == 18\n",
    "\n",
    "assert  math.isclose(2/3, gradient_descent(10,fprime1,\n",
    "                          alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                          decay=hyper1['gradient_descent']['decay'],\n",
    "                          epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.01) \n",
    "\n",
    "assert  math.isclose(2/3, newton(-10,fprime1,fsec1,\n",
    "                                      beta0=hyper1['newton']['beta0'],\n",
    "                                      decay=hyper1['newton']['decay'],\n",
    "                                      epsilon=hyper1['newton']['epsilon']),abs_tol=0.0001) \n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for x0 in range(-4,5,1):\n",
    "    assert math.isclose(2/3,gradient_descent(x0,fprime1,\n",
    "                                                 alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                                                 decay=hyper1['gradient_descent']['decay'],\n",
    "                                                 epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.01) \n",
    "for x0 in range(-4,5,1):\n",
    "    assert math.isclose(2/3,newton(x0,fprime1,\n",
    "                                       fsec1,\n",
    "                                       beta0=hyper1['newton']['beta0'],\n",
    "                                       decay=hyper1['newton']['decay'],\n",
    "                                       epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.0001) \n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41be6de",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1df2c-922a-4550-852a-f55db16a9c6e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a8fbd6216bd89fd45158fabefdbfc76",
     "grade": true,
     "grade_id": "cell-cf0a6365f55d71c3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a1ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2\n",
    "\n",
    "For the function $f(x) = x (x-2) (x+2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f8fba10",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.836068Z",
     "iopub.status.busy": "2023-11-21T18:42:18.835873Z",
     "iopub.status.idle": "2023-11-21T18:42:18.849002Z",
     "shell.execute_reply": "2023-11-21T18:42:18.848431Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45a306d24fa3f803881189cf0003a9b1",
     "grade": false,
     "grade_id": "cell-52bab07b7893d341",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# PLOT SECTION\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun2(x):\n",
    "    return x * (x-2) * (x+2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f47138",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.851675Z",
     "iopub.status.busy": "2023-11-21T18:42:18.851491Z",
     "iopub.status.idle": "2023-11-21T18:42:18.855453Z",
     "shell.execute_reply": "2023-11-21T18:42:18.854946Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9a5ee9b7431b17290802e05a95c199",
     "grade": false,
     "grade_id": "cell-1c1b1c3c6e926549",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def fsec2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8848e296-620e-42ec-96a4-c5291dcb24fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.857844Z",
     "iopub.status.busy": "2023-11-21T18:42:18.857667Z",
     "iopub.status.idle": "2023-11-21T18:42:18.880981Z",
     "shell.execute_reply": "2023-11-21T18:42:18.880492Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d60d5d62f69ad1af186c5abc6d633f91",
     "grade": true,
     "grade_id": "cell-430469fb0cff2901",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m### TEST EXAMPLES\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mfprime2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m fsec2(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     12\u001b[0m solset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1.28077641\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.7807764064043429\u001b[39m]\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mfprime2\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    x (float) : the x value\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    float. the first order derivative at x\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 0.0001,'epsilon':0.00001},\n",
    "          \"newton\"          :{'epsilon':0.00001}}\n",
    "### END HIDDEN TESTS\n",
    "import math\n",
    "\n",
    "### TEST EXAMPLES\n",
    "\n",
    "assert fprime2(1) == -6 \n",
    "assert fsec2(-1)  == -8\n",
    "\n",
    "solset = [-2, 1.28077641,-0.7807764064043429]\n",
    "\n",
    "sol = gradient_descent(10,fprime2,\n",
    "                          alpha0=hyper2['gradient_descent']['alpha0'],\n",
    "                          epsilon=hyper2['gradient_descent']['epsilon'])\n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "sol = newton(-10,fprime2,fsec2,epsilon=hyper2['newton']['epsilon']) \n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "xinit  = [-3,-1,1,2]\n",
    "\n",
    "for x0 in xinit:\n",
    "    sol = gradient_descent(x0,fprime2,alpha0=hyper2['gradient_descent']['alpha0'],\n",
    "                                      epsilon=hyper2['gradient_descent']['epsilon'])\n",
    "    assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "\n",
    "for x0 in xinit:\n",
    "    sol = newton(x0,fprime2,fsec2, epsilon=hyper2['gradient_descent']['epsilon']) \n",
    "    assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ed057",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bec1e-f510-4461-a3e7-add37d2315fb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a69839a3c1b1d588613aa5876c8a12d",
     "grade": true,
     "grade_id": "cell-f41e03369aa87879",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb3247",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3\n",
    "\n",
    "For the function $f(x) = -e^{-(3x-1)^2} + \\frac{x^2}{100}$. \n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9bcdf1b",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.884075Z",
     "iopub.status.busy": "2023-11-21T18:42:18.883892Z",
     "iopub.status.idle": "2023-11-21T18:42:18.896859Z",
     "shell.execute_reply": "2023-11-21T18:42:18.896404Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7fde5e26b7e279781db79e4d1560bbf",
     "grade": false,
     "grade_id": "cell-2a439514a8324088",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m exp(\u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m+\u001b[39m (x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#PLOT SECTION\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "def fun3(x):\n",
    "    return - exp(-(3*x-1)**2)+ (x**2) / 100 \n",
    "\n",
    "\n",
    "#PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3547dc8a",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.899573Z",
     "iopub.status.busy": "2023-11-21T18:42:18.899391Z",
     "iopub.status.idle": "2023-11-21T18:42:18.903209Z",
     "shell.execute_reply": "2023-11-21T18:42:18.902757Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68a144bc6dd6a327fd337f64a0e09a72",
     "grade": false,
     "grade_id": "cell-3d46db14698e4ffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy import exp\n",
    "\n",
    "def fprime3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def fsec3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper3 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "\n",
    "# it will not be graded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "362cc8b1-378e-48d8-96b3-99f70364a0b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.905689Z",
     "iopub.status.busy": "2023-11-21T18:42:18.905504Z",
     "iopub.status.idle": "2023-11-21T18:42:18.937074Z",
     "shell.execute_reply": "2023-11-21T18:42:18.936567Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0a220365dc55259f4e1c01029247393",
     "grade": true,
     "grade_id": "cell-14215a159d01e168",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m### END HIDDEN TESTS\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m math\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m,\u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfprime3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43malpha0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,abs_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m) \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m math\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m, newton(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m,fprime3,fsec3,\n\u001b[1;32m     19\u001b[0m                                 beta0 \u001b[38;5;241m=\u001b[39m hyper3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     20\u001b[0m                                 decay \u001b[38;5;241m=\u001b[39m hyper3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m                                 epsilon\u001b[38;5;241m=\u001b[39mhyper3[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m]),abs_tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m### BEGIN HIDDEN TESTS\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(x0, fprime, alpha0, decay, epsilon)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m np\u001b[38;5;241m.\u001b[39mabs(x\u001b[38;5;241m-\u001b[39mxprev) \u001b[38;5;241m>\u001b[39m epsilon:\n\u001b[1;32m     54\u001b[0m     xprev \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 55\u001b[0m     dx    \u001b[38;5;241m=\u001b[39m \u001b[43mfprime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mdecay\u001b[38;5;241m*\u001b[39mt)\n\u001b[1;32m     57\u001b[0m     x     \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m*\u001b[39m dx\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mfprime3\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    x (float) : the x value\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    float. the first order derivative at x\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############# TEST CELL #################\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "hyper3['gradient_descent']['alpha0'] = 0.001\n",
    "hyper3['newton']['beta'] = 1.0\n",
    "hyper3['newton']['decay'] = 0.01\n",
    "hyper3['newton']['epsilon'] = 0.0001\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "assert math.isclose(1/3,gradient_descent(6,fprime3,\n",
    "                                           alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                           decay=hyper3['gradient_descent']['decay'],\n",
    "                                           epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "\n",
    "assert math.isclose(1/3, newton(-4,fprime3,fsec3,\n",
    "                                beta0 = hyper3['newton']['beta0'],\n",
    "                                decay = hyper3['newton']['decay'],\n",
    "                                epsilon=hyper3['newton']['epsilon']),abs_tol=0.001)\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "xinit = [-7,-5,-3,-1,1,3,5,7]\n",
    "\n",
    "for x0 in xinit:\n",
    "    assert math.isclose(1/3,gradient_descent(x0,fprime3,\n",
    "                                                 alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                                 decay=hyper3['gradient_descent']['decay'],\n",
    "                                                 epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "for x0 in xinit:\n",
    "    assert math.isclose(1/3,newton(x0,fprime3,fsec3,\n",
    "                                   beta0 = hyper3['newton']['beta0'],\n",
    "                                   decay = hyper3['newton']['decay'],\n",
    "                                   epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaaf0f",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb49d-bffc-4418-9ef8-93f6ad9ec040",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a80e10beb700b7a2f356537cab94cc8",
     "grade": true,
     "grade_id": "cell-271653af45566715",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd69bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "For the function $f(x_1,x_2) = \\sum_{i=1}^2 x_i^4 - 16x_i^2 + 5 x_i$. \n",
    "   1. Plot the function for $x_1 \\in [-5,5]$ and $x_2 \\in [-5,5]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21f406c",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.940407Z",
     "iopub.status.busy": "2023-11-21T18:42:18.940223Z",
     "iopub.status.idle": "2023-11-21T18:42:18.953948Z",
     "shell.execute_reply": "2023-11-21T18:42:18.953397Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efde0f5dd286f16292347928690b3ed4",
     "grade": false,
     "grade_id": "cell-bbd5f5bd932369ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum( x[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m16\u001b[39m\u001b[38;5;241m*\u001b[39mx[i]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m*\u001b[39mx[i]  \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m] )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# PLOT SECTION\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun4(x):\n",
    "    return np.sum( x[i]**4 - 16*x[i]**2 + 5*x[i]  for i in [0,1] )\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3e12b6",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.957022Z",
     "iopub.status.busy": "2023-11-21T18:42:18.956794Z",
     "iopub.status.idle": "2023-11-21T18:42:18.961994Z",
     "shell.execute_reply": "2023-11-21T18:42:18.961491Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "705d04256b4328e5e84d1b893f48056c",
     "grade": false,
     "grade_id": "cell-b989ef95391d75eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy.linalg import inv,norm\n",
    "\n",
    "def grad4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def hess4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent_mv(x0,grad,alpha0=1.0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton_mv(x0,grad,hessian,beta0=1.,decay=0.0,epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        hessian(functional): the hessian function\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper4 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# e.g test with scipy.optimize.minimize\n",
    "# it will not be graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c44942-199c-4092-9516-12ae01f899c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.964994Z",
     "iopub.status.busy": "2023-11-21T18:42:18.964792Z",
     "iopub.status.idle": "2023-11-21T18:42:18.993813Z",
     "shell.execute_reply": "2023-11-21T18:42:18.993304Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98f03bc41ff9ee502649a8f53e5d39d6",
     "grade": true,
     "grade_id": "cell-0bf436b40ac9c469",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m solution_set \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.90353404\u001b[39m,  \u001b[38;5;241m2.74680293\u001b[39m], [\u001b[38;5;241m2.74680289\u001b[39m, \u001b[38;5;241m2.74680289\u001b[39m], [ \u001b[38;5;241m2.74680277\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.90353403\u001b[39m],[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.90353403\u001b[39m , \u001b[38;5;241m2.74680277\u001b[39m],[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.90353403\u001b[39m , \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.90353403\u001b[39m]])\n\u001b[1;32m      9\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m sol \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent_mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad4\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper4\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper4\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m(np\u001b[38;5;241m.\u001b[39misclose(s,sol,atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000001\u001b[39m)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m solution_set)\n\u001b[1;32m     14\u001b[0m sol \u001b[38;5;241m=\u001b[39m newton_mv(x0,grad4,hess4, beta0 \u001b[38;5;241m=\u001b[39m hyper4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m'\u001b[39m], decay \u001b[38;5;241m=\u001b[39m hyper4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m'\u001b[39m],epsilon\u001b[38;5;241m=\u001b[39mhyper4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mgradient_descent_mv\u001b[0;34m(x0, grad, alpha0, decay, epsilon)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    x0   (numpy.array) : the initial iterate\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    numpy.array. The value of the last iterate\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper4['gradient_descent']['alpha0'] = 0.002\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "### TEST SECTION ###\n",
    "\n",
    "solution_set = np.array([ [-2.90353404,  2.74680293], [2.74680289, 2.74680289], [ 2.74680277, -2.90353403],[-2.90353403 , 2.74680277],[-2.90353403 , -2.90353403]])\n",
    "\n",
    "x0 = np.array([-10,10])\n",
    "sol = gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n",
    "\n",
    "\n",
    "sol = newton_mv(x0,grad4,hess4, beta0 = hyper4['newton']['beta0'], decay = hyper4['newton']['decay'],epsilon=hyper4['newton']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n",
    "\n",
    "### BEGIN HIDDEN TESTS \n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    sol = gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon'])\n",
    "    assert any(np.isclose(s,sol,atol=0.00001).all() for s in solution_set)\n",
    "\n",
    "for x0 in inits:\n",
    "    sol = newton_mv(x0,grad4,hess4, beta0 = hyper4['newton']['beta0'], decay = hyper4['newton']['decay'],epsilon=hyper4['newton']['epsilon'])\n",
    "    assert any(np.isclose(s,sol,atol=0.00001).all() for s in solution_set)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab159",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? \n",
    " - What are the key properties of this function ? is it convex ? how many solutions can you identify ?\n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, beta epsilon ?\n",
    "\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e75a4c-92fe-4d84-9c0d-83fea3108137",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9e1489ff2d397949d7e37795fc37e96",
     "grade": true,
     "grade_id": "cell-3007bdb2c8d1eabb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924427af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "For the function  $f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2$ \n",
    "\n",
    "   1. Plot the function for $x_1 \\in [-2,2]$ and $x_2 \\in [-2,2]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85bffa7",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:18.998671Z",
     "iopub.status.busy": "2023-11-21T18:42:18.998448Z",
     "iopub.status.idle": "2023-11-21T18:42:19.013355Z",
     "shell.execute_reply": "2023-11-21T18:42:19.012865Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f71f00c21b1f875db5543bd77879b2fc",
     "grade": false,
     "grade_id": "cell-2884df9df649664e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# PLOT SECTION\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun5(x):\n",
    "    return (1-x[0])**2 + 100 * (x[1]-x[0]**2)**2\n",
    "\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726bda64",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:19.016279Z",
     "iopub.status.busy": "2023-11-21T18:42:19.016071Z",
     "iopub.status.idle": "2023-11-21T18:42:19.019926Z",
     "shell.execute_reply": "2023-11-21T18:42:19.019388Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c67c26001d8aa8b715fb3ade3f5982e6",
     "grade": false,
     "grade_id": "cell-c61847ed7bff74a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def grad5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def hess5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper5 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# it will not be graded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c27fdb57-1452-40ab-a481-29f85f7e84de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:19.022618Z",
     "iopub.status.busy": "2023-11-21T18:42:19.022420Z",
     "iopub.status.idle": "2023-11-21T18:42:19.049323Z",
     "shell.execute_reply": "2023-11-21T18:42:19.048650Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "916a9476d7db6eab8795eb301b3a3dd7",
     "grade": true,
     "grade_id": "cell-67b3ad764a181171",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m x0  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m      9\u001b[0m rsol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m sol \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent_mv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgrad5\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper5\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhyper5\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper5\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(rsol,sol,atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     14\u001b[0m sol \u001b[38;5;241m=\u001b[39m newton_mv(x0,grad5,hess5,beta0 \u001b[38;5;241m=\u001b[39m hyper5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m'\u001b[39m], decay\u001b[38;5;241m=\u001b[39mhyper5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m'\u001b[39m], epsilon\u001b[38;5;241m=\u001b[39mhyper5[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 52\u001b[0m, in \u001b[0;36mgradient_descent_mv\u001b[0;34m(x0, grad, alpha0, decay, epsilon)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    x0   (numpy.array) : the initial iterate\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    numpy.array. The value of the last iterate\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "######### TEST SECTION #################\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "hyper5['gradient_descent']['alpha0'] = 0.0001\n",
    "hyper5['gradient_descent']['decay'] = 0.0\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "x0  = np.array([3,-3])\n",
    "rsol = np.array([1,1])\n",
    "\n",
    "sol = gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], decay= hyper5['gradient_descent']['decay'], epsilon=hyper5['gradient_descent']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.0001).all()\n",
    "\n",
    "sol = newton_mv(x0,grad5,hess5,beta0 = hyper5['newton']['beta0'], decay=hyper5['newton']['decay'], epsilon=hyper5['newton']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.000001).all()\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    sol = gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], decay= hyper5['gradient_descent']['decay'], epsilon=hyper5['gradient_descent']['epsilon'])\n",
    "    assert np.isclose(rsol,sol,atol=0.0001).all()\n",
    "    \n",
    "for x0 in inits:\n",
    "    sol = newton_mv(x0,grad5,hess5,beta0 = hyper5['newton']['beta0'], decay=hyper5['newton']['decay'], epsilon=hyper5['newton']['epsilon'])\n",
    "    assert np.isclose(rsol,sol,atol=0.000001).all()\n",
    "### END HIDDEN TESTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b89f14",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a902d11-60c2-42de-a84b-a35042214215",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "747bb3229019773f51de9f2684c8d08b",
     "grade": true,
     "grade_id": "cell-7c9a207e488bfc1f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c747d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "In this exercise we implement parameter estimation for the multivariate linear regression model on the `Iris` dataset. The first step if to preprocess the raw dataset in order to get a design matrix  $\\mathbf{X}\\in\\mathbb{R}^{n\\times k}$ with $n$ lines whose $k$ columns are predictors and a $\\mathbf{y}$ vector\n",
    "with reference values to be predicted. In matrix notation the sum of squares loss can be reformulated as:\n",
    "\n",
    "$$\n",
    "ssq_{\\cal D}(\\mathbf{p}) = \\sum_{i=1}^n(\\mathbf{X}\\mathbf{p} - \\mathbf{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "    \n",
    "   1. Compute the gradient. To do so you may compute it analytically or take advantage of sympy. \n",
    "      The result can be expressed in matrix form as $\\nabla ssq_{\\cal D}(\\mathbf{p}) = \\mathbf{X}^\\top (\\mathbf{X}\\mathbf{p} - \\mathbf{y})$ \n",
    "   2. Compute the hessian.  To do so you may compute it analytically or take advantage of sympy.\n",
    "      The result can be expressed in matrix form as $\\mathbf{H}_{ssq_{\\cal D}} (\\mathbf{p}) = \\mathbf{X}^\\top \\mathbf{X}$\n",
    "   3. Optimize the function with gradient descent\n",
    "   4. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1611a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:19.052817Z",
     "iopub.status.busy": "2023-11-21T18:42:19.052567Z",
     "iopub.status.idle": "2023-11-21T18:42:19.772653Z",
     "shell.execute_reply": "2023-11-21T18:42:19.772053Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris variable names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pa\n",
    "\n",
    "\n",
    "#PREPARE DATA SECTION\n",
    "#grabs the full iris dataset from the sklearn library \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print('Iris variable names',iris.feature_names)\n",
    "iris = pa.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "def do_data_matrix(iris_frame,x_colnames,y_colname,add_bias=True):\n",
    "    \"\"\"\n",
    "    Creates a numpy matrix encoding the dataset.\n",
    "    It converts a pandas dataframe to a couple (X,y) of numpy arrays \n",
    "    Args:\n",
    "        iris_frame (pandas.DataFrame): the iris dataframe\n",
    "        x_colnames (list) : list of strings, the predictor names\n",
    "        y_colname   (str) : the name of the predicted variable\n",
    "        add_bias    (bool): whether we add a bias to the model or not   \n",
    "    Returns:\n",
    "        (numpy.array,numpy.array). Tuple (X,y) the first element is a design matrix. \n",
    "                                   A matrix whose columns are x predictor variables with one row for each data line\n",
    "                                   The second element is a vector with the y values of the predicted variable\n",
    "    \"\"\"\n",
    "    X             = iris_frame[x_colnames].to_numpy()\n",
    "    nlines, ncols = X.shape\n",
    "    if add_bias:\n",
    "        X = np.concatenate([X,np.ones((nlines,1))],axis=1)\n",
    "        \n",
    "    return X, iris_frame[y_colname].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3321de48",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:19.776081Z",
     "iopub.status.busy": "2023-11-21T18:42:19.775740Z",
     "iopub.status.idle": "2023-11-21T18:42:19.784002Z",
     "shell.execute_reply": "2023-11-21T18:42:19.783328Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "150d3d85d33b9fdc002b874dcef44b3f",
     "grade": false,
     "grade_id": "cell-23957233a607f203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "\n",
    "def leastsq_loss(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params         (numpy.array): the vector of variables (linear regression parameters)\n",
    "        X              (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref           (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        float. the sum of squares value for this dataset and the current value of the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def gradient_lstsq(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "        X     (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref  (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        numpy.array. a gradient vector \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "def hessian_lstsq(params,X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "       X      (numpy.array): the design matrix (columns are predictor variables)\n",
    "    Returns:\n",
    "        numpy.array. a hessian matrix \n",
    "        \n",
    "    @note: for linear regression, the hessian is constant for a given dataset, \n",
    "    we keep the signature with params for homogeneity\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the current value of the loss\n",
    "#  - the current value of the iterates\n",
    "#  - for gradient descent, the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "# ...    \n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "    \n",
    "from numpy.linalg import inv,norm   \n",
    "\n",
    "\n",
    "def gradient_descent_lstsq(x0,loss_fnc,grad,alpha0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : a decay parameter for the scheduler\n",
    "        epsilon (float)    : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses these values when calling your optimization functions\n",
    "\n",
    "hyper_lstsq = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.01},\n",
    "               \"newton\"          :{'epsilon':0.00001}}\n",
    "    \n",
    "    \n",
    "def newton_lstsq(x0,loss_fnc,grad,hessian,beta0=1.,decay=0.,epsilon=0.0000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array)  : the initial iterate\n",
    "        loss_fnc(functional): the loss function\n",
    "        grad  (functional)  : the gradient function\n",
    "        hessian(functional) : the hessian function\n",
    "        epsilon             : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_lm(dataset,predictor_lst,predicted,optim_method,alpha0=0.1,beta0=0.1,decay=0,epsilon=0.001,add_bias=True):\n",
    "    \"\"\"\n",
    "    Given a data matrix, returns the estimates of the parameters using least squares estimation.\n",
    "    Optimisation is performed either with the Newton method or the gradient method.\n",
    "    \n",
    "    Args: \n",
    "        dataset (pandas.DataFrame) : a dataset\n",
    "        predictor_lst        (list): a list of strings with the predictor variable names\n",
    "        predicted             (str): the name of the predicted variable\n",
    "        optim_method          (str): either 'grad_descent' or 'newton'\n",
    "        epsilon             (float): epsilon value for the optimizer\n",
    "    Returns :\n",
    "        numpy.array: a vector with parameter estimates\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "lr_params = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "             \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447dcfd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2023-11-21T18:42:19.786875Z",
     "iopub.status.busy": "2023-11-21T18:42:19.786662Z",
     "iopub.status.idle": "2023-11-21T18:42:19.877923Z",
     "shell.execute_reply": "2023-11-21T18:42:19.877422Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d475f842c4e985346a8ee8be8b956fe",
     "grade": true,
     "grade_id": "cell-eb6a6ecaf1596345",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m### END HIDDEN TESTS\u001b[39;00m\n\u001b[1;32m      7\u001b[0m sols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([  \u001b[38;5;241m0.22282854\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.20726607\u001b[39m,  \u001b[38;5;241m0.52408311\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.24030739\u001b[39m ])\n\u001b[0;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfit_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43miris\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msepal width (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msepal length (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpetal length (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpetal width (cm)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43malpha0\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mlr_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdecay\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mlr_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mlr_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgradient_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepsilon\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43moptim_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrad_descent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(result,sols,atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     17\u001b[0m result \u001b[38;5;241m=\u001b[39m fit_lm(iris,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal width (cm)\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal length (cm)\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal length (cm)\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal width (cm)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m                 beta0  \u001b[38;5;241m=\u001b[39m  lr_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta0\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m                 decay   \u001b[38;5;241m=\u001b[39m  lr_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     20\u001b[0m                 epsilon \u001b[38;5;241m=\u001b[39m  lr_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     21\u001b[0m                 optim_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 124\u001b[0m, in \u001b[0;36mfit_lm\u001b[0;34m(dataset, predictor_lst, predicted, optim_method, alpha0, beta0, decay, epsilon, add_bias)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mGiven a data matrix, returns the estimates of the parameters using least squares estimation.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mOptimisation is performed either with the Newton method or the gradient method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    numpy.array: a vector with parameter estimates\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### TEST CELL ###\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "np.random.seed(12345)\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "sols = np.array([  0.22282854, -0.20726607,  0.52408311, -0.24030739 ])\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                alpha0  =  lr_params['gradient_descent']['alpha0'],\n",
    "                decay   =  lr_params['gradient_descent']['decay'], \n",
    "                epsilon =  lr_params['gradient_descent']['epsilon'], \n",
    "                optim_method = 'grad_descent')\n",
    "assert np.isclose(result,sols,atol=0.0001).all()\n",
    "\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                beta0  =  lr_params['newton']['beta0'],\n",
    "                decay   =  lr_params['newton']['decay'], \n",
    "                epsilon =  lr_params['newton']['epsilon'], \n",
    "                optim_method = 'newton')\n",
    "assert np.isclose(result,sols,atol=0.000001).all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
