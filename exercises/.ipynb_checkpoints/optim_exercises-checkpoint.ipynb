{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d42461c",
   "metadata": {},
   "source": [
    "# Optimization exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782d3d",
   "metadata": {},
   "source": [
    "As should be clear there is no silver bullet for optimizing any function. Although convex functions are the theoretically easy case, \n",
    "non convex optimization is becoming the classical use case since the advent of deep learning. \n",
    "In practice it is important to monitor the optimization process and to be able to understand when the optimization works well, struggles or entirely fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d0e87",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 \n",
    "\n",
    "For the function $f(x) = (3x-2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "95ee495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def fun1(x):\n",
    "    return (3*x-2)**2\n",
    "\n",
    "\n",
    "\n",
    "# PLOT SECTION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun1(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "89f813e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666703172082821\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return 18*x - 12\n",
    "    ################\n",
    "\n",
    "\n",
    "def fsec1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return 18\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "def gradient_descent(x0,fprime,alpha0=1.0,epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    xprev = x0\n",
    "    dx    = fprime(x0)\n",
    "    x     = x0 - alpha0 * dx\n",
    "    while abs(x-xprev) > epsilon:\n",
    "        xprev = x\n",
    "        x     = x - alpha0 * dx\n",
    "        dx    = fprime(x)\n",
    "    return x\n",
    "    ################\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed and the current iterate.\n",
    "\n",
    "def newton(x0,fprime,fsec,epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        fsec(functional)   : the second derivative function\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    xprev     = x0\n",
    "    dx        = fprime(x0)\n",
    "    x         = x0 - (1/fsec(x0)) * dx\n",
    "    while abs(x-xprev) > epsilon:\n",
    "        xprev = x\n",
    "        alpha = 1/fsec(x)\n",
    "        x     = x - alpha * dx\n",
    "        dx    = fprime(x)\n",
    "    return x\n",
    "    ################\n",
    "\n",
    "    \n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.00001},\n",
    "          \"newton\"          :{'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION/FIND HYPERPARAMETERS\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# it will not be graded\n",
    "\n",
    "\n",
    "\n",
    "############ TESTS ###########################\n",
    "hyper1['gradient_descent']['alpha0'] = 0.1\n",
    "print(gradient_descent(100,fprime1,alpha0=hyper1['gradient_descent']['alpha0'],epsilon=hyper1['gradient_descent']['epsilon'])) \n",
    "print(newton(100,fprime1,fsec1,epsilon=hyper1['newton']['epsilon']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41be6de",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a1ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2\n",
    "\n",
    "For the function $f(x) = x (x-2) (x+2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "8f8fba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun2(x):\n",
    "    return x * (x-2) * (x+2)**2\n",
    "\n",
    "\n",
    "# PLOT SECTION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun2(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "93f47138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0006052614301586\n",
      "-0.7807764064043429\n",
      "      fun: -9.914949590828147\n",
      " hess_inv: array([[6.48915929]])\n",
      "      jac: array([-2.81928259e-10])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 9\n",
      "      nit: 2\n",
      "     njev: 9\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([1.28077641])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcrabbe/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py:525: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return x*(x - 2)*(2*x + 4) + x*(x + 2)**2 + (x - 2)*(x + 2)**2\n",
    "    ################\n",
    "\n",
    "def fsec2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return 2*(x*(x - 2) + 2*x*(x + 2) + 2*(x - 2)*(x + 2) + (x + 2)**2)\n",
    "    ################\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.00001},\n",
    "          \"newton\"          :{'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# it will not be graded\n",
    "\n",
    "############ TESTS ###########################\n",
    "hyper2['gradient_descent']['alpha0'] = 0.001\n",
    "print(gradient_descent(-10,fprime2,alpha0=hyper2['gradient_descent']['alpha0'],epsilon=hyper2['gradient_descent']['epsilon'])) \n",
    "print(newton(-10,fprime2,fsec2,epsilon=hyper2['newton']['epsilon']))\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "print(minimize(fun2,-0.75,method=\"BFGS\",jac=fprime2,hess=fsec2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ed057",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb3247",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3\n",
    "\n",
    "For the function $f(x) = -e^{-(3x-1)^2} + \\frac{x^2}{100}$. \n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d9bcdf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "def fun3(x):\n",
    "    return - exp(-(3*x-1)**2)+ (x**2) / 100 \n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun3(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3547dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3335020324663581\n",
      "0.3329633736307264\n",
      "      fun: -0.9988901220873291\n",
      " hess_inv: array([[0.05549448]])\n",
      "      jac: array([6.66663896e-09])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 13\n",
      "      nit: 4\n",
      "     njev: 13\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.33296337])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcrabbe/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/_minimize.py:525: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy import exp\n",
    "\n",
    "def fprime3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    return x/50 - (6 - 18*x)*exp(-(3*x - 1)**2)\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "def fsec3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "    ### SOLUTION ###\n",
    "    return -36*(3*x - 1)**2 * exp(-(3*x - 1)**2) + 1/50 + 18 * exp(-(3*x - 1)**2)\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper3 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.00001},\n",
    "          \"newton\"          :{'epsilon':0.001}}\n",
    "\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "\n",
    "# it will not be graded\n",
    "\n",
    "############# TESTS #################\n",
    "hyper3['gradient_descent']['alpha0'] = 0.001\n",
    "\n",
    "\n",
    "print(gradient_descent(3,fprime3,alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                  epsilon=hyper3['gradient_descent']['epsilon'])) \n",
    "\n",
    "print(newton(16,fprime3,fsec3,epsilon=hyper3['newton']['epsilon']))\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "print(minimize(fun3,3,method=\"BFGS\",jac=fprime3,hess=fsec3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaaf0f",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd69bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "For the function $f(x_1,x_2) = \\sum_{i=1}^2 x_i^4 - 16x_i^2 + 5 x_i$. \n",
    "   1. Plot the function for $x_1 \\in [-5,5]$ and $x_2 \\in [-5,5]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d21f406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun4(x):\n",
    "    return np.sum( x[i]**4 - 16*x[i]**2 + 5*x[i]  for i in [0,1] )\n",
    "\n",
    "# PLOT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0e3e12b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.90353627  2.74681957]\n",
      "[-2.90354414 -2.90354414]\n",
      "[2.74681447 2.74681447]\n",
      "[ 2.74681957 -2.90353627]\n",
      "[-2.90353403  2.74680277]\n",
      "[-2.90354053 -2.90354053]\n",
      "[2.74680277 2.74680277]\n",
      "[ 2.74680277 -2.90353403]\n",
      "      fun: -128.39122471811072\n",
      " hess_inv: array([[ 0.01423515, -0.00021142],\n",
      "       [-0.00021142,  0.01688206]])\n",
      "      jac: array([1.84016457e-07, 1.47787219e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 15\n",
      "      nit: 13\n",
      "     njev: 15\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-2.90353403,  2.74680277])\n",
      "      fun: -156.66466281508565\n",
      " hess_inv: array([[ 0.50722856, -0.49277144],\n",
      "       [-0.49277144,  0.50722856]])\n",
      "      jac: array([-1.05148956e-09, -1.05148956e-09])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 10\n",
      "      nit: 9\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-2.90353403, -2.90353403])\n",
      "      fun: -100.11778662113576\n",
      " hess_inv: array([[ 0.50853927, -0.49146073],\n",
      "       [-0.49146073,  0.50853927]])\n",
      "      jac: array([3.33284902e-08, 3.33284902e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 10\n",
      "      nit: 9\n",
      "     njev: 10\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([2.74680277, 2.74680277])\n",
      "      fun: -128.39122471811072\n",
      " hess_inv: array([[ 0.01688206, -0.00021142],\n",
      "       [-0.00021142,  0.01423515]])\n",
      "      jac: array([1.47787219e-07, 1.84016457e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 15\n",
      "      nit: 13\n",
      "     njev: 15\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([ 2.74680277, -2.90353403])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcrabbe/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy.linalg import inv,norm\n",
    "\n",
    "def grad4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    return np.array([4*x[0]**3 - 32*x[0] + 5,4*x[1]**3 - 32*x[1] + 5])\n",
    "    ################\n",
    "\n",
    "def hess4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    return np.array([[4*(3*x[0]**2 - 8),0],[0,4*(3*x[1]**2 - 8)]])\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper4 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.001},\n",
    "          \"newton\"          :{'epsilon':0.001}}\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current value of the gradient norm.\n",
    "\n",
    "def gradient_descent_mv(x0,grad,alpha0=1.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    while norm(dx) > epsilon:\n",
    "        x     = x - alpha0 * dx \n",
    "        dx    = grad(x)\n",
    "    return x\n",
    "    ################\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current value of the gradient norm.\n",
    "\n",
    "def newton_mv(x0,grad,hessian,epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        hessian(functional): the hessian function\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    while norm(dx) > epsilon:\n",
    "        alpha = inv(hessian(x))\n",
    "        x     = x - alpha @ dx \n",
    "        dx    = grad(x)\n",
    "    return x\n",
    "    ################\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# it will not be graded\n",
    "hyper4['gradient_descent']['alpha0'] = 0.001\n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    print(gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon']))\n",
    "for x0 in inits:\n",
    "    print(newton_mv(x0,grad4,hess4,epsilon=hyper4['newton']['epsilon']))\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "for x0 in inits:\n",
    "    print(minimize(fun4,x0,method=\"BFGS\",jac=grad4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab159",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    "\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924427af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "For the function  $f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2$ \n",
    "\n",
    "   1. Plot the function for $x_1 \\in [-2,2]$ and $x_2 \\in [-2,2]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f85bffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun5(x):\n",
    "    return (1-x[0])**2 + 100 * (x[1]-x[0]**2)**2\n",
    "\n",
    "\n",
    "# PLOT SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "726bda64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99888297 0.99776271]\n",
      "[0.99888297 0.99776271]\n",
      "[1.00111903 1.00224379]\n",
      "[0.99888297 0.99776271]\n",
      "[1. 1.]\n",
      "[1. 1.]\n",
      "[1. 1.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def grad5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return np.array([2*x[0] - 2 - 400 * x[0] * (x[1] - x[0]**2), 200 * (x[1] -  x[0]**2)])\n",
    "    ################\n",
    "\n",
    "\n",
    "def hess5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    pass #todo\n",
    "  \n",
    "    ### SOLUTION ###\n",
    "    return np.array([[2 * (600*x[0]**2 - 200*x[1] + 1),-400*x[0]],[-400*x[0],200]])\n",
    "    ################\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper5 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.001},\n",
    "          \"newton\"          :{'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# it will not be graded\n",
    "\n",
    "\n",
    "######### TESTS #################\n",
    "hyper5['gradient_descent']['alpha0'] = 0.00001\n",
    "\n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    print(gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], epsilon=hyper5['gradient_descent']['epsilon']))\n",
    "for x0 in inits:\n",
    "    print(newton_mv(x0,grad5,hess5,epsilon=hyper5['newton']['epsilon']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b89f14",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c747d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "In this exercise we implement parameter estimation for the multivariate linear regression model on the `Iris` dataset. The first step if to preprocess the raw dataset in order to get a design matrix  $\\mathbf{X}\\in\\mathbb{R}^{n\\times k}$ with $n$ lines whose $k$ columns are predictors and a $\\mathbf{y}$ vector\n",
    "with reference values to be predicted. In matrix notation the sum of squares loss can be reformulated as:\n",
    "\n",
    "$$\n",
    "ssq_{\\cal D}(\\mathbf{p}) = \\sum_{i=1}^n(\\mathbf{X}\\mathbf{p} - \\mathbf{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "    \n",
    "   1. Compute the gradient. To do so you may compute it analytically or take advantage of sympy. \n",
    "      The result can be expressed in matrix form as $\\nabla ssq_{\\cal D}(\\mathbf{p}) = \\mathbf{X}^\\top (\\mathbf{X}\\mathbf{p} - \\mathbf{y})$ \n",
    "   2. Compute the hessian.  To do so you may compute it analytically or take advantage of sympy.\n",
    "      The result can be expressed in matrix form as $\\mathbf{H}_{ssq_{\\cal D}} (\\mathbf{p}) = \\mathbf{X}^\\top \\mathbf{X}$\n",
    "   3. Optimize the function with gradient descent\n",
    "   4. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ee1611a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pa\n",
    "\n",
    "\n",
    "#PREPARE DATA SECTION\n",
    "#grabs the full iris dataset from the sklearn library\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print('variable names',iris.feature_names)\n",
    "iris = pa.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "def do_data_matrix(iris_frame,x_colnames,y_colname,add_bias=True):\n",
    "    \"\"\"\n",
    "    Creates a numpy matrix encoding the dataset.\n",
    "    It converts a pandas dataframe to a couple (X,y) of numpy arrays \n",
    "    Args:\n",
    "        iris_frame (pandas.DataFrame): the iris dataframe\n",
    "        x_colnames (list) : list of strings, the predictor names\n",
    "        y_colname   (str) : the name of the predicted variable\n",
    "        add_bias    (bool): whether we add a bias to the model or not   \n",
    "    Returns:\n",
    "        (numpy.array,numpy.array). Tuple (X,y) the first element is a design matrix. \n",
    "                                   A matrix whose columns are x predictor variables with one row for each data line\n",
    "                                   The second element is a vector with the y values of the predicted variable\n",
    "    \"\"\"\n",
    "    X             = iris_frame[x_colnames].to_numpy()\n",
    "    nlines, ncols = X.shape\n",
    "    if add_bias:\n",
    "        X = np.concatenate([X,np.ones((nlines,1))],axis=1)\n",
    "        \n",
    "    return X, iris_frame[y_colname].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3321de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "\n",
    "def leastsq_loss(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params         (numpy.array): the vector of variables (linear regression parameters)\n",
    "        X              (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref           (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        float. the sum of squares value for this dataset and the current value of the parameters\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    \n",
    "    yhat = X @ params\n",
    "    ssq  = (yhat - yref)**2\n",
    "    return np.sum(ssq)\n",
    "\n",
    "    ################\n",
    "\n",
    "\n",
    "def gradient_lstsq(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "        X     (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref  (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        numpy.array. a gradient vector \n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "    ### SOLUTION ###    \n",
    "    return X.T @ (X @ params - yref)\n",
    "    ################\n",
    "\n",
    "    \n",
    "def hessian_lstsq(params,X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "       X      (numpy.array): the design matrix (columns are predictor variables)\n",
    "    Returns:\n",
    "        numpy.array. a hessian matrix \n",
    "        \n",
    "    @note: for linear regression, the hessian is constant for a given dataset, \n",
    "    we keep the signature with params for homogeneity\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    return X.T @ X\n",
    "    ################\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the current value of the loss\n",
    "#  - the current value of the iterates\n",
    "#  - for gradient descent, the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "# ...    \n",
    "    \n",
    "    \n",
    "from numpy.linalg import inv,norm   \n",
    "\n",
    "\n",
    "def gradient_descent_lstsq(x0,loss_fnc,grad,alpha0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    #print('loss (sum of squares)',loss_fnc(x))\n",
    "    while norm(dx) > epsilon:\n",
    "        x     = x - alpha0 * dx\n",
    "        dx    = grad(x)\n",
    "        #print('loss (sum of squares)',loss_fnc(x))\n",
    "    return x\n",
    "    ################\n",
    "\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses these values when calling your optimization functions\n",
    "\n",
    "hyper_lstsq = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.01},\n",
    "               \"newton\"          :{'epsilon':0.00001}}\n",
    "    \n",
    "    \n",
    "def newton_lstsq(x0,loss_fnc,grad,hessian,epsilon):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array)  : the initial iterate\n",
    "        loss_fnc(functional): the loss function\n",
    "        grad  (functional)  : the gradient function\n",
    "        hessian(functional) : the hessian function\n",
    "        epsilon             : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    ### SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    #print('loss (sum of squares)',loss_fnc(x))\n",
    "    while norm(dx) > epsilon:\n",
    "        alpha = inv(hessian(x))\n",
    "        x     = x - alpha @ dx\n",
    "        dx    = grad(x)\n",
    "        #print('loss (sum of squares)',loss_fnc(x))\n",
    "    return x\n",
    "    ################\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_lm(dataset,predictor_lst,predicted,optim_method,alpha0=0.1,epsilon=0.001,add_bias=True):\n",
    "    \"\"\"\n",
    "    Given a data matrix, returns the estimates of the parameters using least squares estimation.\n",
    "    Optimisation is performed either with the Newton method or the gradient method.\n",
    "    \n",
    "    Args: \n",
    "        dataset (pandas.DataFrame) : a dataset\n",
    "        predictor_lst        (list): a list of strings with the predictor variable names\n",
    "        predicted             (str): the name of the predicted variable\n",
    "        optim_method          (str): either 'grad_descent' or 'newton'\n",
    "        epsilon             (float): epsilon value for the optimizer\n",
    "    Returns :\n",
    "        numpy.array: a vector with parameter estimates\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    " \n",
    "    \n",
    "    ### SOLUTION ###\n",
    "    X, y = do_data_matrix(dataset,predictor_lst,predicted,add_bias=add_bias)\n",
    "    nlines, nvars = X.shape\n",
    "    if optim_method == 'newton':\n",
    "        x0 = np.random.randn(nvars)\n",
    "        return newton_lstsq(x0,lambda x: leastsq_loss(x,X,y),\n",
    "                               lambda x: gradient_lstsq(x,X,y),\n",
    "                               lambda x: hessian_lstsq(x,X),\n",
    "                               epsilon=epsilon)\n",
    "    elif optim_method == 'grad_descent':\n",
    "        x0 = np.random.randn(nvars)\n",
    "        return gradient_descent_lstsq(x0,\n",
    "                                      lambda x: leastsq_loss(x,X,y),\n",
    "                                      lambda x: gradient_lstsq(x,X,y),\n",
    "                                      alpha0 = 0.0001,\n",
    "                                      epsilon=epsilon)\n",
    "    ################    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "447dcfd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates [ 0.22282854 -0.20726607  0.52408311 -0.24030739]\n"
     ]
    }
   ],
   "source": [
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',optim_method='newton')\n",
    "print('estimates',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f1f231d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimates [ 0.22218844 -0.20870714  0.52452619 -0.2315293 ]\n"
     ]
    }
   ],
   "source": [
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',optim_method='grad_descent',epsilon=0.01)\n",
    "print('estimates',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bdcabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
