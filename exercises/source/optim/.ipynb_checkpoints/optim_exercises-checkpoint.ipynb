{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d42461c",
   "metadata": {},
   "source": [
    "# Optimization exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782d3d",
   "metadata": {},
   "source": [
    "As should be clear there is no silver bullet for optimizing any function. Although convex functions are the theoretically easy case, \n",
    "non convex optimization is becoming the classical use case since the advent of deep learning. \n",
    "In practice it is important to monitor the optimization process and to be able to understand when the optimization works well, struggles or entirely fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd19e35-dd6d-4585-9153-14729d45db7d",
   "metadata": {},
   "source": [
    "**Important Note** some of these exercises are automatically graded. You have to pay attention to:\n",
    "\n",
    "- Never modify existing function signatures. You may add other functions and testing code if you like. It does not impact grading. \n",
    "- Pass the assertion tests found in the notebook. These tests and others you cannot see are used to grade part of your homework\n",
    "- Pay attention to provide reasonably efficient solutions. Any notebook cell that takes more than 30 seconds at execution is considered as failed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d0e87",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 \n",
    "\n",
    "For the function $f(x) = (3x-2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ee495e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79f91cfe4432eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def fun1(x):\n",
    "    return (3*x-2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun1(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f813e3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d8d0acd6d5c9fd34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION \n",
    "    return 18*x - 12\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def fsec1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return 18\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent(x0,fprime,alpha0=1.0,decay=0.,epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : scheduler decay \n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \"\"\"    \n",
    "    ### BEGIN SOLUTION\n",
    "    alpha = alpha0\n",
    "    xprev = x0\n",
    "    dx    = fprime(x0)\n",
    "    x     = x0 - alpha * dx\n",
    "    t     = 1\n",
    "    while abs(x-xprev) > epsilon:\n",
    "        xprev = x\n",
    "        alpha = alpha/(1+decay*t)\n",
    "        x     = x - alpha * dx\n",
    "        dx    = fprime(x)\n",
    "        t    += 1\n",
    "    return x\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton(x0,fprime,fsec,beta0=1.0,decay=0.0,epsilon=0.000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        fsec(functional)   : the second derivative function\n",
    "        beta0 (float)      : the initial heuristic learning rate to be used by the scheduler\n",
    "        decay(float)       : the scheduler decay (like in gradient descent)\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    xprev     = x0\n",
    "    dx        = fprime(x0)\n",
    "    x         = x0 - (1/fsec(x0)) * dx\n",
    "    t         = 0\n",
    "    while abs(x-xprev) > epsilon:\n",
    "        xprev = x\n",
    "        beta  = beta0 * np.exp(-decay*t)\n",
    "        alpha = 1/fsec(x)\n",
    "        x     = x - beta*alpha * dx\n",
    "        dx    = fprime(x)\n",
    "        t    += 1.\n",
    "    return x\n",
    "    ### END SOLUTION\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited \n",
    "#   for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.00,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta':1.0,'decay':0.00,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "    \n",
    "# ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION/FIND HYPERPARAMETERS\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf15609b-5a5d-4c62-811c-45191d821949",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7881f5b6bc2d69e6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 0.1,'epsilon':0.001,'decay':0.001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0.0,'epsilon':0.00001}}\n",
    "### END HIDDEN TESTS\n",
    "### TEST GRADIENT DESCENT  (2pts)\n",
    "import math\n",
    "\n",
    "assert fprime1(1) == 6 \n",
    "\n",
    "assert  math.isclose(2/3, gradient_descent(10,fprime1,\n",
    "                          alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                          decay=hyper1['gradient_descent']['decay'],\n",
    "                          epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.01) \n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for x0 in range(-4,5,1):\n",
    "    assert math.isclose(2/3,gradient_descent(x0,fprime1,\n",
    "                                                 alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                                                 decay=hyper1['gradient_descent']['decay'],\n",
    "                                                 epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.01) \n",
    "### END HIDDEN TESTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db5258f3-f532-414e-8656-a0161082269b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-376fb8c591747113",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST NEWTON (2pts)\n",
    "\n",
    "assert fsec1(-1)  == 18\n",
    "\n",
    "assert  math.isclose(2/3, newton(-10,fprime1,fsec1,\n",
    "                                      beta0=hyper1['newton']['beta0'],\n",
    "                                      decay=hyper1['newton']['decay'],\n",
    "                                      epsilon=hyper1['newton']['epsilon']),abs_tol=0.0001) \n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for x0 in range(-4,5,1):\n",
    "    assert math.isclose(2/3,newton(x0,fprime1,\n",
    "                                       fsec1,\n",
    "                                       beta0=hyper1['newton']['beta0'],\n",
    "                                       decay=hyper1['newton']['decay'],\n",
    "                                       epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.0001) \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41be6de",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1df2c-922a-4550-852a-f55db16a9c6e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cf0a6365f55d71c3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a71a1ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2\n",
    "\n",
    "For the function $f(x) = x (x-2) (x+2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f8fba10",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-52bab07b7893d341",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun2(x):\n",
    "    return x * (x-2) * (x+2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun2(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93f47138",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1c1b1c3c6e926549",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return x*(x - 2)*(2*x + 4) + x*(x + 2)**2 + (x - 2)*(x + 2)**2\n",
    "    ### END SOLUTION\n",
    "\n",
    "def fsec2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return 2*(x*(x - 2) + 2*x*(x + 2) + 2*(x - 2)*(x + 2) + (x + 2)**2)\n",
    "    ### END SOLUTION\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8848e296-620e-42ec-96a4-c5291dcb24fc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-430469fb0cff2901",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 0.0001,'epsilon':0.00001},\n",
    "          \"newton\"          :{'epsilon':0.00001}}\n",
    "### END HIDDEN TESTS\n",
    "import math\n",
    "\n",
    "# GRADIENT DESCENT TESTS (2pts)\n",
    "\n",
    "assert fprime2(1) == -6 \n",
    "\n",
    "solset = [-2, 1.28077641,-0.7807764064043429]\n",
    "\n",
    "sol = gradient_descent(10,fprime2,\n",
    "                          alpha0=hyper2['gradient_descent']['alpha0'],\n",
    "                          epsilon=hyper2['gradient_descent']['epsilon'])\n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "xinit  = [-3,-1,1,2]\n",
    "\n",
    "for x0 in xinit:\n",
    "    sol = gradient_descent(x0,fprime2,alpha0=hyper2['gradient_descent']['alpha0'],\n",
    "                                      epsilon=hyper2['gradient_descent']['epsilon'])\n",
    "    assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "### END HIDDEN TESTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd66156-adab-4d79-8c38-ed171e688fae",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9c180c93fbf5c0ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NEWTON TESTS (2pts)\n",
    "\n",
    "assert fsec2(-1)  == -8\n",
    "\n",
    "sol = newton(-10,fprime2,fsec2,epsilon=hyper2['newton']['epsilon']) \n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for x0 in xinit:\n",
    "    sol = newton(x0,fprime2,fsec2, epsilon=hyper2['gradient_descent']['epsilon']) \n",
    "    assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ed057",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bec1e-f510-4461-a3e7-add37d2315fb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f41e03369aa87879",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47fb3247",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3\n",
    "\n",
    "For the function $f(x) = -e^{-(3x-1)^2} + \\frac{x^2}{100}$. \n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9bcdf1b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a439514a8324088",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "def fun3(x):\n",
    "    return - exp(-(3*x-1)**2)+ (x**2) / 100 \n",
    "\n",
    "\n",
    "#PLOT SECTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(-3,3,0.01)\n",
    "y = fun3(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3547dc8a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3d46db14698e4ffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy import exp\n",
    "\n",
    "def fprime3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"    \n",
    "    ### BEGIN SOLUTION ###\n",
    "    return x/50 - (6 - 18*x)*exp(-(3*x - 1)**2)\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def fsec3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return -36*(3*x - 1)**2 * exp(-(3*x - 1)**2) + 1/50 + 18 * exp(-(3*x - 1)**2)\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper3 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "\n",
    "# it will not be graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "362cc8b1-378e-48d8-96b3-99f70364a0b1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-14215a159d01e168",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "hyper3['gradient_descent']['alpha0'] = 0.001\n",
    "hyper3['newton']['beta'] = 1.0\n",
    "hyper3['newton']['decay'] = 0.01\n",
    "hyper3['newton']['epsilon'] = 0.0001\n",
    "### END HIDDEN TESTS\n",
    "import math\n",
    "\n",
    "#### TEST GRADIENT DESCENT #### (2pts)\n",
    "\n",
    "assert fprime3(3) == 0.06\n",
    "\n",
    "assert math.isclose(1/3,gradient_descent(6,fprime3,\n",
    "                                           alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                           decay=hyper3['gradient_descent']['decay'],\n",
    "                                           epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "xinit = [-7,-5,-3,-1,1,3,5,7]\n",
    "\n",
    "for x0 in xinit:\n",
    "    assert math.isclose(1/3,gradient_descent(x0,fprime3,\n",
    "                                                 alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                                 decay=hyper3['gradient_descent']['decay'],\n",
    "                                                 epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "### END HIDDEN TESTS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "006c5b42-7a5d-4f78-a383-785b3ff83f91",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07455e938a15f8b9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### TEST NEWTON #### (2pts)\n",
    "\n",
    "assert fsec3(-2) == 0.02\n",
    "\n",
    "assert math.isclose(1/3, newton(-4,fprime3,fsec3,\n",
    "                                beta0 = hyper3['newton']['beta0'],\n",
    "                                decay = hyper3['newton']['decay'],\n",
    "                                epsilon=hyper3['newton']['epsilon']),abs_tol=0.001)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "for x0 in xinit:\n",
    "    assert math.isclose(1/3,newton(x0,fprime3,fsec3,\n",
    "                                   beta0 = hyper3['newton']['beta0'],\n",
    "                                   decay = hyper3['newton']['decay'],\n",
    "                                   epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaaf0f",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb49d-bffc-4418-9ef8-93f6ad9ec040",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-271653af45566715",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27bd69bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "For the function $f(x_1,x_2) = \\sum_{i=1}^2 x_i^4 - 16x_i^2 + 5 x_i$. \n",
    "   1. Plot the function for $x_1 \\in [-5,5]$ and $x_2 \\in [-5,5]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d21f406c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbd5f5bd932369ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun4(x):\n",
    "    return np.sum( x[i]**4 - 16*x[i]**2 + 5*x[i]  for i in [0,1] )\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "pass\n",
    "#todo\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e3e12b6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b989ef95391d75eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy.linalg import inv,norm\n",
    "\n",
    "def grad4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"    \n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.array([4*x[0]**3 - 32*x[0] + 5,4*x[1]**3 - 32*x[1] + 5])\n",
    "    ### END SOLUTION ############\n",
    "\n",
    "def hess4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.array([[4*(3*x[0]**2 - 8),0],[0,4*(3*x[1]**2 - 8)]])\n",
    "    ### END SOLUTION #############\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent_mv(x0,grad,alpha0=1.0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    t  = 0.\n",
    "    alpha = alpha0\n",
    "    while norm(dx) > epsilon:\n",
    "        alpha = alpha/(1+decay*t)\n",
    "        x     = x - alpha * dx \n",
    "        dx    = grad(x)\n",
    "        t     = t+1\n",
    "    return x\n",
    "    ### END SOLUTION \n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton_mv(x0,grad,hessian,beta0=1.,decay=0.0,epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        hessian(functional): the hessian function\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    t  = 0\n",
    "    while norm(dx) > epsilon:\n",
    "        beta  = beta0*np.exp(-decay*t)\n",
    "        alpha = inv(hessian(x))\n",
    "        x     = x - beta*alpha @ dx \n",
    "        dx    = grad(x)\n",
    "        t    += 1\n",
    "    return x\n",
    "    ### END SOLUTION ##\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper4 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# e.g test with scipy.optimize.minimize\n",
    "# it will not be graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c44942-199c-4092-9516-12ae01f899c3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0bf436b40ac9c469",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper4['gradient_descent']['alpha0'] = 0.002\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "### GRADIENT DESCENT TEST SECTION (2pts)\n",
    "\n",
    "solution_set = np.array([ [-2.90353404,  2.74680293], [2.74680289, 2.74680289], [ 2.74680277, -2.90353403],[-2.90353403 , 2.74680277],[-2.90353403 , -2.90353403]])\n",
    "\n",
    "x0 = np.array([-10,10])\n",
    "sol = gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n",
    "### BEGIN HIDDEN TESTS \n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    sol = gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon'])\n",
    "    assert any(np.isclose(s,sol,atol=0.00001).all() for s in solution_set)\n",
    "### END HIDDEN TESTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ffbc7c3-1387-4de4-98d7-ce76ca5542ef",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96b2ca54d8d5bfca",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### NEWTON TEST SECTION (2pts)\n",
    "\n",
    "sol = newton_mv(x0,grad4,hess4, beta0 = hyper4['newton']['beta0'], decay = hyper4['newton']['decay'],epsilon=hyper4['newton']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n",
    "\n",
    "### BEGIN HIDDEN TESTS \n",
    "\n",
    "for x0 in inits:\n",
    "    sol = newton_mv(x0,grad4,hess4, beta0 = hyper4['newton']['beta0'], decay = hyper4['newton']['decay'],epsilon=hyper4['newton']['epsilon'])\n",
    "    assert any(np.isclose(s,sol,atol=0.00001).all() for s in solution_set)\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab159",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? \n",
    " - What are the key properties of this function ? is it convex ? how many solutions can you identify ?\n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, beta epsilon ?\n",
    "\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e75a4c-92fe-4d84-9c0d-83fea3108137",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3007bdb2c8d1eabb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "924427af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "For the function  $f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2$ \n",
    "\n",
    "   1. Plot the function for $x_1 \\in [-2,2]$ and $x_2 \\in [-2,2]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f85bffa7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2884df9df649664e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun5(x):\n",
    "    return (1-x[0])**2 + 100 * (x[1]-x[0]**2)**2\n",
    "\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "pass\n",
    "### END SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "726bda64",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c61847ed7bff74a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def grad5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.array([2*x[0] - 2 - 400 * x[0] * (x[1] - x[0]**2), 200 * (x[1] -  x[0]**2)])\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "def hess5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.array([[2 * (600*x[0]**2 - 200*x[1] + 1),-400*x[0]],[-400*x[0],200]])\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper5 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# it will not be graded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c27fdb57-1452-40ab-a481-29f85f7e84de",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-67b3ad764a181171",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "hyper5['gradient_descent']['alpha0'] = 0.0001\n",
    "hyper5['gradient_descent']['decay'] = 0.0\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "\n",
    "######### GRADIENT DESCENT TEST (2pts)\n",
    "\n",
    "x0  = np.array([3,-3])\n",
    "rsol = np.array([1,1])\n",
    "\n",
    "sol = gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], decay= hyper5['gradient_descent']['decay'], epsilon=hyper5['gradient_descent']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.0001).all()\n",
    "\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "inits = np.array([ [-2,2], [-2,-2],[2,2],[2,-2] ])*3\n",
    "for x0 in inits:\n",
    "    sol = gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], decay= hyper5['gradient_descent']['decay'], epsilon=hyper5['gradient_descent']['epsilon'])\n",
    "    assert np.isclose(rsol,sol,atol=0.0001).all()\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086df9d-c1cc-4333-b07c-ef3daeaf4347",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8805ac027f830bf8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "######### NEWTON TEST (2pts)\n",
    "\n",
    "sol = newton_mv(x0,grad5,hess5,beta0 = hyper5['newton']['beta0'], decay=hyper5['newton']['decay'], epsilon=hyper5['newton']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.000001).all()\n",
    "\n",
    "### BEGIN HIDDEN TESTS    \n",
    "for x0 in inits:\n",
    "    sol = newton_mv(x0,grad5,hess5,beta0 = hyper5['newton']['beta0'], decay=hyper5['newton']['decay'], epsilon=hyper5['newton']['epsilon'])\n",
    "    assert np.isclose(rsol,sol,atol=0.000001).all()\n",
    "### END HIDDEN TESTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b89f14",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a902d11-60c2-42de-a84b-a35042214215",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7c9a207e488bfc1f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c747d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "In this exercise we implement parameter estimation for the multivariate linear regression model on the `Iris` dataset. The first step if to preprocess the raw dataset in order to get a design matrix  $\\mathbf{X}\\in\\mathbb{R}^{n\\times k}$ with $n$ lines whose $k$ columns are predictors and a $\\mathbf{y}$ vector\n",
    "with reference values to be predicted. In matrix notation the sum of squares loss can be reformulated as:\n",
    "\n",
    "$$\n",
    "ssq_{\\cal D}(\\mathbf{p}) = \\sum_{i=1}^n(\\mathbf{X}\\mathbf{p} - \\mathbf{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "    \n",
    "   1. Compute the gradient. To do so you may compute it analytically or take advantage of sympy. \n",
    "      The result can be expressed in matrix form as $\\nabla ssq_{\\cal D}(\\mathbf{p}) = \\mathbf{X}^\\top (\\mathbf{X}\\mathbf{p} - \\mathbf{y})$ \n",
    "   2. Compute the hessian.  To do so you may compute it analytically or take advantage of sympy.\n",
    "      The result can be expressed in matrix form as $\\mathbf{H}_{ssq_{\\cal D}} (\\mathbf{p}) = \\mathbf{X}^\\top \\mathbf{X}$\n",
    "   3. Optimize the function with gradient descent\n",
    "   4. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee1611a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris variable names ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pa\n",
    "\n",
    "\n",
    "#PREPARE DATA SECTION\n",
    "#grabs the full iris dataset from the sklearn library \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print('Iris variable names',iris.feature_names)\n",
    "iris = pa.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "def do_data_matrix(iris_frame,x_colnames,y_colname,add_bias=True):\n",
    "    \"\"\"\n",
    "    Creates a numpy matrix encoding the dataset.\n",
    "    It converts a pandas dataframe to a couple (X,y) of numpy arrays \n",
    "    Args:\n",
    "        iris_frame (pandas.DataFrame): the iris dataframe\n",
    "        x_colnames (list) : list of strings, the predictor names\n",
    "        y_colname   (str) : the name of the predicted variable\n",
    "        add_bias    (bool): whether we add a bias to the model or not   \n",
    "    Returns:\n",
    "        (numpy.array,numpy.array). Tuple (X,y) the first element is a design matrix. \n",
    "                                   A matrix whose columns are x predictor variables with one row for each data line\n",
    "                                   The second element is a vector with the y values of the predicted variable\n",
    "    \"\"\"\n",
    "    X             = iris_frame[x_colnames].to_numpy()\n",
    "    nlines, ncols = X.shape\n",
    "    if add_bias:\n",
    "        X = np.concatenate([X,np.ones((nlines,1))],axis=1)\n",
    "        \n",
    "    return X, iris_frame[y_colname].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3321de48",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-23957233a607f203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "\n",
    "def leastsq_loss(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params         (numpy.array): the vector of variables (linear regression parameters)\n",
    "        X              (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref           (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        float. the sum of squares value for this dataset and the current value of the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "    yhat = X @ params\n",
    "    ssq  = (yhat - yref)**2\n",
    "    return np.sum(ssq)\n",
    "\n",
    "    ### END SOLUTION ####\n",
    "\n",
    "\n",
    "def gradient_lstsq(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "        X     (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref  (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        numpy.array. a gradient vector \n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION ###    \n",
    "    return X.T @ (X @ params - yref)\n",
    "    ### END SOLUTION\n",
    "\n",
    "    \n",
    "def hessian_lstsq(params,X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "       X      (numpy.array): the design matrix (columns are predictor variables)\n",
    "    Returns:\n",
    "        numpy.array. a hessian matrix \n",
    "        \n",
    "    @note: for linear regression, the hessian is constant for a given dataset, \n",
    "    we keep the signature with params for homogeneity\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return X.T @ X\n",
    "    ### END SOLUTION\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the current value of the loss\n",
    "#  - the current value of the iterates\n",
    "#  - for gradient descent, the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "# ...    \n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "    \n",
    "from numpy.linalg import inv,norm   \n",
    "\n",
    "\n",
    "def gradient_descent_lstsq(x0,loss_fnc,grad,alpha0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : a decay parameter for the scheduler\n",
    "        epsilon (float)    : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    alpha = alpha0\n",
    "    t=0\n",
    "    while norm(dx) > epsilon:\n",
    "        alpha = alpha/(1+decay*t)\n",
    "        x     = x - alpha * dx\n",
    "        dx    = grad(x)\n",
    "        t    +=1\n",
    "    return x\n",
    "    ### END SOLUTION\n",
    "\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses these values when calling your optimization functions\n",
    "\n",
    "hyper_lstsq = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.01},\n",
    "               \"newton\"          :{'epsilon':0.00001}}\n",
    "    \n",
    "    \n",
    "def newton_lstsq(x0,loss_fnc,grad,hessian,beta0=1.,decay=0.,epsilon=0.0000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array)  : the initial iterate\n",
    "        loss_fnc(functional): the loss function\n",
    "        grad  (functional)  : the gradient function\n",
    "        hessian(functional) : the hessian function\n",
    "        epsilon             : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    x  = x0\n",
    "    dx = grad(x)\n",
    "    #print('loss (sum of squares)',loss_fnc(x))\n",
    "    t  = 0\n",
    "    while norm(dx) > epsilon:\n",
    "        alpha = inv(hessian(x))\n",
    "        beta  = beta0 * np.exp(-decay*t)\n",
    "        x     = x - beta * alpha @ dx\n",
    "        dx    = grad(x)\n",
    "        t    += 1.\n",
    "        #print('loss (sum of squares)',loss_fnc(x))\n",
    "    return x\n",
    "    ### END SOLUTION\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_lm(dataset,predictor_lst,predicted,optim_method,alpha0=0.1,beta0=0.1,decay=0,epsilon=0.001,add_bias=True):\n",
    "    \"\"\"\n",
    "    Given a data matrix, returns the estimates of the parameters using least squares estimation.\n",
    "    Optimisation is performed either with the Newton method or the gradient method.\n",
    "    \n",
    "    Args: \n",
    "        dataset (pandas.DataFrame) : a dataset\n",
    "        predictor_lst        (list): a list of strings with the predictor variable names\n",
    "        predicted             (str): the name of the predicted variable\n",
    "        optim_method          (str): either 'grad_descent' or 'newton'\n",
    "        epsilon             (float): epsilon value for the optimizer\n",
    "    Returns :\n",
    "        numpy.array: a vector with parameter estimates\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    X, y = do_data_matrix(dataset,predictor_lst,predicted,add_bias=add_bias)\n",
    "    nlines, nvars = X.shape\n",
    "    if optim_method == 'newton':\n",
    "        x0 = np.random.randn(nvars)\n",
    "        return newton_lstsq(x0,lambda x: leastsq_loss(x,X,y),\n",
    "                               lambda x: gradient_lstsq(x,X,y),\n",
    "                               lambda x: hessian_lstsq(x,X),\n",
    "                               epsilon=epsilon)\n",
    "    elif optim_method == 'grad_descent':\n",
    "        x0 = np.random.randn(nvars)\n",
    "        return gradient_descent_lstsq(x0,\n",
    "                                      lambda x: leastsq_loss(x,X,y),\n",
    "                                      lambda x: gradient_lstsq(x,X,y),\n",
    "                                      alpha0 = 0.0001,\n",
    "                                      epsilon=epsilon)\n",
    "    ### END SOLUTION   \n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "lr_params = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "             \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447dcfd9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eb6a6ecaf1596345",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TEST LINEAR REGRESSION (5pts)\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "np.random.seed(12345)\n",
    "### END HIDDEN TESTS\n",
    "\n",
    "sols = np.array([  0.22282854, -0.20726607,  0.52408311, -0.24030739 ])\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                alpha0  =  lr_params['gradient_descent']['alpha0'],\n",
    "                decay   =  lr_params['gradient_descent']['decay'], \n",
    "                epsilon =  lr_params['gradient_descent']['epsilon'], \n",
    "                optim_method = 'grad_descent')\n",
    "assert np.isclose(result,sols,atol=0.0001).all()\n",
    "\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                beta0  =  lr_params['newton']['beta0'],\n",
    "                decay   =  lr_params['newton']['decay'], \n",
    "                epsilon =  lr_params['newton']['epsilon'], \n",
    "                optim_method = 'newton')\n",
    "assert np.isclose(result,sols,atol=0.000001).all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
