{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c2adf4",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565779e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24258e4b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42461c",
   "metadata": {},
   "source": [
    "# Optimization exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782d3d",
   "metadata": {},
   "source": [
    "As should be clear there is no silver bullet for optimizing any function. Although convex functions are the theoretically easy case, \n",
    "non convex optimization is becoming the classical use case since the advent of deep learning. \n",
    "In practice it is important to monitor the optimization process and to be able to understand when the optimization works well, struggles or entirely fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd19e35-dd6d-4585-9153-14729d45db7d",
   "metadata": {},
   "source": [
    "**Important Note** some of these exercises are automatically graded. You have to pay attention to:\n",
    "\n",
    "- Never modify existing function signatures. You may add other functions and testing code if you like. It does not impact grading. \n",
    "- Pass the assertion tests found in the notebook. These tests and others you cannot see are used to grade part of your homework\n",
    "- Pay attention to provide reasonably efficient solutions. Any notebook cell that takes more than 30 seconds at execution is considered as failed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d0e87",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1 \n",
    "\n",
    "For the function $f(x) = (3x-2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee495e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20bee017f34500e41e595edfe87807ee",
     "grade": false,
     "grade_id": "cell-d79f91cfe4432eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def fun1(x):\n",
    "    return (3*x-2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f813e3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01918c356e78cd7dacd0a82f04612bbc",
     "grade": false,
     "grade_id": "cell-d8d0acd6d5c9fd34",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def fsec1(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent(x0,fprime,alpha0=1.0,decay=0.,epsilon=0.0001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : scheduler decay \n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton(x0,fprime,fsec,beta0=1.0,decay=0.0,epsilon=0.000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0        (float)  : the initial iterate\n",
    "        fprime(functional) : the first derivative function\n",
    "        fsec(functional)   : the second derivative function\n",
    "        beta0 (float)      : the initial heuristic learning rate to be used by the scheduler\n",
    "        decay(float)       : the scheduler decay (like in gradient descent)\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        float. the value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited \n",
    "#   for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "hyper1 = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta':1.0,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "    \n",
    "# ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION/FIND HYPERPARAMETERS\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15609b-5a5d-4c62-811c-45191d821949",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a0b97fdba27d32dd7aa979942a18ceb",
     "grade": true,
     "grade_id": "cell-7881f5b6bc2d69e6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST GRADIENT DESCENT  (2pts)\n",
    "import math\n",
    "\n",
    "assert fprime1(1) == 6 \n",
    "\n",
    "assert  math.isclose(2/3, gradient_descent(10,fprime1,\n",
    "                          alpha0=hyper1['gradient_descent']['alpha0'],\n",
    "                          decay=hyper1['gradient_descent']['decay'],\n",
    "                          epsilon=hyper1['gradient_descent']['epsilon']),abs_tol=0.01) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5258f3-f532-414e-8656-a0161082269b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2385c852eb64996a08f24155593785c",
     "grade": true,
     "grade_id": "cell-376fb8c591747113",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST NEWTON (2pts)\n",
    "\n",
    "assert fsec1(-1)  == 18\n",
    "\n",
    "assert  math.isclose(2/3, newton(-10,fprime1,fsec1,\n",
    "                                      beta0=hyper1['newton']['beta0'],\n",
    "                                      decay=hyper1['newton']['decay'],\n",
    "                                      epsilon=hyper1['newton']['epsilon']),abs_tol=0.0001) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41be6de",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1df2c-922a-4550-852a-f55db16a9c6e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a8fbd6216bd89fd45158fabefdbfc76",
     "grade": true,
     "grade_id": "cell-cf0a6365f55d71c3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a1ea5",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2\n",
    "\n",
    "For the function $f(x) = x (x-2) (x+2)^2$\n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8fba10",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45a306d24fa3f803881189cf0003a9b1",
     "grade": false,
     "grade_id": "cell-52bab07b7893d341",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun2(x):\n",
    "    return x * (x-2) * (x+2)**2\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f47138",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9a5ee9b7431b17290802e05a95c199",
     "grade": false,
     "grade_id": "cell-1c1b1c3c6e926549",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def fprime2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def fsec2(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper2 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "# test against scipy.optimize.minimize\n",
    "# it will not be graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848e296-620e-42ec-96a4-c5291dcb24fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00ad97b41449a04f828b796fd49d203a",
     "grade": true,
     "grade_id": "cell-430469fb0cff2901",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# GRADIENT DESCENT TESTS (2pts)\n",
    "\n",
    "assert fprime2(1) == -6 \n",
    "\n",
    "solset = [-2, 1.28077641,-0.7807764064043429]\n",
    "\n",
    "sol = gradient_descent(10,fprime2,\n",
    "                          alpha0=hyper2['gradient_descent']['alpha0'],\n",
    "                          epsilon=hyper2['gradient_descent']['epsilon'])\n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd66156-adab-4d79-8c38-ed171e688fae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85b04596948bd187044b882db40d2fbe",
     "grade": true,
     "grade_id": "cell-9c180c93fbf5c0ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# NEWTON TESTS (2pts)\n",
    "\n",
    "assert fsec2(-1)  == -8\n",
    "\n",
    "sol = newton(-10,fprime2,fsec2,epsilon=hyper2['newton']['epsilon']) \n",
    "assert any(math.isclose(s,sol,abs_tol=0.01) for s in solset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ed057",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    "\n",
    " - What are the key properties of this function ? is it convex ? How many solutions can you find ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - How did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27bec1e-f510-4461-a3e7-add37d2315fb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a69839a3c1b1d588613aa5876c8a12d",
     "grade": true,
     "grade_id": "cell-f41e03369aa87879",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb3247",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3\n",
    "\n",
    "For the function $f(x) = -e^{-(3x-1)^2} + \\frac{x^2}{100}$. \n",
    "   1. Plot the function within the interval $[-3,3]$\n",
    "   2. Compute the first order derivative\n",
    "   3. Compute the second order derivative\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcdf1b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7fde5e26b7e279781db79e4d1560bbf",
     "grade": false,
     "grade_id": "cell-2a439514a8324088",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "def fun3(x):\n",
    "    return - exp(-(3*x-1)**2)+ (x**2) / 100 \n",
    "\n",
    "\n",
    "#PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547dc8a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f4851d753c5970ce47f3ccbbcf085be",
     "grade": false,
     "grade_id": "cell-3d46db14698e4ffb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy import exp\n",
    "\n",
    "def fprime3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the first order derivative at x\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def fsec3(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (float) : the x value\n",
    "    Returns:\n",
    "        float. the second order derivative at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper3 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# test against several initial conditions, several learning rates, several epsilon\n",
    "\n",
    "# it will not be graded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362cc8b1-378e-48d8-96b3-99f70364a0b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ef46f4c65d51c5eb80d41a8f76dfd16",
     "grade": true,
     "grade_id": "cell-14215a159d01e168",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "#### TEST GRADIENT DESCENT #### (2pts)\n",
    "\n",
    "assert fprime3(3) == 0.06\n",
    "\n",
    "assert math.isclose(1/3,gradient_descent(6,fprime3,\n",
    "                                           alpha0=hyper3['gradient_descent']['alpha0'],\n",
    "                                           decay=hyper3['gradient_descent']['decay'],\n",
    "                                           epsilon=hyper3['gradient_descent']['epsilon']),abs_tol=0.001) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c5b42-7a5d-4f78-a383-785b3ff83f91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "534617bad06619ba352c51e4b16d0438",
     "grade": true,
     "grade_id": "cell-07455e938a15f8b9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#### TEST NEWTON #### (2pts)\n",
    "\n",
    "assert fsec3(-2) == 0.02\n",
    "\n",
    "assert math.isclose(1/3, newton(-4,fprime3,fsec3,\n",
    "                                beta0 = hyper3['newton']['beta0'],\n",
    "                                decay = hyper3['newton']['decay'],\n",
    "                                epsilon=hyper3['newton']['epsilon']),abs_tol=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaaf0f",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the analytical solution ? how did you compute it ? with sympy ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb49d-bffc-4418-9ef8-93f6ad9ec040",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a80e10beb700b7a2f356537cab94cc8",
     "grade": true,
     "grade_id": "cell-271653af45566715",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd69bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "For the function $f(x_1,x_2) = \\sum_{i=1}^2 x_i^4 - 16x_i^2 + 5 x_i$. \n",
    "   1. Plot the function for $x_1 \\in [-5,5]$ and $x_2 \\in [-5,5]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f406c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efde0f5dd286f16292347928690b3ed4",
     "grade": false,
     "grade_id": "cell-bbd5f5bd932369ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun4(x):\n",
    "    return np.sum( x[i]**4 - 16*x[i]**2 + 5*x[i]  for i in [0,1] )\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e12b6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "705d04256b4328e5e84d1b893f48056c",
     "grade": false,
     "grade_id": "cell-b989ef95391d75eb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "from numpy.linalg import inv,norm\n",
    "\n",
    "def grad4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def hess4(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def gradient_descent_mv(x0,grad,alpha0=1.0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0             : the initial learning rate\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the successive values of the iterates.\n",
    "#  - the successive values of alpha or beta at each iteration in case alpha is non constant in your implementation.\n",
    "#  - the current distance from the theoretical solution you computed analytically and the current iterate.\n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "\n",
    "def newton_mv(x0,grad,hessian,beta0=1.,decay=0.0,epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        hessian(functional): the hessian function\n",
    "        epsilon            : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper4 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# e.g test with scipy.optimize.minimize\n",
    "# it will not be graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c44942-199c-4092-9516-12ae01f899c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca82c72bc77980aadb1cbf7f025dfcf3",
     "grade": true,
     "grade_id": "cell-0bf436b40ac9c469",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "### GRADIENT DESCENT TEST SECTION (2pts)\n",
    "\n",
    "solution_set = np.array([ [-2.90353404,  2.74680293], [2.74680289, 2.74680289], [ 2.74680277, -2.90353403],[-2.90353403 , 2.74680277],[-2.90353403 , -2.90353403]])\n",
    "\n",
    "x0 = np.array([-10,10])\n",
    "sol = gradient_descent_mv(x0,grad4,alpha0 = hyper4['gradient_descent']['alpha0'], epsilon=hyper4['gradient_descent']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffbc7c3-1387-4de4-98d7-ce76ca5542ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07539a1c0ffbae36ea011cf607c9cbdf",
     "grade": true,
     "grade_id": "cell-96b2ca54d8d5bfca",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### NEWTON TEST SECTION (2pts)\n",
    "\n",
    "sol = newton_mv(x0,grad4,hess4, beta0 = hyper4['newton']['beta0'], decay = hyper4['newton']['decay'],epsilon=hyper4['newton']['epsilon'])\n",
    "assert any(np.isclose(s,sol,atol=0.000001).all() for s in solution_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ab159",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? \n",
    " - What are the key properties of this function ? is it convex ? how many solutions can you identify ?\n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, beta epsilon ?\n",
    "\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e75a4c-92fe-4d84-9c0d-83fea3108137",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9e1489ff2d397949d7e37795fc37e96",
     "grade": true,
     "grade_id": "cell-3007bdb2c8d1eabb",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924427af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "For the function  $f(x_1,x_2) = (1-x_1)^2 + 100 (x_2-x_1^2)^2$ \n",
    "\n",
    "   1. Plot the function for $x_1 \\in [-2,2]$ and $x_2 \\in [-2,2]$\n",
    "   2. Compute the gradient\n",
    "   3. Compute the hessian\n",
    "   4. Optimize the function with gradient descent. \n",
    "   5. Optimize the function with the newton method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bffa7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f71f00c21b1f875db5543bd77879b2fc",
     "grade": false,
     "grade_id": "cell-2884df9df649664e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def fun5(x):\n",
    "    return (1-x[0])**2 + 100 * (x[1]-x[0]**2)**2\n",
    "\n",
    "\n",
    "# PLOT SECTION\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726bda64",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c67c26001d8aa8b715fb3ade3f5982e6",
     "grade": false,
     "grade_id": "cell-c61847ed7bff74a9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "def grad5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The gradient vector at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def hess5(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x (numpy.array) : the x value\n",
    "    Returns:\n",
    "        numpy.array. The hessian matrix at x\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "hyper5 = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "          \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n",
    "# FEEL FREE TO ADD ANY OTHER OPTIONAL CODE HERE TO TEST YOUR IMPLEMENTATION.\n",
    "# it will not be graded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fdb57-1452-40ab-a481-29f85f7e84de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4bf9484bfe3f7d462fe8ba56340c11bc",
     "grade": true,
     "grade_id": "cell-67b3ad764a181171",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "######### GRADIENT DESCENT TEST (2pts)\n",
    "\n",
    "x0  = np.array([3,-3])\n",
    "rsol = np.array([1,1])\n",
    "\n",
    "sol = gradient_descent_mv(x0,grad5,alpha0 = hyper5['gradient_descent']['alpha0'], decay= hyper5['gradient_descent']['decay'], epsilon=hyper5['gradient_descent']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.0001).all()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086df9d-c1cc-4333-b07c-ef3daeaf4347",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af7940f7db78e0d698f8924d5da6b4a1",
     "grade": true,
     "grade_id": "cell-8805ac027f830bf8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "######### NEWTON TEST (2pts)\n",
    "\n",
    "sol = newton_mv(x0,grad5,hess5,beta0 = hyper5['newton']['beta0'], decay=hyper5['newton']['decay'], epsilon=hyper5['newton']['epsilon'])\n",
    "assert np.isclose(rsol,sol,atol=0.000001).all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b89f14",
   "metadata": {},
   "source": [
    "**Free content and comments section**: \n",
    " - What is the exact solution ? how did you compute it ? with scipy.optimize.minimize ? \n",
    " - What are the key properties of this function ? is it convex ? \n",
    " - In case your solution is far from the exact solution can you explain why ? In case both algorithms return different solutions, explain why.\n",
    " - how did you tune alpha, epsilon ?\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a902d11-60c2-42de-a84b-a35042214215",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "747bb3229019773f51de9f2684c8d08b",
     "grade": true,
     "grade_id": "cell-7c9a207e488bfc1f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c747d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 6\n",
    "\n",
    "In this exercise we implement parameter estimation for the multivariate linear regression model on the `Iris` dataset. The first step if to preprocess the raw dataset in order to get a design matrix  $\\mathbf{X}\\in\\mathbb{R}^{n\\times k}$ with $n$ lines whose $k$ columns are predictors and a $\\mathbf{y}$ vector\n",
    "with reference values to be predicted. In matrix notation the sum of squares loss can be reformulated as:\n",
    "\n",
    "$$\n",
    "ssq_{\\cal D}(\\mathbf{p}) = \\sum_{i=1}^n(\\mathbf{X}\\mathbf{p} - \\mathbf{y})^2\n",
    "$$\n",
    "\n",
    "\n",
    "    \n",
    "   1. Compute the gradient. To do so you may compute it analytically or take advantage of sympy. \n",
    "      The result can be expressed in matrix form as $\\nabla ssq_{\\cal D}(\\mathbf{p}) = \\mathbf{X}^\\top (\\mathbf{X}\\mathbf{p} - \\mathbf{y})$ \n",
    "   2. Compute the hessian.  To do so you may compute it analytically or take advantage of sympy.\n",
    "      The result can be expressed in matrix form as $\\mathbf{H}_{ssq_{\\cal D}} (\\mathbf{p}) = \\mathbf{X}^\\top \\mathbf{X}$\n",
    "   3. Optimize the function with gradient descent\n",
    "   4. Optimize the function with the newton method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1611a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pa\n",
    "\n",
    "\n",
    "#PREPARE DATA SECTION\n",
    "#grabs the full iris dataset from the sklearn library \n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print('Iris variable names',iris.feature_names)\n",
    "iris = pa.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "def do_data_matrix(iris_frame,x_colnames,y_colname,add_bias=True):\n",
    "    \"\"\"\n",
    "    Creates a numpy matrix encoding the dataset.\n",
    "    It converts a pandas dataframe to a couple (X,y) of numpy arrays \n",
    "    Args:\n",
    "        iris_frame (pandas.DataFrame): the iris dataframe\n",
    "        x_colnames (list) : list of strings, the predictor names\n",
    "        y_colname   (str) : the name of the predicted variable\n",
    "        add_bias    (bool): whether we add a bias to the model or not   \n",
    "    Returns:\n",
    "        (numpy.array,numpy.array). Tuple (X,y) the first element is a design matrix. \n",
    "                                   A matrix whose columns are x predictor variables with one row for each data line\n",
    "                                   The second element is a vector with the y values of the predicted variable\n",
    "    \"\"\"\n",
    "    X             = iris_frame[x_colnames].to_numpy()\n",
    "    nlines, ncols = X.shape\n",
    "    if add_bias:\n",
    "        X = np.concatenate([X,np.ones((nlines,1))],axis=1)\n",
    "        \n",
    "    return X, iris_frame[y_colname].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321de48",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "150d3d85d33b9fdc002b874dcef44b3f",
     "grade": false,
     "grade_id": "cell-23957233a607f203",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "#  *** AUTOGRADING SECTION ***\n",
    "#\n",
    "#  !!! FUNCTION SIGNATURES MUST BE LEFT UNCHANGED !!!\n",
    "#\n",
    "#  Each of your functions must return a value (@see docstring)\n",
    "\n",
    "\n",
    "def leastsq_loss(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        params         (numpy.array): the vector of variables (linear regression parameters)\n",
    "        X              (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref           (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        float. the sum of squares value for this dataset and the current value of the parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def gradient_lstsq(params,X,yref):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "        X     (numpy.array): the design matrix (columns are predictor variables)\n",
    "        yref  (numpy.array): the vector of y values\n",
    "    Returns:\n",
    "        numpy.array. a gradient vector \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "def hessian_lstsq(params,X):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       params (numpy.array): a paremeter vector\n",
    "       X      (numpy.array): the design matrix (columns are predictor variables)\n",
    "    Returns:\n",
    "        numpy.array. a hessian matrix \n",
    "        \n",
    "    @note: for linear regression, the hessian is constant for a given dataset, \n",
    "    we keep the signature with params for homogeneity\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# It is strongly advised to output intermediate results during optimisation such as:\n",
    "#  - the current value of the loss\n",
    "#  - the current value of the iterates\n",
    "#  - for gradient descent, the successive values of alpha at each iteration in case alpha is non constant in your implementation.\n",
    "# ...    \n",
    "\n",
    "################## NOTE #######################################################\n",
    "# For some exercises it may be useful to use a learning rate scheduler for the newton method: it may help convergence.\n",
    "# @see https://en.wikipedia.org/wiki/Learning_rate\n",
    "###############################################################################\n",
    "    \n",
    "from numpy.linalg import inv,norm   \n",
    "\n",
    "\n",
    "def gradient_descent_lstsq(x0,loss_fnc,grad,alpha0,decay=0.0,epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array) : the initial iterate\n",
    "        grad  (functional) : the gradient function\n",
    "        alpha0 (float)     : the initial learning rate\n",
    "        decay (float)      : a decay parameter for the scheduler\n",
    "        epsilon (float)    : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    \n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses these values when calling your optimization functions\n",
    "\n",
    "hyper_lstsq = {\"gradient_descent\":{\"alpha0\": 1.0,'epsilon':0.01},\n",
    "               \"newton\"          :{'epsilon':0.00001}}\n",
    "    \n",
    "    \n",
    "def newton_lstsq(x0,loss_fnc,grad,hessian,beta0=1.,decay=0.,epsilon=0.0000001):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x0   (numpy.array)  : the initial iterate\n",
    "        loss_fnc(functional): the loss function\n",
    "        grad  (functional)  : the gradient function\n",
    "        hessian(functional) : the hessian function\n",
    "        epsilon             : the precision of the solution\n",
    "    Returns:\n",
    "        numpy.array. The value of the last iterate\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit_lm(dataset,predictor_lst,predicted,optim_method,alpha0=0.1,beta0=0.1,decay=0,epsilon=0.001,add_bias=True):\n",
    "    \"\"\"\n",
    "    Given a data matrix, returns the estimates of the parameters using least squares estimation.\n",
    "    Optimisation is performed either with the Newton method or the gradient method.\n",
    "    \n",
    "    Args: \n",
    "        dataset (pandas.DataFrame) : a dataset\n",
    "        predictor_lst        (list): a list of strings with the predictor variable names\n",
    "        predicted             (str): the name of the predicted variable\n",
    "        optim_method          (str): either 'grad_descent' or 'newton'\n",
    "        epsilon             (float): epsilon value for the optimizer\n",
    "    Returns :\n",
    "        numpy.array: a vector with parameter estimates\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "# HYPERPARAMETERS\n",
    "#  You may change the default hyperparameter values and set them to the values best suited for your implementation\n",
    "#  The autograder uses theses values when calling your optimization functions\n",
    "\n",
    "lr_params = {\"gradient_descent\":{\"alpha0\": 1.0,'decay':0.,'epsilon':0.00001},\n",
    "             \"newton\"          :{'beta0':1.0,'decay':0. ,'epsilon':0.00001}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447dcfd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d25367fd6ca6a9ebd4368604b82fb3c7",
     "grade": true,
     "grade_id": "cell-eb6a6ecaf1596345",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### TEST LINEAR REGRESSION (5pts)\n",
    "\n",
    "\n",
    "sols = np.array([  0.22282854, -0.20726607,  0.52408311, -0.24030739 ])\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                alpha0  =  lr_params['gradient_descent']['alpha0'],\n",
    "                decay   =  lr_params['gradient_descent']['decay'], \n",
    "                epsilon =  lr_params['gradient_descent']['epsilon'], \n",
    "                optim_method = 'grad_descent')\n",
    "assert np.isclose(result,sols,atol=0.0001).all()\n",
    "\n",
    "\n",
    "result = fit_lm(iris,['sepal width (cm)','sepal length (cm)','petal length (cm)'],'petal width (cm)',\n",
    "                beta0  =  lr_params['newton']['beta0'],\n",
    "                decay   =  lr_params['newton']['decay'], \n",
    "                epsilon =  lr_params['newton']['epsilon'], \n",
    "                optim_method = 'newton')\n",
    "assert np.isclose(result,sols,atol=0.000001).all()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
